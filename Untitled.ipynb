{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LAMBDA_PEN = 1000\n",
    "L_BOUND = -10\n",
    "U_BOUND = 10\n",
    "N_POINTS = 1000\n",
    "\n",
    "d_p = np.linspace(lower_bound, upper_bound, n_points)\n",
    "DISCRETE_POINTS = [torch.tensor([i], requires_grad=True, dtype=torch.float) for i in d_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING MODEL CLASS\n",
    "class Nonlinear(nn.Module):\n",
    "    def __init__(self, n, given_fn):\n",
    "        # One hidden layer with n nodes\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1, n)\n",
    "        self.output = nn.Linear(n, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.target_fn = given_fn\n",
    "\n",
    "    def forward(self, x, use_tanh_fn = False):\n",
    "        if use_tanh_fn == True:\n",
    "            x = self.hidden(x)\n",
    "            x = self.tanh(x)\n",
    "            x = self.output(x)\n",
    "        else:\n",
    "            x = self.hidden(x)\n",
    "            x = self.sigmoid(x)\n",
    "            x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def normalize_model(self, lower_bound, upper_bound, n_points):\n",
    "        \"\"\"\n",
    "        GOAL: Normalize the output weight layer\n",
    "        model.output *= c\n",
    "        where,\n",
    "        scalar c = 1/denom\n",
    "        \"\"\"\n",
    "        discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
    "        h = discrete_points[1] - discrete_points[0]\n",
    "        s = 0\n",
    "        for i in discrete_points:\n",
    "            x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "            s += model_u(x_i)**2\n",
    "        denom = math.sqrt(h) * torch.sqrt(s)\n",
    "        c = 1/denom\n",
    "\n",
    "        print(\"Before normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        \n",
    "        self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
    "        self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
    "\n",
    "        print(\"After normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        print(\"c value = \" + str(c))\n",
    "\n",
    "        return \n",
    "\n",
    "    def u_prime(self, x_in):\n",
    "        y = self(x_in)\n",
    "        y_prime = torch.autograd.grad(y.sum(), x_in, create_graph=True)\n",
    "        return y_prime[0]\n",
    "    \n",
    "    # TRANING MODEL\n",
    "    def train_network_with_penalty(self, num_epochs, v_x, optimizer, lambda_pen,\n",
    "                                    lower_bound, upper_bound, n_points):\n",
    "        # For plotting loss value over epochs:\n",
    "        x_epochs = []\n",
    "        y_loss = []\n",
    "        y_loss_pen = []\n",
    "\n",
    "        # stopping criterion:\n",
    "        stop_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            loss_pen = epsilon_Loss_penalty(v_x, self, lambda_pen,\n",
    "                                        lower_bound, upper_bound, n_points)\n",
    "            loss = epsilon_Loss(v_x, self,\n",
    "                                lower_bound, upper_bound, n_points)\n",
    "\n",
    "            y_loss_pen.append(loss_pen.detach().numpy().item())\n",
    "            y_loss.append(loss.detach().numpy().item())\n",
    "            x_epochs.append(epoch)\n",
    "\n",
    "            #check if need to stop training:\n",
    "            \"\"\"\n",
    "            if epoch > 0 and stop_counter >= 5:\n",
    "\n",
    "                #Normalize model before return\n",
    "                self.normalize_model(lower_bound, upper_bound, n_points)\n",
    "\n",
    "                print(\"LOSS VALUE WITH LAMBDA PENALTY = \" \n",
    "                      + str(epsilon_Loss_penalty(v_x, self, lambda_pen,\n",
    "                                                 lower_bound, upper_bound, n_points)))\n",
    "                print(\"LOSS VALUE  = \" \n",
    "                      + str(epsilon_Loss(v_x, self, lower_bound, upper_bound, n_points)))\n",
    "\n",
    "                break\n",
    "            elif epoch > 0 and stop_counter < 5:\n",
    "                if torch.abs(y_loss_pen[epoch-1]-loss_pen) <= 1e-5:\n",
    "                    stop_counter += 1\n",
    "                else:\n",
    "                    stop_counter = 0\n",
    "            else:\n",
    "                print(\"Uncatched case\")\n",
    "            \"\"\"\n",
    "\n",
    "            print('epoch {}, loss with penalty {}'.format(epoch, loss_pen.item()))\n",
    "            loss_pen.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Please normalize after training')\n",
    "        return (x_epochs, y_loss_pen, y_loss)\n",
    "\n",
    "    def train_network(self, num_epochs, v_x, optimizer,\n",
    "                                    lower_bound, upper_bound, n_points):\n",
    "        # For plotting loss value over epochs:\n",
    "        x_epochs = []\n",
    "        y_loss = []\n",
    "\n",
    "        # stopping criterion:\n",
    "        stop_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            if epoch > 0 and epoch % 50 == 0:\n",
    "                c = normalize_u(self, lower_bound, upper_bound, n_points)\n",
    "                print(\"Pre normalize: \")\n",
    "                print(self.output.weight.data)\n",
    "                print(self.output.bias.data)\n",
    "                print(\"After normalize: \")\n",
    "\n",
    "                self.normalize_model(lower_bound, upper_bound, n_points)\n",
    "\n",
    "                print(self.output.weight.data)\n",
    "                print(self.output.bias.data)\n",
    "                print(\"c value = \" + str(c))\n",
    "            optimizer.zero_grad()\n",
    "            loss = epsilon_Loss(v_x, self,\n",
    "                                lower_bound, upper_bound, n_points)\n",
    "            y_loss.append(loss.detach().numpy().item())\n",
    "            x_epochs.append(epoch)\n",
    "            #check if need to stop training:\n",
    "            if epoch > 0 and stop_counter >= 5:\n",
    "                print(\"LOSS VALUE = \" \n",
    "                      + str(epsilon_Loss(v_x, self, lower_bound, upper_bound, n_points)))\n",
    "                break\n",
    "            elif epoch > 0 and stop_counter < 5:\n",
    "                if torch.abs(y_loss[epoch-1]-loss) <= 1e-5:\n",
    "                    stop_counter += 1\n",
    "                else:\n",
    "                    stop_counter = 0\n",
    "\n",
    "            print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return (x_epochs, y_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_Loss(v_x, model_u, lower_bound, upper_bound, n_points):\n",
    "    \"\"\"\n",
    "    GOAL: Epsilon function evaluated at u using discretized estimation\n",
    "    minimizing Epsilon(u) = \n",
    "    \n",
    "    ARGS: \n",
    "    n_points (int): number of discretized points on the interval [-L, L]\n",
    "    e.g.: -(L)|---|---|---|---|(L) interval has n_points = 5\n",
    "\n",
    "    v_x (torch.Tensor): function instance\n",
    "    model_u (torch.Tensor): model output\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
    "    h = discrete_points[1] - discrete_points[0]\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "        u_xi = model_u(x_i)\n",
    "\n",
    "        u_prime_x = model_u.u_prime(x_i)\n",
    "        \n",
    "        v_xi = v_x(i)\n",
    "        t = torch.abs(torch.square(u_prime_x)) + v_xi*torch.square(u_xi)\n",
    "        total += t\n",
    "    return 0.5*h*total\n",
    "\n",
    "def epsilon_Loss_penalty(v_x, model_u, lambda_pen,\n",
    "                         lower_bound, upper_bound, n_points):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    eps_sum = 0\n",
    "    pen = 0\n",
    "\n",
    "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
    "    h = discrete_points[1] - discrete_points[0]\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "        u_prime = model_u.u_prime(x_i)\n",
    "        \n",
    "        v_xi = v_x(i)\n",
    "        u_xi = model_u(x_i)\n",
    "        u_xi_square = torch.square(u_xi)\n",
    "\n",
    "        t = torch.abs(torch.square(u_prime)) + v_xi*u_xi_square\n",
    "        eps_sum += t\n",
    "        \n",
    "        pen+= u_xi_square\n",
    "        \n",
    "    epsilon_fn = h*eps_sum\n",
    "\n",
    "    penalty = lambda_pen * torch.square(h*pen-1)\n",
    "#     print(\"epsilon_fn value = \" + str(epsilon_fn))\n",
    "#     print(\"penalty value = \" + str(penalty))\n",
    "    return epsilon_fn + penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_points = np.linspace(L_BOUND, U_BOUND, N_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "r1 = epsilon_Loss_penalty(potential_fn, model, \n",
    "                     LAMBDA_PEN, L_BOUND, U_BOUND, N_POINTS)\n",
    "end = time.time()\n",
    "time1 = end-start\n",
    "print(r1)\n",
    "print(time1)\n",
    "\n",
    "start = time.time()\n",
    "r2 = old_eps_pen(potential_fn, model, \n",
    "            LAMBDA_PEN, L_BOUND, U_BOUND, N_POINTS)\n",
    "end = time.time()\n",
    "time2 = end-start\n",
    "print(r2)\n",
    "print(time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_eps_pen(v_x, model_u, lambda_pen,\n",
    "                         lower_bound, upper_bound, n_points):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    eps_sum = 0\n",
    "    pen = 0\n",
    "\n",
    "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
    "    h = discrete_points[1] - discrete_points[0]\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "        u_prime = model_u.u_prime(x_i)\n",
    "        \n",
    "        u_xi = model_u(x_i)\n",
    "        v_xi = v_x(i)\n",
    "\n",
    "        t = torch.abs(torch.square(u_prime)) + v_xi*torch.square(u_xi)\n",
    "        eps_sum += t\n",
    "    epsilon_fn = h*eps_sum\n",
    "    print(\"epsilon_fn value = \" + str(epsilon_fn))\n",
    "    \n",
    "    temp = 0\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "        temp += torch.square(model_u(x_i))\n",
    "    \n",
    "    pen = lambda_pen * torch.square((h * temp-1))\n",
    "    print(\"penalty value = \" + str(pen))\n",
    "    return epsilon_fn + pen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_func_iterative(x, M_points=20, L_endpoint=U_BOUND, alpha=3, c_0=100):\n",
    "    f_value = 0\n",
    "    summation = 0\n",
    "\n",
    "    #Iterative method:\n",
    "    for i in range(1, M_points+1):\n",
    "        t_i = np.random.normal(loc=0, scale=1.0)\n",
    "        c_i = (math.pi/(i * L_endpoint))**alpha\n",
    "        cos_val = np.cos((i * L_endpoint * x)/math.pi)\n",
    "        summation += t_i * c_i * cos_val\n",
    "\n",
    "    f_value += summation\n",
    "    f_value += c_0\n",
    "\n",
    "    return f_value\n",
    "\n",
    "\n",
    "def potential_func_linalg(x, M_points=20, L_endpoint=U_BOUND, alpha=3, c_0=100):    \n",
    "    # Linear Algebra method:\n",
    "    t_i = np.random.normal(0, 1, size=N_points)\n",
    "    \n",
    "    iter1 = ((math.pi/(i*L_endpoint))**alpha for i in range(1, M_points+1))\n",
    "    c_i = np.fromiter(iter1, float)\n",
    "    a = np.multiply(t_i, c_i)\n",
    "\n",
    "    iter2 = (i for i in range(1, M_points+1))\n",
    "    v = np.fromiter(iter2, float)\n",
    "    s = (L_endpoint*x/math.pi)*v\n",
    "    cos_s = np.cos(s)\n",
    "    \n",
    "    res_vector = np.multiply(a, cos_s)\n",
    "    return np.sum(res_vector) + c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.01112319056631"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_fn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.05\n",
    "num_epochs = 20000\n",
    "\n",
    "potential_fn = potential_func_iterative\n",
    "\n",
    "#INIT MODEL\n",
    "model = Nonlinear(20, potential_func_iterative)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# INIT OPTIMIZER CLASS\n",
    "# Adam:\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                    lr=learningRate, \n",
    "                                    betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, \n",
    "                                    weight_decay=0, \n",
    "                                    amsgrad=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dear Wuzhe, I'm trying to improve the speed for my loss function. Right now, the time measured for one run of my loss function is 0.5003s, which makes my training process really slow. In comparison, the normal MSE loss function implemented in Torch is 0.00028, which is roughly 1000 times faster. \n",
    "\n",
    "My loss function involves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "res1 = epsilon_Loss_penalty()\n",
    "end = time.time()\n",
    "time1 = end-start\n",
    "print(time1)\n",
    "\n",
    "start = time.time()\n",
    "res2 = epsilon_Loss_linalg()\n",
    "end = time.time()\n",
    "time2 = end-start\n",
    "print(time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13143, loss with penalty 97.53775024414062\n",
      "epoch 13144, loss with penalty 97.53833770751953\n",
      "epoch 13145, loss with penalty 97.5372543334961\n",
      "epoch 13146, loss with penalty 97.53797149658203\n",
      "epoch 13147, loss with penalty 97.53782653808594\n",
      "epoch 13148, loss with penalty 97.53736877441406\n",
      "epoch 13149, loss with penalty 97.53668975830078\n",
      "epoch 13150, loss with penalty 97.53852081298828\n",
      "epoch 13151, loss with penalty 97.53575897216797\n",
      "epoch 13152, loss with penalty 97.5372314453125\n",
      "epoch 13153, loss with penalty 97.53662109375\n",
      "epoch 13154, loss with penalty 97.53755187988281\n",
      "epoch 13155, loss with penalty 97.53727722167969\n",
      "epoch 13156, loss with penalty 97.53700256347656\n",
      "epoch 13157, loss with penalty 97.53841400146484\n",
      "epoch 13158, loss with penalty 97.5368881225586\n",
      "epoch 13159, loss with penalty 97.53744506835938\n",
      "epoch 13160, loss with penalty 97.53614044189453\n",
      "epoch 13161, loss with penalty 97.53711700439453\n",
      "epoch 13162, loss with penalty 97.53801727294922\n",
      "epoch 13163, loss with penalty 97.537109375\n",
      "epoch 13164, loss with penalty 97.53777313232422\n",
      "epoch 13165, loss with penalty 97.53585052490234\n",
      "epoch 13166, loss with penalty 97.53922271728516\n"
     ]
    }
   ],
   "source": [
    "model.train_network_with_penalty(num_epochs, potential_fn, adam_optimizer, \n",
    "                                LAMBDA_PEN, L_BOUND, U_BOUND, N_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
