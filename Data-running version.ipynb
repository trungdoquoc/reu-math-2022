{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "U3Z0FXavK5LD"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "# from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "P5y5jqCDK5LG"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS FOR \n",
    "LAMBDA_PEN = 1000\n",
    "L_BOUND = -10\n",
    "U_BOUND = 10\n",
    "N_POINTS = 512\n",
    "\n",
    "NUM_EPOCHS = 20000\n",
    "\n",
    "M_POINTS = 10\n",
    "ALPHA = 2\n",
    "\n",
    "SELECTION_RATE = 0.01\n",
    "\n",
    "DISCRETE_POINTS = np.linspace(L_BOUND, U_BOUND, N_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "03rjt3_6K5LG",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class sinFn(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(input):\n",
    "        return torch.sin(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bIUFbmpwcCeA",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def model_on_interval(nn_model, discrete_points):\n",
    "    x_vals = [torch.tensor([i], requires_grad=True, dtype=torch.float) for i in discrete_points]\n",
    "\n",
    "    model_output = []\n",
    "    for i in x_vals:\n",
    "        model_output.append(nn_model(i).detach().numpy().item())\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "P1WpAhwrK5LI",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class X_Square_sin:\n",
    "    def __init__(self, const, use_sin=False):\n",
    "        self.const = const\n",
    "        if use_sin == True:\n",
    "            self.sin_const = np.random.choice(np.arange(10, 50))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x**2 + self.const + self.sin_const*math.sin(x)\n",
    "    \n",
    "class Potential_Function:\n",
    "    def __init__(self, c_0=0,\n",
    "                 M_points=M_POINTS,\n",
    "                 L_endpoint=U_BOUND, \n",
    "                 alpha=ALPHA, \n",
    "                 rescale=1):\n",
    "        self.M_points = M_points\n",
    "        self.L_endpoint = L_endpoint\n",
    "        self.alpha = alpha\n",
    "        self.c_0 = c_0\n",
    "        self.rescale = 1\n",
    "        \n",
    "        self.ti_list = np.random.normal(loc=0, scale=1.0, size= self.M_points)\n",
    "        self.ci_list = [(self.L_endpoint/(i * math.pi))**self.alpha \n",
    "                        for i in range(1, self.M_points+1)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        f_value = 0\n",
    "        summation = 0\n",
    "        \n",
    "        #Iterative method:\n",
    "\n",
    "        for i in range(1, self.M_points+1):\n",
    "            cos_val = np.cos((i * math.pi * x)/self.L_endpoint)\n",
    "            summation += self.ti_list[i-1] * self.ci_list[i-1] * cos_val\n",
    "\n",
    "        f_value += summation\n",
    "        f_value += self.c_0\n",
    "        return self.rescale * f_value\n",
    "    \n",
    "    def set_c0_value(self, val):\n",
    "        self.c_0 = val\n",
    "        return\n",
    "    \n",
    "    def set_rescale_factor(self, val):\n",
    "        self.rescale = val\n",
    "        return \n",
    "    \n",
    "    def plot_function(self, discrete_points):\n",
    "        y_values = [self.forward(i) for i in discrete_points]\n",
    "        plt.plot(discrete_points, y_values)\n",
    "    \n",
    "    def update_potential_fn(self, discrete_points):\n",
    "        # (1) Check lowest value to set appropriate c_0 value:\n",
    "        y_values = [self.forward(i) for i in discrete_points]\n",
    "        min_val = min(y_values)\n",
    "        print(\"Original min val = \" + str(min_val))\n",
    "        # plt.plot(DISCRETE_POINTS, y_values)\n",
    "\n",
    "        # (1) Set c_0 value s.t. every value in y_values is > 0: \n",
    "        if min_val < 0:\n",
    "            c_0 = math.ceil(abs(min_val))\n",
    "            self.set_c0_value(c_0)\n",
    "            new_y = [self.forward(i) for i in discrete_points]\n",
    "            print(\"Min val with c_0 updated = \" + str(min(new_y)))\n",
    "        # plt.plot(DISCRETE_POINTS, new_y)\n",
    "        else:\n",
    "            new_y = [self.forward(i) for i in discrete_points]\n",
    "            print(\"Min val with c_0 updated = \" + str(min(new_y)))\n",
    "        # (2) Rescale potential function s.t. every value lies between 0 and 5:\n",
    "        max_v = max(new_y)\n",
    "#         min_v = min(new_y)\n",
    "#         print(max_v, min_v)\n",
    "#         max_min = max_v-min_v\n",
    "\n",
    "#         rescaled_y = [(i-min_v)/(max_min) for i in new_y]\n",
    "#         plt.plot(DISCRETE_POINTS, rescaled_y)\n",
    "\n",
    "        c = 5/max_v\n",
    "        self.set_rescale_factor(c)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gwppbWioK5LJ",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_Loss_penalty(v_x, model_u, \n",
    "                         lambda_pen,\n",
    "                         upper_bound, \n",
    "                         discrete_points):\n",
    "    eps_sum = 0\n",
    "    pen = 0\n",
    "\n",
    "    h = (2*upper_bound)/(len(discrete_points)-1)\n",
    "#     print(\"h = \"+ str(h))\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "\n",
    "        u_prime_square = torch.square(model_u.u_prime(x_i))\n",
    "        u_xi_square = torch.square(model_u(x_i))\n",
    "        \n",
    "        v_xi = v_x.forward(i)\n",
    "        if v_xi < 0:\n",
    "            raise ValueError('Potential Function value is < 0')\n",
    "\n",
    "        t = u_prime_square + v_xi*u_xi_square\n",
    "        eps_sum += t\n",
    "        \n",
    "        pen+= u_xi_square\n",
    "        \n",
    "    epsilon_fn = h*eps_sum\n",
    "    \n",
    "    penalty = lambda_pen * torch.square(h*pen-1)\n",
    "#     print(\"epsilon_fn value = \" + str(epsilon_fn))\n",
    "#     print(\"penalty value = \" + str(penalty))\n",
    "    return (epsilon_fn, epsilon_fn + penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "clBtQzFcK5LK"
   },
   "outputs": [],
   "source": [
    "# CREATING MODEL CLASS\n",
    "class Nonlinear_1(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network with 1 layers\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_width, use_sin=False):\n",
    "        super(Nonlinear_1, self).__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = sinFn()\n",
    "        self.use_sin = use_sin\n",
    "#         self.tanh = nn.Tanh()\n",
    "\n",
    "        self.hidden = nn.Linear(1, layer_width)\n",
    "        self.output = nn.Linear(layer_width, 1)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if self.use_sin == True:\n",
    "            x = self.hidden(x)\n",
    "            x = self.sin(x)\n",
    "            x = self.output(x)\n",
    "        else:\n",
    "            x = self.hidden(x)\n",
    "            x = self.sigmoid(x)\n",
    "            x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def normalize_model(self, discrete_points):\n",
    "        \"\"\"\n",
    "        GOAL: Normalize the output weight layer\n",
    "        model.output *= c\n",
    "        where,\n",
    "        scalar c = 1/denom\n",
    "        \"\"\"\n",
    "        \n",
    "        h = discrete_points[1]-discrete_points[0]\n",
    "        s = 0\n",
    "        for i in discrete_points:\n",
    "            x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "            s += self(x_i)**2\n",
    "        denom = math.sqrt(h) * torch.sqrt(s)\n",
    "        c = 1/denom\n",
    "\n",
    "        print(\"Before normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        \n",
    "        self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
    "        self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
    "\n",
    "        print(\"After normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        print(\"c value = \" + str(c))\n",
    "\n",
    "        return \n",
    "\n",
    "    def u_prime(self, x_in):\n",
    "        y = self(x_in)\n",
    "        y_prime = torch.autograd.grad(y.sum(), x_in, create_graph=True)\n",
    "        return y_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "i0X45z26K5LJ"
   },
   "outputs": [],
   "source": [
    "class Nonlinear_2(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network with 2 layers\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_width, use_sin=False):\n",
    "        super(Nonlinear_2, self).__init__()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = sinFn()\n",
    "#         self.tanh = nn.Tanh()\n",
    "        self.use_sin = use_sin\n",
    "        \n",
    "        self.hidden1 = nn.Linear(1, layer_width)\n",
    "        self.hidden2 = nn.Linear(layer_width, layer_width)\n",
    "        self.output = nn.Linear(layer_width, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.use_sin == True:\n",
    "            x = self.hidden1(x)\n",
    "            x = self.sin(x)\n",
    "            \n",
    "            x = self.hidden2(x)\n",
    "            x = self.sin(x)\n",
    "            \n",
    "            x = self.output(x)\n",
    "            \n",
    "        else:\n",
    "            x = self.hidden1(x)\n",
    "            x = self.sigmoid(x)\n",
    "            \n",
    "            x = self.hidden2(x)\n",
    "            x = self.sigmoid(x)\n",
    "            \n",
    "            x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def normalize_model(self, discrete_points):\n",
    "        \"\"\"\n",
    "        GOAL: Normalize the output weight layer\n",
    "        model.output *= c\n",
    "        where,\n",
    "        scalar c = 1/denom\n",
    "        \"\"\"\n",
    "        \n",
    "        h = discrete_points[1]-discrete_points[0]\n",
    "        s = 0\n",
    "        for i in discrete_points:\n",
    "            x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "            s += self(x_i)**2\n",
    "        denom = math.sqrt(h) * torch.sqrt(s)\n",
    "        c = 1/denom\n",
    "\n",
    "        print(\"Before normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        \n",
    "        self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
    "        self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
    "\n",
    "        print(\"After normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        print(\"c value = \" + str(c))\n",
    "\n",
    "        return \n",
    "\n",
    "    def u_prime(self, x_in):\n",
    "        y = self(x_in)\n",
    "        y_prime = torch.autograd.grad(y.sum(), x_in, create_graph=True)\n",
    "        return y_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "-yehv8TrK5LL"
   },
   "outputs": [],
   "source": [
    "# TRANING MODEL\n",
    "def minibatch_train_with_penalty(model, \n",
    "                               num_epochs, \n",
    "                               v_x, \n",
    "                               optimizer, \n",
    "                               lambda_pen,\n",
    "                               discrete_points,\n",
    "                               batch_size=32):\n",
    "    # For plotting loss value over epochs:\n",
    "    x_epochs = []\n",
    "    y_loss = []\n",
    "    y_loss_pen = []\n",
    "\n",
    "    m = len(discrete_points)\n",
    "    num_batches = int(m/batch_size)\n",
    "#         print(\"Number of batches \" + str(num_batches))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        perm = torch.randperm(m)\n",
    "        avg_loss_pen = 0\n",
    "        avg_loss = 0\n",
    "\n",
    "#                 start1 = time.time()\n",
    "        for i in range(0, m, batch_size):\n",
    "#                 print(\"i = \" + str(i))\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            indices = perm[i: i+batch_size]\n",
    "\n",
    "            loss_values = epsilon_Loss_penalty(v_x, model, lambda_pen,\n",
    "                                            U_BOUND, \n",
    "                                            discrete_points[indices])\n",
    "\n",
    "            loss = loss_values[0]\n",
    "            loss_pen = loss_values[1]\n",
    "\n",
    "            avg_loss_pen += loss_pen.item()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            loss_pen.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss_pen = avg_loss_pen/num_batches\n",
    "        avg_loss = avg_loss/num_batches\n",
    "\n",
    "        y_loss_pen.append(avg_loss_pen)\n",
    "        y_loss.append(avg_loss)\n",
    "        x_epochs.append(epoch)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print('epoch {}, loss with penalty {}'.format(epoch, avg_loss_pen))\n",
    "\n",
    "#     print('Please normalize after training')\n",
    "    return (x_epochs, y_loss, y_loss_pen)\n",
    "\n",
    "def batch_train_with_penalty(model, \n",
    "                         num_epochs, \n",
    "                         v_x, \n",
    "                         optimizer, \n",
    "                         lambda_pen, \n",
    "                         discrete_points):\n",
    "    # For plotting loss value over epochs:\n",
    "    x_epochs = []\n",
    "    y_loss = []\n",
    "    y_loss_pen = []\n",
    "\n",
    "    #Early stopping criteria:\n",
    "    last_min_loss = 2.0\n",
    "    patience = 4\n",
    "    min_delta = 1e-3 #minimum difference to be considered better loss\n",
    "    stop_counter = 0\n",
    "    best_model = None\n",
    "    \n",
    "    now = datetime.datetime.now(tz=timezone('US/Eastern'))\n",
    "    print(\"Start time is \" + str(now))\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss, loss_pen = epsilon_Loss_penalty(v_x, model, lambda_pen,\n",
    "                                    U_BOUND, discrete_points)\n",
    "\n",
    "        y_loss_pen.append(loss_pen.detach().numpy().item())\n",
    "        y_loss.append(loss.detach().numpy().item())\n",
    "        x_epochs.append(epoch)\n",
    "        \n",
    "        # Save best model\n",
    "        if loss_pen < last_min_loss and abs(loss_pen-last_min_loss) >= min_delta:\n",
    "            print(\"New min loss = \" + str(loss_pen))\n",
    "            best_model = copy.deepcopy(model)\n",
    "            last_min_loss = loss_pen\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print('epoch {}, loss with penalty {}'.format(epoch, loss_pen.item()))\n",
    "        loss_pen.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    end = time.time()\n",
    "    time1 = end-start\n",
    "    print(\"Using potential function defined as class takes \" + str(time1))\n",
    "\n",
    "    model.normalize_model(DISCRETE_POINTS)\n",
    "    return (x_epochs, y_loss, y_loss_pen, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH = './model_results/'\n",
    "p_fn_fnames = {p_fn1: 'p_fn1',\n",
    "               p_fn2: 'p_fn2',\n",
    "               p_fn3: 'p_fn3',\n",
    "               p_fn4: 'p_fn4',\n",
    "               p_fn5: 'p_fn5'}\n",
    "model_1_1_sin_info = {'model_fname': 'model_1_1_sin'}\n",
    "model_1_1_sigmoid= 'model_1_1_sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_p_fn(res_path, p_fn_dict: dict):\n",
    "    \"\"\"\n",
    "    Save a dict of p_fn as fixed files, \n",
    "    when need to train p_fn, just load these p_fn saved files. \n",
    "    \"\"\"\n",
    "    for p in p_fn_dict:\n",
    "        p_fn_fpath = res_path + p_fn_dict[p]\n",
    "        with open(p_fn_fpath, \"wb\") as fp:\n",
    "            pickle.dump(p, fp)\n",
    "    return\n",
    "\n",
    "def load_p_fn(p_fn_fpath: str):\n",
    "    # TODO\n",
    "    with open(p_fn_fpath, \"rb\") as fp:\n",
    "        p_fn = pickle.load(fp)\n",
    "    return p_fn\n",
    "\n",
    "def auto_test_documentation(model, optimizer, p_fn, \n",
    "                            model_name: str, res_path: str,\n",
    "                            loss_fname: str, loss_with_pen_fname: str):\n",
    "    \"\"\"\n",
    "    Automate training model, then save its results for future reference. \n",
    "    \n",
    "    \"\"\"\n",
    "    loss_fpath = res_path + loss_fname\n",
    "    loss_with_pen_fpath = res_path + loss_with_pen_fname\n",
    "    model_fpath = res_path + model_name\n",
    "    x_epochs, y_loss, y_loss_pen, best_model = batch_train_with_penalty(model, \n",
    "                                                                        NUM_EPOCHS, \n",
    "                                                                        p_fn, \n",
    "                                                                        optimizer, \n",
    "                                                                        LAMBDA_PEN, \n",
    "                                                                        DISCRETE_POINTS)\n",
    "    \n",
    "    #: store loss & loss with penalty value in a file type\n",
    "    with open(loss_fpath, \"wb\") as fp:\n",
    "        pickle.dump(y_loss, fp) #use: with open() as fp: pickle.load(fp) to load file\n",
    "    \n",
    "    with open(loss_with_pen_fpath, \"wb\") as fp:\n",
    "        pickle.dump(y_loss_pen, fp)\n",
    "        \n",
    "    #: store best_model from training\n",
    "    torch.save(best_model.state_dict(), model_fpath)\n",
    "    \n",
    "    ### How to load model from saved file\n",
    "    # load_model = MODEL_CLASS(...)\n",
    "    # load_model.load_state_dict(torch.load(f_path))\n",
    "    ###\n",
    "    \n",
    "    #: store eigenvalue + corresponding eigenfunction using FD method\n",
    "#     eig_val, ground_state = finite_diff_method(DISCRETE_POINTS, p_fn)\n",
    "#     with open(eig_val_fpath, \"wb\") as fp:\n",
    "#         pickle.dump(l, fp)\n",
    "        \n",
    "#     with open(ground_state_fpath, \"rb\") as fp:\n",
    "#         pickle.dump(l, fp)\n",
    "        \n",
    "    return x_epochs, y_loss, y_loss_pen, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5O3XwauEK5LH"
   },
   "outputs": [],
   "source": [
    "# PLOT DATA\n",
    "def plot_figure(x_val, y_val, x_test=None, predicted=None, log_scale=False):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 6), dpi=80)\n",
    "    if log_scale==True:\n",
    "        plt.yscale('log')\n",
    "    plt.plot(x_val, y_val, '--', label='True data', alpha=0.5)\n",
    "    if predicted != None:\n",
    "        plt.plot(x_test, predicted, '--', label='Predictions', alpha=0.5)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_11_sin\n",
    "del model_11_sigmoid\n",
    "\n",
    "del adam_opt_11_sin\n",
    "del adam_opt_11_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "8Ph_ZvmuK5LL"
   },
   "outputs": [],
   "source": [
    "learningRate = 0.01\n",
    "\n",
    "#INIT MODEL\n",
    "model_11_sin = Nonlinear_1(20, use_sin=True)\n",
    "model_11_sigmoid = Nonlinear_1(20)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_11_sin.cuda()\n",
    "    model_11_sigmoid.cuda()\n",
    "\n",
    "# INIT OPTIMIZER CLASS\n",
    "# Adam:\n",
    "adam_opt_11_sin = torch.optim.Adam(model_11_sin.parameters(), \n",
    "                                    lr=learningRate, \n",
    "                                    betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, \n",
    "                                    weight_decay=0, \n",
    "                                    amsgrad=False)\n",
    "\n",
    "adam_opt_11_sigmoid = torch.optim.Adam(model_11_sigmoid.parameters(), \n",
    "                                    lr=learningRate, \n",
    "                                    betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, \n",
    "                                    weight_decay=0, \n",
    "                                    amsgrad=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "88EiIGL3cvoA",
    "outputId": "7865ac7f-7d7b-4529-a045-6bfea26c1775"
   },
   "source": [
    "# INIT POTENTIAL FUNCTIONS\n",
    "p_fn1 = Potential_Function(M_points=5)\n",
    "p_fn1.update_potential_fn(DISCRETE_POINTS)\n",
    "p_fn1.plot_function(DISCRETE_POINTS)\n",
    "\n",
    "p_fn2 = Potential_Function(M_points=5)\n",
    "p_fn1.update_potential_fn(DISCRETE_POINTS)\n",
    "p_fn1.plot_function(DISCRETE_POINTS)\n",
    "\n",
    "p_fn3 = Potential_Function(M_points=5)\n",
    "p_fn1.update_potential_fn(DISCRETE_POINTS)\n",
    "p_fn1.plot_function(DISCRETE_POINTS)\n",
    "\n",
    "p_fn4 = Potential_Function(M_points=5)\n",
    "p_fn1.update_potential_fn(DISCRETE_POINTS)\n",
    "p_fn1.plot_function(DISCRETE_POINTS)\n",
    "\n",
    "p_fn5 = Potential_Function()\n",
    "p_fn5.update_potential_fn(DISCRETE_POINTS)\n",
    "p_fn5.plot_function(DISCRETE_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7900, loss with penalty 0.6460030674934387\n",
      "epoch 8000, loss with penalty 0.6391268372535706\n",
      "epoch 8100, loss with penalty 0.6339598894119263\n",
      "epoch 8200, loss with penalty 0.6244485378265381\n",
      "epoch 8300, loss with penalty 0.6281969547271729\n",
      "epoch 8400, loss with penalty 0.6240236163139343\n",
      "epoch 8500, loss with penalty 0.6293421983718872\n",
      "epoch 8600, loss with penalty 0.6206277012825012\n",
      "epoch 8700, loss with penalty 0.661446750164032\n",
      "epoch 8800, loss with penalty 0.62567138671875\n",
      "New min loss = tensor([0.6199], grad_fn=<AddBackward0>)\n",
      "epoch 8900, loss with penalty 0.7087817192077637\n",
      "epoch 9000, loss with penalty 0.629014790058136\n",
      "epoch 9100, loss with penalty 0.6594228744506836\n",
      "epoch 9200, loss with penalty 0.6582466959953308\n",
      "epoch 9300, loss with penalty 0.6652678847312927\n",
      "epoch 9400, loss with penalty 0.6210907697677612\n",
      "epoch 9500, loss with penalty 0.7169267535209656\n",
      "epoch 9600, loss with penalty 0.6998403072357178\n",
      "epoch 9700, loss with penalty 0.7200407385826111\n",
      "epoch 9800, loss with penalty 0.7247006893157959\n",
      "epoch 9900, loss with penalty 0.6220712661743164\n",
      "epoch 10000, loss with penalty 0.6228466033935547\n",
      "epoch 10100, loss with penalty 0.7874695062637329\n",
      "epoch 10200, loss with penalty 0.6405841112136841\n",
      "epoch 10300, loss with penalty 0.6298623085021973\n",
      "epoch 10400, loss with penalty 0.6215176582336426\n",
      "epoch 10500, loss with penalty 0.6422837972640991\n",
      "epoch 10600, loss with penalty 0.647558331489563\n",
      "epoch 10700, loss with penalty 0.6455913186073303\n",
      "epoch 10800, loss with penalty 5.734091758728027\n",
      "epoch 10900, loss with penalty 1.4309136867523193\n",
      "epoch 11000, loss with penalty 1.0676692724227905\n",
      "epoch 11100, loss with penalty 0.8881140947341919\n",
      "epoch 11200, loss with penalty 0.8074119687080383\n",
      "epoch 11300, loss with penalty 0.7705650329589844\n",
      "epoch 11400, loss with penalty 0.7521513104438782\n",
      "epoch 11500, loss with penalty 0.7418597936630249\n",
      "epoch 11600, loss with penalty 0.735508382320404\n",
      "epoch 11700, loss with penalty 0.7312768697738647\n",
      "epoch 11800, loss with penalty 0.7282835245132446\n",
      "epoch 11900, loss with penalty 0.7260532975196838\n",
      "epoch 12000, loss with penalty 0.7243058681488037\n",
      "epoch 12100, loss with penalty 0.7228706479072571\n",
      "epoch 12200, loss with penalty 0.721636950969696\n",
      "epoch 12300, loss with penalty 0.7205327153205872\n",
      "epoch 12400, loss with penalty 0.7195084095001221\n",
      "epoch 12500, loss with penalty 0.7185311317443848\n",
      "epoch 12600, loss with penalty 0.717575192451477\n",
      "epoch 12700, loss with penalty 0.7166282534599304\n",
      "epoch 12800, loss with penalty 0.7156797647476196\n",
      "epoch 12900, loss with penalty 0.7147229909896851\n",
      "epoch 13000, loss with penalty 0.713754415512085\n",
      "epoch 13100, loss with penalty 0.7127735614776611\n",
      "epoch 13200, loss with penalty 0.7117785811424255\n",
      "epoch 13300, loss with penalty 0.7107651829719543\n",
      "epoch 13400, loss with penalty 0.7097312808036804\n",
      "epoch 13500, loss with penalty 0.7086657881736755\n",
      "epoch 13600, loss with penalty 0.7075589299201965\n",
      "epoch 13700, loss with penalty 0.7063906192779541\n",
      "epoch 13800, loss with penalty 0.7051412463188171\n",
      "epoch 13900, loss with penalty 0.7037829756736755\n",
      "epoch 14000, loss with penalty 0.7022866606712341\n",
      "epoch 14100, loss with penalty 0.865938663482666\n",
      "epoch 14200, loss with penalty 0.6976941823959351\n",
      "epoch 14300, loss with penalty 0.7036229968070984\n",
      "epoch 14400, loss with penalty 0.6920827627182007\n",
      "epoch 14500, loss with penalty 0.6892418265342712\n",
      "epoch 14600, loss with penalty 0.698964536190033\n",
      "epoch 14700, loss with penalty 0.6816207766532898\n",
      "epoch 14800, loss with penalty 1.1629164218902588\n",
      "epoch 14900, loss with penalty 0.6727015972137451\n",
      "epoch 15000, loss with penalty 0.6683436036109924\n",
      "epoch 15100, loss with penalty 0.6638979911804199\n",
      "epoch 15200, loss with penalty 0.6580127477645874\n",
      "epoch 15300, loss with penalty 0.8327921032905579\n",
      "epoch 15400, loss with penalty 0.6482333540916443\n",
      "epoch 15500, loss with penalty 0.6455238461494446\n",
      "epoch 15600, loss with penalty 0.6398301720619202\n",
      "epoch 15700, loss with penalty 0.6360895037651062\n",
      "epoch 15800, loss with penalty 0.6775609254837036\n",
      "epoch 15900, loss with penalty 0.6303881406784058\n",
      "epoch 16000, loss with penalty 0.6279886960983276\n",
      "epoch 16100, loss with penalty 0.6276542544364929\n",
      "epoch 16200, loss with penalty 0.6246570944786072\n",
      "epoch 16300, loss with penalty 0.7074697017669678\n",
      "epoch 16400, loss with penalty 0.6224643588066101\n",
      "epoch 16500, loss with penalty 0.6241535544395447\n",
      "epoch 16600, loss with penalty 0.6212400197982788\n",
      "epoch 16700, loss with penalty 0.6204356551170349\n",
      "epoch 16800, loss with penalty 0.6208086609840393\n",
      "epoch 16900, loss with penalty 0.6196946501731873\n",
      "epoch 17000, loss with penalty 0.6240333914756775\n",
      "epoch 17100, loss with penalty 0.6191897392272949\n",
      "epoch 17200, loss with penalty 0.6193550825119019\n",
      "New min loss = tensor([0.6189], grad_fn=<AddBackward0>)\n",
      "epoch 17300, loss with penalty 0.6188404560089111\n",
      "epoch 17400, loss with penalty 0.619088351726532\n",
      "epoch 17500, loss with penalty 0.6185890436172485\n",
      "epoch 17600, loss with penalty 0.6191335320472717\n",
      "epoch 17700, loss with penalty 0.6184154748916626\n",
      "epoch 17800, loss with penalty 0.6202015280723572\n",
      "epoch 17900, loss with penalty 0.6182945370674133\n",
      "epoch 18000, loss with penalty 0.6722826361656189\n",
      "epoch 18100, loss with penalty 0.6182296276092529\n",
      "epoch 18200, loss with penalty 1.1155883073806763\n",
      "epoch 18300, loss with penalty 0.6181565523147583\n",
      "epoch 18400, loss with penalty 0.6205242872238159\n",
      "epoch 18500, loss with penalty 0.6183066368103027\n",
      "epoch 18600, loss with penalty 0.6179949045181274\n",
      "epoch 18700, loss with penalty 0.6189874410629272\n",
      "epoch 18800, loss with penalty 0.6744601726531982\n",
      "New min loss = tensor([0.6179], grad_fn=<AddBackward0>)\n",
      "epoch 18900, loss with penalty 0.6268057823181152\n",
      "epoch 19000, loss with penalty 0.6225855946540833\n",
      "epoch 19100, loss with penalty 0.7049769759178162\n",
      "epoch 19200, loss with penalty 0.6818515062332153\n",
      "epoch 19300, loss with penalty 0.63575279712677\n",
      "epoch 19400, loss with penalty 0.6186619997024536\n",
      "epoch 19500, loss with penalty 0.6174905300140381\n",
      "epoch 19600, loss with penalty 0.6484152674674988\n",
      "epoch 19700, loss with penalty 0.6190537810325623\n",
      "epoch 19800, loss with penalty 0.632815957069397\n",
      "epoch 19900, loss with penalty 0.6172662973403931\n",
      "Using potential function defined as class takes 4597.161396980286\n",
      "Before normalization: \n",
      "tensor([[-0.0408, -0.0051, -0.0499,  0.0520,  0.0919,  0.0011, -0.0030, -0.0017,\n",
      "          0.0070,  0.0007, -0.0026, -0.0992,  0.0219,  0.0881,  0.0830, -0.0826,\n",
      "          0.0199, -0.0290, -0.0007,  0.0808]])\n",
      "tensor([-0.1787])\n",
      "After normalization: \n",
      "tensor([[-0.0408, -0.0051, -0.0500,  0.0520,  0.0920,  0.0011, -0.0030, -0.0017,\n",
      "          0.0071,  0.0007, -0.0026, -0.0993,  0.0219,  0.0882,  0.0831, -0.0827,\n",
      "          0.0199, -0.0291, -0.0007,  0.0809]])\n",
      "tensor([-0.1788])\n",
      "c value = tensor([1.0008], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "res_1_1_sin = auto_test_documentation(model_11_sin, adam_opt_11_sin, p_fn1, \n",
    "                                      model_name='model_11_sin_pfn1',\n",
    "                                      res_path=RESULT_PATH,\n",
    "                                      loss_fname='loss_11_sin_pfn1',\n",
    "                                      loss_with_pen_fname = 'loss_pen_11_sin_pfn1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time is 2022-08-04 12:28:30.653446-04:00\n",
      "epoch 0, loss with penalty 924.5447998046875\n",
      "epoch 100, loss with penalty 2.6889307498931885\n",
      "epoch 200, loss with penalty 2.6280672550201416\n",
      "epoch 300, loss with penalty 2.5624821186065674\n",
      "epoch 400, loss with penalty 2.482923984527588\n",
      "epoch 500, loss with penalty 2.393411636352539\n",
      "epoch 600, loss with penalty 2.299161911010742\n",
      "epoch 700, loss with penalty 2.20574951171875\n",
      "epoch 800, loss with penalty 2.117751359939575\n",
      "epoch 900, loss with penalty 2.0377402305603027\n",
      "New min loss = tensor([1.9988], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9973], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9959], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9945], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9931], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9917], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9903], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9889], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9874], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9860], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9846], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9832], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9819], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9805], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9791], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9777], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9763], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9749], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9736], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9722], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9708], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9695], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9681], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9667], grad_fn=<AddBackward0>)\n",
      "epoch 1000, loss with penalty 1.9660594463348389\n",
      "New min loss = tensor([1.9654], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9640], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9627], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9613], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9600], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9586], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9573], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9560], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9546], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9533], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9520], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9506], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9493], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9480], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9467], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9453], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9440], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9427], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9414], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9401], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9388], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9375], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9362], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9349], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9336], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9323], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9310], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9297], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9284], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9272], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9259], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9246], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9233], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9220], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9208], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9195], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9182], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9170], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9157], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9144], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9132], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9119], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9107], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9094], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9082], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9069], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9057], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9044], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9032], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9019], grad_fn=<AddBackward0>)\n",
      "epoch 1100, loss with penalty 1.9013220071792603\n",
      "New min loss = tensor([1.9007], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8995], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8982], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8970], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8958], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8945], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8933], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8921], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8908], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8896], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8884], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8872], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8860], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8847], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8835], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8823], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8811], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8799], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8787], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8775], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8763], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8751], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8739], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8727], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8715], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8703], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8691], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8679], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8667], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8655], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8643], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8631], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8619], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8607], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8595], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8583], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8572], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8560], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8548], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8536], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8524], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8512], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8501], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8489], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8477], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8465], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8454], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8442], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8430], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8418], grad_fn=<AddBackward0>)\n",
      "epoch 1200, loss with penalty 1.8412590026855469\n",
      "New min loss = tensor([1.8407], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8395], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8383], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8372], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8360], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8348], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8337], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8325], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8313], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8302], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8290], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8278], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8267], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8255], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8244], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8232], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8221], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8209], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8197], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8186], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8174], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8163], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8151], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8140], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8128], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8117], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8105], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8093], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8082], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8070], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8059], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8047], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8036], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8024], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8013], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8001], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7990], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7978], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7967], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7955], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7944], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7933], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7921], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7910], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7898], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7887], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7875], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7864], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7852], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7841], grad_fn=<AddBackward0>)\n",
      "epoch 1300, loss with penalty 1.7835031747817993\n",
      "New min loss = tensor([1.7829], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7818], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7806], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7795], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7783], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7772], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7761], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7749], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7738], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7726], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7715], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7703], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7692], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7680], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7669], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7657], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7646], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7635], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7623], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7612], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7600], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7589], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7577], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7566], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7554], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7543], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7531], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7520], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7508], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7497], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7485], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7474], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7462], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7451], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7439], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7428], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7416], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7405], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7393], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7382], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7370], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7359], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7347], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7336], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7324], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7313], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7301], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7290], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7278], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7267], grad_fn=<AddBackward0>)\n",
      "epoch 1400, loss with penalty 1.7260881662368774\n",
      "New min loss = tensor([1.7255], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7244], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7232], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7220], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7209], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7197], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7186], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7174], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7163], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7151], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7139], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7128], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7116], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7104], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7093], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7081], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7070], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7058], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7046], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7035], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7023], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7011], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7000], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6988], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6976], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6965], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6953], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6941], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6930], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6918], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6906], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6894], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6883], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6871], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6859], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6848], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6836], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6824], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6813], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6802], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6792], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6781], grad_fn=<AddBackward0>)\n",
      "epoch 1500, loss with penalty 3.4815750122070312\n",
      "New min loss = tensor([1.6576], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6533], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6447], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6417], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6383], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6341], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6303], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6290], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6272], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6252], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6236], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6222], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6209], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6199], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6188], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6173], grad_fn=<AddBackward0>)\n",
      "epoch 1600, loss with penalty 1.6173280477523804\n",
      "New min loss = tensor([1.6158], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6143], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6132], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6119], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6106], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6095], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6083], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6072], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6061], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6050], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6040], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6030], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6015], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6005], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5995], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5980], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5966], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5952], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5942], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5777], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5733], grad_fn=<AddBackward0>)\n",
      "epoch 1700, loss with penalty 1.6044752597808838\n",
      "New min loss = tensor([1.5635], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5623], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5605], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5550], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5525], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5491], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5478], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5453], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5438], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5420], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5408], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5396], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5386], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5369], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5356], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5344], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5331], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5317], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5306], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5296], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5282], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5270], grad_fn=<AddBackward0>)\n",
      "epoch 1800, loss with penalty 1.6773821115493774\n",
      "New min loss = tensor([1.5182], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5131], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4983], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4936], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4873], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4861], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4823], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4812], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4784], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4766], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4753], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4742], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4731], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4721], grad_fn=<AddBackward0>)\n",
      "epoch 1900, loss with penalty 1.4720531702041626\n",
      "New min loss = tensor([1.4709], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4697], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4686], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4674], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4571], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4544], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4339], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4292], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4255], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4235], grad_fn=<AddBackward0>)\n",
      "epoch 2000, loss with penalty 1.4388974905014038\n",
      "New min loss = tensor([1.4209], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4187], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4173], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4161], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4147], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4055], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3860], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3762], grad_fn=<AddBackward0>)\n",
      "epoch 2100, loss with penalty 1.4674245119094849\n",
      "New min loss = tensor([1.3707], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3697], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3666], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3645], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3634], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3389], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3308], grad_fn=<AddBackward0>)\n",
      "epoch 2200, loss with penalty 1.4766072034835815\n",
      "New min loss = tensor([1.3261], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3233], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3194], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3168], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3157], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2860], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2810], grad_fn=<AddBackward0>)\n",
      "epoch 2300, loss with penalty 1.2809641361236572\n",
      "New min loss = tensor([1.2759], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2737], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2719], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2701], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2690], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2523], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2399], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2362], grad_fn=<AddBackward0>)\n",
      "epoch 2400, loss with penalty 1.3862358331680298\n",
      "New min loss = tensor([1.2301], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2252], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2232], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2206], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2188], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2175], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2164], grad_fn=<AddBackward0>)\n",
      "epoch 2500, loss with penalty 2.292227268218994\n",
      "New min loss = tensor([1.1916], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1810], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1754], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1712], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1694], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1682], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1666], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1651], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1639], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1628], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1611], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1599], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1584], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1573], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1562], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1551], grad_fn=<AddBackward0>)\n",
      "epoch 2600, loss with penalty 2.2064766883850098\n",
      "New min loss = tensor([1.1487], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1455], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1392], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1373], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1236], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1208], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1197], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1176], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1140], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1127], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1102], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1090], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1074], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1061], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1049], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1034], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1023], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1010], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0995], grad_fn=<AddBackward0>)\n",
      "epoch 2700, loss with penalty 1.0990227460861206\n",
      "New min loss = tensor([1.0985], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0974], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0964], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0950], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0937], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0924], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0912], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0900], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0888], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0876], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0865], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0854], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0844], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0683], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0578], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0506], grad_fn=<AddBackward0>)\n",
      "epoch 2800, loss with penalty 1.0506023168563843\n",
      "New min loss = tensor([1.0478], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0453], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0435], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0423], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0400], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0376], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0363], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0352], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0336], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0320], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0310], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0297], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0284], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0274], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0261], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0249], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0239], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0228], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0216], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0204], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0193], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0181], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0170], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0158], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0147], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0136], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0125], grad_fn=<AddBackward0>)\n",
      "epoch 2900, loss with penalty 1.0121079683303833\n",
      "New min loss = tensor([1.0114], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0103], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0093], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0081], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9949], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9858], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9840], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9810], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9792], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9742], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9707], grad_fn=<AddBackward0>)\n",
      "epoch 3000, loss with penalty 0.9750572443008423\n",
      "New min loss = tensor([0.9671], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9644], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9631], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9619], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9603], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9588], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9578], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9566], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9552], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9539], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9526], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9516], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9503], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9489], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9479], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9468], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9458], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9448], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9437], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9427], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9414], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9402], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9390], grad_fn=<AddBackward0>)\n",
      "epoch 3100, loss with penalty 1.0454561710357666\n",
      "New min loss = tensor([0.9313], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9227], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9131], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9116], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9062], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9033], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9014], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8997], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8978], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8968], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8954], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8942], grad_fn=<AddBackward0>)\n",
      "epoch 3200, loss with penalty 0.8934171795845032\n",
      "New min loss = tensor([0.8929], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8915], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8904], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8893], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8882], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8870], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8857], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8844], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8832], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8819], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8807], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8796], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8784], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8772], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8761], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8750], grad_fn=<AddBackward0>)\n",
      "epoch 3300, loss with penalty 2.4636428356170654\n",
      "New min loss = tensor([0.8520], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8497], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8485], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8469], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8449], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8430], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8405], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8391], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8379], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8362], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8349], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8339], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8326], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8316], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8305], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8295], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8283], grad_fn=<AddBackward0>)\n",
      "epoch 3400, loss with penalty 0.8273327350616455\n",
      "New min loss = tensor([0.8271], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8259], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8249], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8239], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8226], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8214], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8202], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8190], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8178], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8166], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8155], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8144], grad_fn=<AddBackward0>)\n",
      "epoch 3500, loss with penalty 1.5007166862487793\n",
      "New min loss = tensor([0.7985], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7941], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7912], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7893], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7868], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7848], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7831], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7818], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7805], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7794], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7782], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7771], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7761], grad_fn=<AddBackward0>)\n",
      "epoch 3600, loss with penalty 0.7751200795173645\n",
      "New min loss = tensor([0.7749], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7738], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7727], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7716], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7705], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7694], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7684], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7673], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7662], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7571], grad_fn=<AddBackward0>)\n",
      "epoch 3700, loss with penalty 0.914794921875\n",
      "New min loss = tensor([0.7526], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7483], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7467], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7449], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7438], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7424], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7412], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7402], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7392], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7381], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7370], grad_fn=<AddBackward0>)\n",
      "epoch 3800, loss with penalty 0.7361591458320618\n",
      "New min loss = tensor([0.7359], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7348], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7337], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7326], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7316], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7305], grad_fn=<AddBackward0>)\n",
      "epoch 3900, loss with penalty 0.9264413714408875\n",
      "New min loss = tensor([0.7244], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7177], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7154], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7139], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7126], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7115], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7104], grad_fn=<AddBackward0>)\n",
      "epoch 4000, loss with penalty 0.7096464037895203\n",
      "New min loss = tensor([0.7094], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7083], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7073], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.7062], grad_fn=<AddBackward0>)\n",
      "epoch 4100, loss with penalty 1.4607884883880615\n",
      "New min loss = tensor([0.7011], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.6995], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.6973], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.6961], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.6951], grad_fn=<AddBackward0>)\n",
      "epoch 4200, loss with penalty 0.694731593132019\n",
      "New min loss = tensor([0.6940], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.6930], grad_fn=<AddBackward0>)\n",
      "epoch 4300, loss with penalty 1.6385579109191895\n",
      "New min loss = tensor([0.6886], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.6876], grad_fn=<AddBackward0>)\n",
      "epoch 4400, loss with penalty 0.6875481009483337\n",
      "epoch 4500, loss with penalty 0.6999588012695312\n",
      "epoch 4600, loss with penalty 0.6871045827865601\n",
      "New min loss = tensor([0.6866], grad_fn=<AddBackward0>)\n",
      "epoch 4700, loss with penalty 0.7348637580871582\n",
      "epoch 4800, loss with penalty 0.6887475252151489\n",
      "epoch 4900, loss with penalty 2.357280731201172\n",
      "epoch 5000, loss with penalty 0.6912472248077393\n",
      "epoch 5100, loss with penalty 0.7127482295036316\n",
      "epoch 5200, loss with penalty 0.6931499242782593\n",
      "epoch 5300, loss with penalty 0.7062498927116394\n",
      "epoch 5400, loss with penalty 0.7295711636543274\n",
      "epoch 5500, loss with penalty 0.7042393684387207\n",
      "epoch 5600, loss with penalty 1.2018542289733887\n",
      "epoch 5700, loss with penalty 0.7044175267219543\n",
      "epoch 5800, loss with penalty 0.8114418983459473\n",
      "epoch 5900, loss with penalty 0.7045528888702393\n",
      "epoch 6000, loss with penalty 0.7207284569740295\n",
      "epoch 6100, loss with penalty 0.7049854397773743\n",
      "epoch 6200, loss with penalty 0.716565728187561\n",
      "epoch 6300, loss with penalty 0.9982531070709229\n",
      "epoch 6400, loss with penalty 0.7091546654701233\n",
      "epoch 6500, loss with penalty 0.727881669998169\n",
      "epoch 6600, loss with penalty 0.7565985918045044\n",
      "epoch 6700, loss with penalty 0.7151188254356384\n",
      "epoch 6800, loss with penalty 0.8736652135848999\n",
      "epoch 6900, loss with penalty 0.7180933356285095\n",
      "epoch 7000, loss with penalty 0.743523120880127\n",
      "epoch 7100, loss with penalty 0.7111943364143372\n",
      "epoch 7200, loss with penalty 0.72292160987854\n",
      "epoch 7300, loss with penalty 0.7965385913848877\n",
      "epoch 7400, loss with penalty 0.8009674549102783\n",
      "epoch 7500, loss with penalty 0.7170149087905884\n",
      "epoch 7600, loss with penalty 0.7393018007278442\n",
      "epoch 7700, loss with penalty 0.835858166217804\n",
      "epoch 7800, loss with penalty 0.7210901975631714\n",
      "epoch 7900, loss with penalty 0.7644835114479065\n",
      "epoch 8000, loss with penalty 0.8483595252037048\n",
      "epoch 8100, loss with penalty 0.7142428159713745\n",
      "epoch 8200, loss with penalty 0.7212838530540466\n",
      "epoch 8300, loss with penalty 0.7378532290458679\n",
      "epoch 8400, loss with penalty 0.999087393283844\n",
      "epoch 8500, loss with penalty 0.7144775986671448\n",
      "epoch 8600, loss with penalty 0.7237231135368347\n",
      "epoch 8700, loss with penalty 0.7314237356185913\n",
      "epoch 8800, loss with penalty 0.7372325658798218\n",
      "epoch 8900, loss with penalty 0.7447563409805298\n",
      "epoch 9000, loss with penalty 0.8791618943214417\n",
      "epoch 9100, loss with penalty 0.9083489775657654\n",
      "epoch 9200, loss with penalty 1.267357349395752\n",
      "epoch 9300, loss with penalty 0.7286644577980042\n",
      "epoch 9400, loss with penalty 0.9515674114227295\n",
      "epoch 9500, loss with penalty 1.0200092792510986\n",
      "epoch 9600, loss with penalty 0.8172319531440735\n",
      "epoch 9700, loss with penalty 0.7445815205574036\n",
      "epoch 9800, loss with penalty 0.7455344796180725\n",
      "epoch 9900, loss with penalty 0.7347762584686279\n",
      "epoch 10000, loss with penalty 1.818958044052124\n",
      "epoch 10100, loss with penalty 0.9840108156204224\n",
      "epoch 10200, loss with penalty 0.7388947010040283\n",
      "epoch 10300, loss with penalty 0.7218140363693237\n",
      "epoch 10400, loss with penalty 0.7304911613464355\n",
      "epoch 10500, loss with penalty 1.7901477813720703\n",
      "epoch 10600, loss with penalty 0.8211188912391663\n",
      "epoch 10700, loss with penalty 0.822828471660614\n",
      "epoch 10800, loss with penalty 0.7561026811599731\n",
      "epoch 10900, loss with penalty 0.7304466366767883\n",
      "epoch 11000, loss with penalty 0.8134512901306152\n",
      "epoch 11100, loss with penalty 0.9963883757591248\n",
      "epoch 11200, loss with penalty 0.7506650686264038\n",
      "epoch 11300, loss with penalty 4.076167106628418\n",
      "epoch 11400, loss with penalty 0.7935187220573425\n",
      "epoch 11500, loss with penalty 0.7519885897636414\n",
      "epoch 11600, loss with penalty 1.036206841468811\n",
      "epoch 11700, loss with penalty 0.7722573280334473\n",
      "epoch 11800, loss with penalty 0.7410557270050049\n",
      "epoch 11900, loss with penalty 0.9307725429534912\n",
      "epoch 12000, loss with penalty 0.7498407363891602\n",
      "epoch 12100, loss with penalty 0.7705582976341248\n",
      "epoch 12200, loss with penalty 0.7665472626686096\n",
      "epoch 12300, loss with penalty 1.041934847831726\n",
      "epoch 12400, loss with penalty 0.740084171295166\n",
      "epoch 12500, loss with penalty 0.8184806704521179\n",
      "epoch 12600, loss with penalty 0.9528710842132568\n",
      "epoch 12700, loss with penalty 0.8131186366081238\n",
      "epoch 12800, loss with penalty 2.7151641845703125\n",
      "epoch 12900, loss with penalty 0.7448232769966125\n",
      "epoch 13000, loss with penalty 0.9199610948562622\n",
      "epoch 13100, loss with penalty 1.3516170978546143\n",
      "epoch 13200, loss with penalty 0.7562799453735352\n",
      "epoch 13300, loss with penalty 0.8130288124084473\n",
      "epoch 13400, loss with penalty 0.7848948836326599\n",
      "epoch 13500, loss with penalty 0.8391010165214539\n",
      "epoch 13600, loss with penalty 0.9005642533302307\n",
      "epoch 13700, loss with penalty 0.828502893447876\n",
      "epoch 13800, loss with penalty 1.4219520092010498\n",
      "epoch 13900, loss with penalty 0.8096230030059814\n",
      "epoch 14000, loss with penalty 0.8191113471984863\n",
      "epoch 14100, loss with penalty 1.1644861698150635\n",
      "epoch 14200, loss with penalty 0.7436397075653076\n",
      "epoch 14300, loss with penalty 0.8484447598457336\n",
      "epoch 14400, loss with penalty 0.7650958895683289\n",
      "epoch 14500, loss with penalty 0.7422696948051453\n",
      "epoch 14600, loss with penalty 0.8523133397102356\n",
      "epoch 14700, loss with penalty 1.1333210468292236\n",
      "epoch 14800, loss with penalty 0.794335663318634\n",
      "epoch 14900, loss with penalty 0.83436518907547\n",
      "epoch 15000, loss with penalty 0.9628129005432129\n",
      "epoch 15100, loss with penalty 0.9902850389480591\n"
     ]
    }
   ],
   "source": [
    "res_1_1_sigmoid = auto_test_documentation(model_11_sigmoid, adam_opt_11_sigmoid, p_fn1, \n",
    "                                      model_name='model_11_sigmoid_pfn1',\n",
    "                                      res_path=RESULT_PATH,\n",
    "                                      loss_fname='loss_11_sigmoid_pfn1',\n",
    "                                      loss_with_pen_fname = 'loss_pen_11_sigmoid_pfn1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values = np.linspace(L_BOUND, U_BOUND, N_POINTS)\n",
    "x_vals = [torch.tensor([i], requires_grad=True, dtype=torch.float) for i in np.linspace(L_BOUND, U_BOUND, N_POINTS)]\n",
    "\n",
    "model_output = model_on_interval(res_1_1_sin[3], DISCRETE_POINTS)\n",
    "\n",
    "plot_figure(x_values, model_output)\n",
    "# plot_figure(x_values, model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrbOIPSxK5LM"
   },
   "source": [
    "### The Finite Difference Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "qkg6Zfo7u8Fq"
   },
   "outputs": [],
   "source": [
    "def finite_diff_method(discrete_points, given_fn):\n",
    "    N_points = len(discrete_points)\n",
    "    h = discrete_points[1]-discrete_points[0]\n",
    "    c = np.sqrt(1/h)\n",
    "\n",
    "    H = np.zeros((N_points,N_points))\n",
    "    # Question: Why H has dimension (N-2) x (N-2)?\n",
    "    V = np.zeros((N_points,N_points))\n",
    "\n",
    "    for i in range (N_points):\n",
    "        for j in range (N_points):\n",
    "            if i == j:\n",
    "                H[i][j] = -2\n",
    "            elif abs(i-j) == 1:\n",
    "                H[i][j] = 1\n",
    "    for i in range (N_points):\n",
    "        for j in range (N_points):\n",
    "            if i == j:\n",
    "                V[i][j] = given_fn.forward(discrete_points[i])\n",
    "\n",
    "    A = -H/(h**2) + V\n",
    "    \n",
    "    eig_val, eig_vec = np.linalg.eig(A)\n",
    "    sorted_id_eig_val = np.argsort(eig_val)\n",
    "    # Get the indices that would sort eig_val\n",
    "    z = sorted_id_eig_val[0:1]\n",
    "    # eig_val[z] will return sorted first [T] value of eigenvalues.\n",
    "    energies = eig_val[z]/eig_val[z][0]\n",
    "\n",
    "    ground_state = c * eig_vec[:,z[0]]\n",
    "\n",
    "    return (eig_val[z], ground_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lIGR6Gohu8Fr"
   },
   "outputs": [],
   "source": [
    "def plot_finite_diff_sol(eig_val, ground_state, discrete_points, nn_model=None):\n",
    "    plt.figure(figsize=(12,10))\n",
    "    for i in range(len(eig_val)):\n",
    "        y = []\n",
    "        y = np.append(y, (-1) * ground_state)\n",
    "\n",
    "        plt.plot(discrete_points,y,lw=3, label=\"{} \".format(i))\n",
    "        plt.xlabel('x', size=14)\n",
    "        plt.ylabel('$\\psi$(x)',size=14)\n",
    "    if nn_model != None:\n",
    "        model_output = model_on_interval(nn_model, DISCRETE_POINTS)\n",
    "        plt.plot(discrete_points, model_output)\n",
    "    plt.legend()\n",
    "    plt.title('normalized wavefunctions for a harmonic oscillator using finite difference method',size=14)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "Xx7UHtNBK5LO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The FD method eigenvalue: 0.6143991441290746\n",
      "The NN method loss value: 0.8290562033653259\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eig_val, ground_state = finite_diff_method(DISCRETE_POINTS, p_fn1)\n",
    "loss_val = epsilon_Loss_penalty(p_fn1, res_1_1_sin[3], LAMBDA_PEN, U_BOUND, DISCRETE_POINTS)[0]\n",
    "print(\"The FD method eigenvalue: \" + str(eig_val[0]))\n",
    "print(\"The NN method loss value: \" + str(loss_val.item()))\n",
    "\n",
    "plot_finite_diff_sol(eig_val, ground_state, DISCRETE_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrbOIPSxK5LM"
   },
   "source": [
    "### Load Results for Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Data-running version.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
