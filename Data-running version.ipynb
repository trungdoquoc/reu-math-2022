{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "U3Z0FXavK5LD"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "# from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "id": "P5y5jqCDK5LG"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS FOR \n",
    "LAMBDA_PEN = 1000\n",
    "L_BOUND = -10\n",
    "U_BOUND = 10\n",
    "N_POINTS = 512\n",
    "\n",
    "NUM_EPOCHS = 20000\n",
    "\n",
    "M_POINTS = 10\n",
    "ALPHA = 2\n",
    "\n",
    "SELECTION_RATE = 0.01\n",
    "\n",
    "DISCRETE_POINTS = np.linspace(L_BOUND, U_BOUND, N_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "id": "03rjt3_6K5LG"
   },
   "outputs": [],
   "source": [
    "class sinFn(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(input):\n",
    "        return torch.sin(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "bIUFbmpwcCeA"
   },
   "outputs": [],
   "source": [
    "def model_on_interval(nn_model, discrete_points):\n",
    "    x_vals = [torch.tensor([i], requires_grad=True, dtype=torch.float) for i in discrete_points]\n",
    "\n",
    "    model_output = []\n",
    "    for i in x_vals:\n",
    "        model_output.append(nn_model(i).detach().numpy().item())\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "P1WpAhwrK5LI"
   },
   "outputs": [],
   "source": [
    "class X_Square_sin:\n",
    "    def __init__(self, const, use_sin=False):\n",
    "        self.const = const\n",
    "        if use_sin == True:\n",
    "            self.sin_const = np.random.choice(np.arange(10, 50))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x**2 + self.const + self.sin_const*math.sin(x)\n",
    "    \n",
    "class Potential_Function:\n",
    "    def __init__(self, c_0=0,\n",
    "                 M_points=M_POINTS,\n",
    "                 L_endpoint=U_BOUND, \n",
    "                 alpha=ALPHA, \n",
    "                 rescale=1):\n",
    "        self.M_points = M_points\n",
    "        self.L_endpoint = L_endpoint\n",
    "        self.alpha = alpha\n",
    "        self.c_0 = c_0\n",
    "        self.rescale = 1\n",
    "        \n",
    "        self.ti_list = np.random.normal(loc=0, scale=1.0, size= self.M_points)\n",
    "        self.ci_list = [(self.L_endpoint/(i * math.pi))**self.alpha \n",
    "                        for i in range(1, self.M_points+1)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        f_value = 0\n",
    "        summation = 0\n",
    "        \n",
    "        #Iterative method:\n",
    "\n",
    "        for i in range(1, self.M_points+1):\n",
    "            cos_val = np.cos((i * math.pi * x)/self.L_endpoint)\n",
    "            summation += self.ti_list[i-1] * self.ci_list[i-1] * cos_val\n",
    "\n",
    "        f_value += summation\n",
    "        f_value += self.c_0\n",
    "        return self.rescale * f_value\n",
    "    \n",
    "    def set_c0_value(self, val):\n",
    "        self.c_0 = val\n",
    "        return\n",
    "    \n",
    "    def set_rescale_factor(self, val):\n",
    "        self.rescale = val\n",
    "        return \n",
    "    \n",
    "    def plot_function(self, discrete_points):\n",
    "        y_values = [self.forward(i) for i in discrete_points]\n",
    "        plt.plot(discrete_points, y_values)\n",
    "    \n",
    "    def update_potential_fn(self, discrete_points):\n",
    "        # (1) Check lowest value to set appropriate c_0 value:\n",
    "        y_values = [self.forward(i) for i in discrete_points]\n",
    "        min_val = min(y_values)\n",
    "        print(\"Original min val = \" + str(min_val))\n",
    "        # plt.plot(DISCRETE_POINTS, y_values)\n",
    "\n",
    "        # (1) Set c_0 value s.t. every value in y_values is > 0: \n",
    "        if min_val < 0:\n",
    "            c_0 = math.ceil(abs(min_val))\n",
    "            self.set_c0_value(c_0)\n",
    "            new_y = [self.forward(i) for i in discrete_points]\n",
    "            print(\"Min val with c_0 updated = \" + str(min(new_y)))\n",
    "        # plt.plot(DISCRETE_POINTS, new_y)\n",
    "        else:\n",
    "            new_y = [self.forward(i) for i in discrete_points]\n",
    "            print(\"Min val with c_0 updated = \" + str(min(new_y)))\n",
    "        # (2) Rescale potential function s.t. every value lies between 0 and 5:\n",
    "        max_v = max(new_y)\n",
    "#         min_v = min(new_y)\n",
    "#         print(max_v, min_v)\n",
    "#         max_min = max_v-min_v\n",
    "\n",
    "#         rescaled_y = [(i-min_v)/(max_min) for i in new_y]\n",
    "#         plt.plot(DISCRETE_POINTS, rescaled_y)\n",
    "\n",
    "        c = 5/max_v\n",
    "        self.set_rescale_factor(c)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "id": "gwppbWioK5LJ"
   },
   "outputs": [],
   "source": [
    "def epsilon_Loss_penalty(v_x, model_u, \n",
    "                         lambda_pen,\n",
    "                         upper_bound, \n",
    "                         discrete_points):\n",
    "    eps_sum = 0\n",
    "    pen = 0\n",
    "\n",
    "    h = (2*upper_bound)/(len(discrete_points)-1)\n",
    "#     print(\"h = \"+ str(h))\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "\n",
    "        u_prime_square = torch.square(model_u.u_prime(x_i))\n",
    "        u_xi_square = torch.square(model_u(x_i))\n",
    "        \n",
    "        v_xi = v_x.forward(i)\n",
    "        if v_xi < 0:\n",
    "            raise ValueError('Potential Function value is < 0')\n",
    "\n",
    "        t = u_prime_square + v_xi*u_xi_square\n",
    "        eps_sum += t\n",
    "        \n",
    "        pen+= u_xi_square\n",
    "        \n",
    "    epsilon_fn = h*eps_sum\n",
    "    \n",
    "    penalty = lambda_pen * torch.square(h*pen-1)\n",
    "#     print(\"epsilon_fn value = \" + str(epsilon_fn))\n",
    "#     print(\"penalty value = \" + str(penalty))\n",
    "    return (epsilon_fn, epsilon_fn + penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "id": "clBtQzFcK5LK"
   },
   "outputs": [],
   "source": [
    "# CREATING MODEL CLASS\n",
    "class Nonlinear_1(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network with 1 layers\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_width, use_sin=False):\n",
    "        super(Nonlinear_1, self).__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = sinFn()\n",
    "        self.use_sin = use_sin\n",
    "#         self.tanh = nn.Tanh()\n",
    "\n",
    "        self.hidden = nn.Linear(1, layer_width)\n",
    "        self.output = nn.Linear(layer_width, 1)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if self.use_sin == True:\n",
    "            x = self.hidden(x)\n",
    "            x = self.sin(x)\n",
    "            x = self.output(x)\n",
    "        else:\n",
    "            x = self.hidden(x)\n",
    "            x = self.sigmoid(x)\n",
    "            x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def normalize_model(self, discrete_points):\n",
    "        \"\"\"\n",
    "        GOAL: Normalize the output weight layer\n",
    "        model.output *= c\n",
    "        where,\n",
    "        scalar c = 1/denom\n",
    "        \"\"\"\n",
    "        \n",
    "        h = discrete_points[1]-discrete_points[0]\n",
    "        s = 0\n",
    "        for i in discrete_points:\n",
    "            x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "            s += self(x_i)**2\n",
    "        denom = math.sqrt(h) * torch.sqrt(s)\n",
    "        c = 1/denom\n",
    "\n",
    "        print(\"Before normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        \n",
    "        self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
    "        self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
    "\n",
    "        print(\"After normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        print(\"c value = \" + str(c))\n",
    "\n",
    "        return \n",
    "\n",
    "    def u_prime(self, x_in):\n",
    "        y = self(x_in)\n",
    "        y_prime = torch.autograd.grad(y.sum(), x_in, create_graph=True)\n",
    "        return y_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "id": "i0X45z26K5LJ"
   },
   "outputs": [],
   "source": [
    "class Nonlinear_2(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network with 2 layers\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_width, use_sin=False):\n",
    "        super(Nonlinear_2, self).__init__()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = sinFn()\n",
    "#         self.tanh = nn.Tanh()\n",
    "        self.use_sin = use_sin\n",
    "        \n",
    "        self.hidden1 = nn.Linear(1, layer_width)\n",
    "        self.hidden2 = nn.Linear(layer_width, layer_width)\n",
    "        self.output = nn.Linear(layer_width, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.use_sin == True:\n",
    "            x = self.hidden1(x)\n",
    "            x = self.sin(x)\n",
    "            \n",
    "            x = self.hidden2(x)\n",
    "            x = self.sin(x)\n",
    "            \n",
    "            x = self.output(x)\n",
    "            \n",
    "        else:\n",
    "            x = self.hidden1(x)\n",
    "            x = self.sigmoid(x)\n",
    "            \n",
    "            x = self.hidden2(x)\n",
    "            x = self.sigmoid(x)\n",
    "            \n",
    "            x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def normalize_model(self, discrete_points):\n",
    "        \"\"\"\n",
    "        GOAL: Normalize the output weight layer\n",
    "        model.output *= c\n",
    "        where,\n",
    "        scalar c = 1/denom\n",
    "        \"\"\"\n",
    "        \n",
    "        h = discrete_points[1]-discrete_points[0]\n",
    "        s = 0\n",
    "        for i in discrete_points:\n",
    "            x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "            s += self(x_i)**2\n",
    "        denom = math.sqrt(h) * torch.sqrt(s)\n",
    "        c = 1/denom\n",
    "\n",
    "        print(\"Before normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        \n",
    "        self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
    "        self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
    "\n",
    "        print(\"After normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        print(\"c value = \" + str(c))\n",
    "\n",
    "        return \n",
    "\n",
    "    def u_prime(self, x_in):\n",
    "        y = self(x_in)\n",
    "        y_prime = torch.autograd.grad(y.sum(), x_in, create_graph=True)\n",
    "        return y_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yehv8TrK5LL"
   },
   "outputs": [],
   "source": [
    "# TRANING MODEL\n",
    "def batch_train_with_penalty(model, \n",
    "                         num_epochs, \n",
    "                         v_x, \n",
    "                         optimizer, \n",
    "                         lambda_pen, \n",
    "                         discrete_points):\n",
    "    # For plotting loss value over epochs:\n",
    "    x_epochs = []\n",
    "    y_loss = []\n",
    "    y_loss_pen = []\n",
    "\n",
    "    #Early stopping criteria:\n",
    "    last_min_loss = 0\n",
    "    min_delta = 1e-3 #minimum difference to be considered better loss\n",
    "    best_model = None\n",
    "    \n",
    "    now = datetime.datetime.now(tz=timezone('US/Eastern'))\n",
    "    print(\"Start time is \" + str(now))\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss, loss_pen = epsilon_Loss_penalty(v_x, model, lambda_pen,\n",
    "                                    U_BOUND, discrete_points)\n",
    "\n",
    "        y_loss_pen.append(loss_pen.detach().numpy().item())\n",
    "        y_loss.append(loss.detach().numpy().item())\n",
    "        x_epochs.append(epoch)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            print(\"Initiate first min loss = \" + str(loss_pen))\n",
    "            best_model = copy.deepcopy(model)\n",
    "            last_min_loss = loss_pen\n",
    "        \n",
    "        # Save best model\n",
    "        if loss_pen < last_min_loss and abs(loss_pen-last_min_loss) >= min_delta:\n",
    "            print(\"New min loss = \" + str(loss_pen))\n",
    "            best_model = copy.deepcopy(model)\n",
    "            last_min_loss = loss_pen\n",
    "\n",
    "        if epoch % 100 == 0 or (epoch+1)%100 == 0:\n",
    "            print('epoch {}, loss with penalty {}'.format(epoch, loss_pen.item()))\n",
    "        loss_pen.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    end = time.time()\n",
    "    time1 = end-start\n",
    "    print(\"Using potential function defined as class takes \" + str(time1))\n",
    "\n",
    "    model.normalize_model(DISCRETE_POINTS)\n",
    "    best_model.normalize_model(DISCRETE_POINTS)\n",
    "    return (x_epochs, y_loss, y_loss_pen, best_model)\n",
    "\n",
    "\n",
    "def minibatch_train_with_penalty(model, \n",
    "                               num_epochs, \n",
    "                               v_x, \n",
    "                               optimizer, \n",
    "                               lambda_pen,\n",
    "                               discrete_points,\n",
    "                               batch_size=32):\n",
    "    # For plotting loss value over epochs:\n",
    "    x_epochs = []\n",
    "    y_loss = []\n",
    "    y_loss_pen = []\n",
    "    \n",
    "    #Early stopping criteria:\n",
    "    last_min_loss = 0\n",
    "    min_delta = 1e-3 #minimum difference to be considered better loss\n",
    "    best_model = None\n",
    "\n",
    "    m = len(discrete_points)\n",
    "    num_batches = int(m/batch_size)\n",
    "#         print(\"Number of batches \" + str(num_batches))\n",
    "    \n",
    "    now = datetime.datetime.now(tz=timezone('US/Eastern'))\n",
    "    print(\"Start time is \" + str(now))\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        perm = torch.randperm(m)\n",
    "        avg_loss_pen = 0\n",
    "        avg_loss = 0\n",
    "        \n",
    "        for i in range(0, m, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            indices = perm[i: i+batch_size]\n",
    "\n",
    "            loss, loss_pen = epsilon_Loss_penalty(v_x, model, \n",
    "                                                  lambda_pen,\n",
    "                                                  U_BOUND, \n",
    "                                                  discrete_points[indices])\n",
    "            \n",
    "            if epoch == 0 and i == 0:\n",
    "                print(\"Initiate first min loss = \" + str(loss_pen))\n",
    "                best_model = copy.deepcopy(model)\n",
    "                last_min_loss = loss_pen\n",
    "                \n",
    "            avg_loss_pen += loss_pen.item()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            # Save best model\n",
    "            if loss_pen < last_min_loss and abs(loss_pen-last_min_loss) >= min_delta:\n",
    "                print(\"New min loss = \" + str(loss_pen))\n",
    "                best_model = copy.deepcopy(model)\n",
    "                last_min_loss = loss_pen\n",
    "            \n",
    "            loss_pen.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss_pen = avg_loss_pen/num_batches\n",
    "        avg_loss = avg_loss/num_batches\n",
    "\n",
    "        y_loss_pen.append(avg_loss_pen)\n",
    "        y_loss.append(avg_loss)\n",
    "        x_epochs.append(epoch)\n",
    "        \n",
    "        if epoch % 100 == 0 or (epoch+1)%100 == 0:\n",
    "            print('epoch {}, loss with penalty {}'.format(epoch, avg_loss_pen))\n",
    "    \n",
    "    end = time.time()\n",
    "    time1 = end-start\n",
    "    print(\"Using potential function defined as class takes \" + str(time1))\n",
    "    \n",
    "    model.normalize_model(DISCRETE_POINTS)\n",
    "    best_model.normalize_model(DISCRETE_POINTS)\n",
    "    return (x_epochs, y_loss, y_loss_pen, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH = './model_results/'\n",
    "p_fn_fnames = {p_fn1: 'p_fn1',\n",
    "               p_fn2: 'p_fn2',\n",
    "               p_fn3: 'p_fn3',\n",
    "               p_fn4: 'p_fn4',\n",
    "               p_fn5: 'p_fn5'}\n",
    "model_1_1_sin_info = {'model_fname': 'model_1_1_sin'}\n",
    "model_1_1_sigmoid= 'model_1_1_sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_p_fn(res_path, p_fn_dict: dict):\n",
    "    \"\"\"\n",
    "    Save a dict of p_fn as fixed files, \n",
    "    when need to train p_fn, just load these p_fn saved files. \n",
    "    \"\"\"\n",
    "    for p in p_fn_dict:\n",
    "        p_fn_fpath = res_path + p_fn_dict[p]\n",
    "        with open(p_fn_fpath, \"wb\") as fp:\n",
    "            pickle.dump(p, fp)\n",
    "    return\n",
    "\n",
    "def load_p_fn(p_fn_fpath: str):\n",
    "    # TODO\n",
    "    with open(p_fn_fpath, \"rb\") as fp:\n",
    "        p_fn = pickle.load(fp)\n",
    "    return p_fn\n",
    "\n",
    "def auto_test_documentation(model, optimizer, p_fn, \n",
    "                            model_name: str, res_path: str,\n",
    "                            loss_fname: str, loss_with_pen_fname: str):\n",
    "    \"\"\"\n",
    "    Automate training model, then save its results for future reference. \n",
    "    \n",
    "    \"\"\"\n",
    "    loss_fpath = res_path + loss_fname\n",
    "    loss_with_pen_fpath = res_path + loss_with_pen_fname\n",
    "    model_fpath = res_path + model_name\n",
    "    x_epochs, y_loss, y_loss_pen, best_model = batch_train_with_penalty(model, \n",
    "                                                                        NUM_EPOCHS, \n",
    "                                                                        p_fn, \n",
    "                                                                        optimizer, \n",
    "                                                                        LAMBDA_PEN, \n",
    "                                                                        DISCRETE_POINTS)\n",
    "    \n",
    "    #: store loss & loss with penalty value in a file type\n",
    "    with open(loss_fpath, \"wb\") as fp:\n",
    "        pickle.dump(y_loss, fp) #use: with open() as fp: pickle.load(fp) to load file\n",
    "    \n",
    "    with open(loss_with_pen_fpath, \"wb\") as fp:\n",
    "        pickle.dump(y_loss_pen, fp)\n",
    "        \n",
    "    #: store best_model from training\n",
    "    torch.save(best_model.state_dict(), model_fpath)\n",
    "    \n",
    "    ### How to load model from saved file\n",
    "    # load_model = MODEL_CLASS(...)\n",
    "    # load_model.load_state_dict(torch.load(f_path))\n",
    "    ###\n",
    "    \n",
    "    #: store eigenvalue + corresponding eigenfunction using FD method\n",
    "#     eig_val, ground_state = finite_diff_method(DISCRETE_POINTS, p_fn)\n",
    "#     with open(eig_val_fpath, \"wb\") as fp:\n",
    "#         pickle.dump(l, fp)\n",
    "        \n",
    "#     with open(ground_state_fpath, \"rb\") as fp:\n",
    "#         pickle.dump(l, fp)\n",
    "        \n",
    "    return x_epochs, y_loss, y_loss_pen, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "5O3XwauEK5LH"
   },
   "outputs": [],
   "source": [
    "# PLOT DATA\n",
    "def plot_figure(x_val, y_val, x_test=None, predicted=None, log_scale=False):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 6), dpi=80)\n",
    "    if log_scale==True:\n",
    "        plt.yscale('log')\n",
    "    plt.plot(x_val, y_val, '--', label='True data', alpha=0.5)\n",
    "    if predicted != None:\n",
    "        plt.plot(x_test, predicted, '--', label='Predictions', alpha=0.5)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_11_sin\n",
    "del model_11_sigmoid\n",
    "\n",
    "del adam_opt_11_sin\n",
    "del adam_opt_11_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "id": "8Ph_ZvmuK5LL"
   },
   "outputs": [],
   "source": [
    "learningRate = 0.008\n",
    "\n",
    "#INIT Nonlinear 1 MODELS\n",
    "model_11_sin = Nonlinear_1(20, use_sin=True)\n",
    "model_11_sigmoid = Nonlinear_1(20)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_11_sin.cuda()\n",
    "    model_11_sigmoid.cuda()\n",
    "\n",
    "# INIT OPTIMIZER CLASS\n",
    "# Adam:\n",
    "adam_opt_11_sin = torch.optim.Adam(model_11_sin.parameters(), \n",
    "                                    lr=learningRate, \n",
    "                                    betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, \n",
    "                                    weight_decay=0, \n",
    "                                    amsgrad=False)\n",
    "\n",
    "adam_opt_11_sigmoid = torch.optim.Adam(model_11_sigmoid.parameters(), \n",
    "                                    lr=learningRate, \n",
    "                                    betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, \n",
    "                                    weight_decay=0, \n",
    "                                    amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INIT Nonlinear 2 MODELS\n",
    "model_21_sin = Nonlinear_2(layer_width=20, use_sin=True)\n",
    "model_21_sigmoid = Nonlinear_2(layer_width=20)\n",
    "\n",
    "# INIT OPTIMIZER CLASS\n",
    "# Adam:\n",
    "adam_opt_21_sin = torch.optim.Adam(model_21_sin.parameters(), \n",
    "                                    lr=learningRate, \n",
    "                                    betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, \n",
    "                                    weight_decay=0, \n",
    "                                    amsgrad=False)\n",
    "\n",
    "adam_opt_21_sigmoid = torch.optim.Adam(model_21_sigmoid.parameters(), \n",
    "                                    lr=learningRate, \n",
    "                                    betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, \n",
    "                                    weight_decay=0, \n",
    "                                    amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "88EiIGL3cvoA",
    "outputId": "7865ac7f-7d7b-4529-a045-6bfea26c1775"
   },
   "source": [
    "# INIT POTENTIAL FUNCTIONS\n",
    "p_fn1 = Potential_Function(M_points=5)\n",
    "p_fn1.update_potential_fn(DISCRETE_POINTS)\n",
    "p_fn1.plot_function(DISCRETE_POINTS)\n",
    "\n",
    "p_fn2 = Potential_Function(M_points=5)\n",
    "p_fn2.update_potential_fn(DISCRETE_POINTS)\n",
    "p_fn2.plot_function(DISCRETE_POINTS)\n",
    "\n",
    "p_fn3 = Potential_Function(M_points=5)\n",
    "p_fn3.update_potential_fn(DISCRETE_POINTS)\n",
    "p_fn3.plot_function(DISCRETE_POINTS)\n",
    "\n",
    "p_fn4 = Potential_Function(M_points=5)\n",
    "p_fn4.update_potential_fn(DISCRETE_POINTS)\n",
    "p_fn4.plot_function(DISCRETE_POINTS)\n",
    "\n",
    "p_fn5 = Potential_Function()\n",
    "p_fn5.update_potential_fn(DISCRETE_POINTS)\n",
    "p_fn5.plot_function(DISCRETE_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time is 2022-08-04 19:55:11.280743-04:00\n",
      "epoch 0, loss with penalty 2891.668212890625\n",
      "epoch 99, loss with penalty 2.352030038833618\n",
      "epoch 100, loss with penalty 2.3406336307525635\n",
      "New min loss = tensor([1.9986], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9940], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9894], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9848], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9802], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9756], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9710], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9664], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9618], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9572], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9527], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9481], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9436], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9390], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9345], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9300], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9254], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9209], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9164], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9119], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9074], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9030], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8985], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8941], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8896], grad_fn=<AddBackward0>)\n",
      "epoch 199, loss with penalty 1.8896101713180542\n",
      "New min loss = tensor([1.8852], grad_fn=<AddBackward0>)\n",
      "epoch 200, loss with penalty 1.8851759433746338\n",
      "New min loss = tensor([1.8807], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8763], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8719], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8675], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8631], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8588], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8544], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8501], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8457], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8414], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8371], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8328], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8285], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8242], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8199], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8157], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8114], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8072], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8030], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7988], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7946], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7904], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7863], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7821], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7780], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7738], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7697], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7656], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7615], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7575], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7534], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7494], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7453], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7413], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7373], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7333], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7294], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7254], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7215], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7176], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7136], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7098], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7059], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7020], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6982], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6943], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6905], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6867], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6829], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6791], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6754], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6716], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6679], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6642], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6605], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6568], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6531], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6495], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6459], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6422], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6386], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6351], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6315], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6279], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6244], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6209], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6174], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6139], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6104], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6069], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6035], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6001], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5967], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5933], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5899], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5865], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5832], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5798], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5765], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5732], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5699], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5667], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5634], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5602], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5570], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5538], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5506], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5474], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5442], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5411], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5380], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5349], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5318], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5287], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5256], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5226], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5196], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5165], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5135], grad_fn=<AddBackward0>)\n",
      "epoch 299, loss with penalty 1.5135400295257568\n",
      "New min loss = tensor([1.5106], grad_fn=<AddBackward0>)\n",
      "epoch 300, loss with penalty 1.5105551481246948\n",
      "New min loss = tensor([1.5076], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5046], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5017], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4988], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4959], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4930], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4901], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4872], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4844], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4816], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4787], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4759], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4732], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4704], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4676], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4649], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4622], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4594], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4567], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4541], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4514], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4487], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4461], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4435], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4409], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4383], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4357], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4331], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4306], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4280], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4255], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4230], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4205], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4180], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4156], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4131], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4107], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4082], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4058], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4034], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4010], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3986], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3963], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3939], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3916], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3893], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3870], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3847], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3824], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3801], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3779], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3756], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3734], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3712], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3690], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3668], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3646], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3624], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3603], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3581], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3560], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3539], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3518], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3497], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3476], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3455], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3435], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3414], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3394], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3374], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3353], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3333], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3314], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3294], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3274], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3254], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3235], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3216], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3196], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3177], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3158], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3139], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3121], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3102], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3083], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3065], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3047], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3028], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3010], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2992], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2974], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2956], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2938], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2921], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2903], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2886], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2868], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2851], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2834], grad_fn=<AddBackward0>)\n",
      "epoch 399, loss with penalty 1.283406376838684\n",
      "New min loss = tensor([1.2817], grad_fn=<AddBackward0>)\n",
      "epoch 400, loss with penalty 1.281699299812317\n",
      "New min loss = tensor([1.2800], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2783], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2766], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2750], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2733], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2717], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2700], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2684], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2668], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2652], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2636], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2620], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2604], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2588], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2572], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2557], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2541], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2526], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2511], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2495], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2480], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2465], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2450], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2435], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2420], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2406], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2391], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2376], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2362], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2347], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2333], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2319], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2305], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2291], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2277], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2263], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2249], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2235], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2221], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2208], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2194], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2180], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2167], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2154], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2140], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2127], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2114], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2101], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2088], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2075], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2062], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2049], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2037], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2024], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2011], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1999], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1986], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1974], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1962], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1949], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1937], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1925], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1913], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1901], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1889], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1877], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1865], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1854], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1842], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1830], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1819], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1807], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1796], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1784], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1773], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1762], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1751], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1739], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1728], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1717], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1706], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1695], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1684], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1674], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1663], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1652], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1641], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1631], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1620], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1610], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1599], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1589], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1579], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1568], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1558], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1548], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1538], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1528], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1518], grad_fn=<AddBackward0>)\n",
      "epoch 499, loss with penalty 1.1517555713653564\n",
      "epoch 500, loss with penalty 1.1507563591003418\n",
      "New min loss = tensor([1.1498], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1478], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1458], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1439], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1420], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1401], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1382], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1363], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1345], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1327], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1309], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1291], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1273], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1255], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1238], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1221], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1203], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1186], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1170], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1153], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1137], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1120], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1104], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1088], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1072], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1056], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1041], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1025], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1010], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0995], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0980], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0965], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0950], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0935], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0921], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0906], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0892], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0878], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0864], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0850], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0836], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0822], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0809], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0795], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0782], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0769], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0755], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0742], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0729], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0717], grad_fn=<AddBackward0>)\n",
      "epoch 599, loss with penalty 1.0716614723205566\n",
      "epoch 600, loss with penalty 1.071025013923645\n",
      "New min loss = tensor([1.0704], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0691], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0679], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0666], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0654], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0642], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0630], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0618], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0606], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0594], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0583], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0571], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0559], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0548], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0537], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0525], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0514], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0503], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0492], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0481], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0471], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0460], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0449], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0439], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0428], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0418], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0408], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0397], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0387], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0377], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0362], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0348], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0333], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0319], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0304], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0290], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0276], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0263], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0249], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0236], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0222], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0209], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0196], grad_fn=<AddBackward0>)\n",
      "epoch 699, loss with penalty 1.0192128419876099\n",
      "epoch 700, loss with penalty 1.0187851190567017\n",
      "New min loss = tensor([1.0184], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0171], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0158], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0146], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0134], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0122], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0110], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0098], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0086], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0075], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0064], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0052], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0041], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0030], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0019], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0009], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9998], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9988], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9977], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9967], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9957], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9944], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9931], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9918], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9905], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9893], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9880], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9868], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9856], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9845], grad_fn=<AddBackward0>)\n",
      "epoch 799, loss with penalty 0.9838946461677551\n",
      "epoch 800, loss with penalty 0.9836071133613586\n",
      "New min loss = tensor([0.9833], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9822], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9811], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9800], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9789], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9779], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9768], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9758], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9748], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9735], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9723], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9711], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9700], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9688], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9677], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9666], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9656], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9645], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9635], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9623], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9611], grad_fn=<AddBackward0>)\n",
      "epoch 899, loss with penalty 0.9603880047798157\n",
      "epoch 900, loss with penalty 0.9601994156837463\n",
      "New min loss = tensor([0.9600], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9589], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9578], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9568], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9558], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9546], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9535], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9524], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9513], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9503], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9492], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9481], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9470], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9460], grad_fn=<AddBackward0>)\n",
      "epoch 999, loss with penalty 0.9451419115066528\n",
      "epoch 1000, loss with penalty 0.9450214505195618\n",
      "New min loss = tensor([0.9449], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9438], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9428], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9418], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9407], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9396], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9386], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9376], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9365], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9355], grad_fn=<AddBackward0>)\n",
      "epoch 1099, loss with penalty 0.9354391694068909\n",
      "epoch 1100, loss with penalty 0.9353623390197754\n",
      "New min loss = tensor([0.9345], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9334], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9324], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9313], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9303], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9293], grad_fn=<AddBackward0>)\n",
      "epoch 1199, loss with penalty 0.929240882396698\n",
      "epoch 1200, loss with penalty 0.929191529750824\n",
      "New min loss = tensor([0.9283], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9273], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9262], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9252], grad_fn=<AddBackward0>)\n",
      "epoch 1299, loss with penalty 0.9251312017440796\n",
      "epoch 1300, loss with penalty 0.9250974059104919\n",
      "New min loss = tensor([0.9242], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9232], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9222], grad_fn=<AddBackward0>)\n",
      "epoch 1399, loss with penalty 0.9222118854522705\n",
      "epoch 1400, loss with penalty 0.9221864938735962\n",
      "New min loss = tensor([0.9212], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9202], grad_fn=<AddBackward0>)\n",
      "epoch 1499, loss with penalty 0.91996169090271\n",
      "epoch 1500, loss with penalty 0.9199408292770386\n",
      "New min loss = tensor([0.9192], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9182], grad_fn=<AddBackward0>)\n",
      "epoch 1599, loss with penalty 0.9181035161018372\n",
      "epoch 1600, loss with penalty 0.918085515499115\n",
      "New min loss = tensor([0.9172], grad_fn=<AddBackward0>)\n",
      "epoch 1699, loss with penalty 0.9165006875991821\n",
      "epoch 1700, loss with penalty 0.9164857864379883\n",
      "New min loss = tensor([0.9162], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9152], grad_fn=<AddBackward0>)\n",
      "epoch 1799, loss with penalty 0.9150952696800232\n",
      "epoch 1800, loss with penalty 0.915081799030304\n",
      "epoch 1899, loss with penalty 0.9888497591018677\n",
      "epoch 1900, loss with penalty 1.005382776260376\n",
      "New min loss = tensor([0.9140], grad_fn=<AddBackward0>)\n",
      "epoch 1999, loss with penalty 0.9130911231040955\n",
      "epoch 2000, loss with penalty 0.9130780100822449\n",
      "New min loss = tensor([0.9130], grad_fn=<AddBackward0>)\n",
      "epoch 2099, loss with penalty 1.2679404020309448\n",
      "epoch 2100, loss with penalty 1.1718471050262451\n",
      "New min loss = tensor([0.9120], grad_fn=<AddBackward0>)\n",
      "epoch 2199, loss with penalty 0.9117251038551331\n",
      "epoch 2200, loss with penalty 0.9117183089256287\n",
      "epoch 2299, loss with penalty 1.5452477931976318\n",
      "epoch 2300, loss with penalty 1.7068958282470703\n",
      "New min loss = tensor([0.9110], grad_fn=<AddBackward0>)\n",
      "epoch 2399, loss with penalty 0.9108582735061646\n",
      "epoch 2400, loss with penalty 0.9108381867408752\n",
      "epoch 2499, loss with penalty 1.4877026081085205\n",
      "epoch 2500, loss with penalty 1.8366577625274658\n",
      "epoch 2599, loss with penalty 0.910921037197113\n",
      "epoch 2600, loss with penalty 0.9109049439430237\n",
      "New min loss = tensor([0.9100], grad_fn=<AddBackward0>)\n",
      "epoch 2699, loss with penalty 0.9096839427947998\n",
      "epoch 2700, loss with penalty 0.909677267074585\n",
      "epoch 2799, loss with penalty 0.932019054889679\n",
      "epoch 2800, loss with penalty 0.9163126349449158\n",
      "epoch 2899, loss with penalty 0.9095278978347778\n",
      "epoch 2900, loss with penalty 0.9095209836959839\n",
      "epoch 2999, loss with penalty 1.0393751859664917\n",
      "epoch 3000, loss with penalty 1.8968558311462402\n",
      "epoch 3099, loss with penalty 0.9100040197372437\n",
      "epoch 3100, loss with penalty 0.9100126624107361\n",
      "epoch 3199, loss with penalty 0.9090648293495178\n",
      "epoch 3200, loss with penalty 0.9090580940246582\n",
      "New min loss = tensor([0.9089], grad_fn=<AddBackward0>)\n",
      "epoch 3299, loss with penalty 0.9202479124069214\n",
      "epoch 3300, loss with penalty 0.9228726625442505\n",
      "epoch 3399, loss with penalty 0.9093857407569885\n",
      "epoch 3400, loss with penalty 0.9093757271766663\n",
      "epoch 3499, loss with penalty 0.9102258086204529\n",
      "epoch 3500, loss with penalty 0.9110043048858643\n",
      "epoch 3599, loss with penalty 0.9101914763450623\n",
      "epoch 3600, loss with penalty 0.9104750156402588\n",
      "epoch 3699, loss with penalty 0.9088660478591919\n",
      "epoch 3700, loss with penalty 0.9088602662086487\n",
      "epoch 3799, loss with penalty 0.9586214423179626\n",
      "epoch 3800, loss with penalty 1.046484112739563\n",
      "epoch 3899, loss with penalty 0.9093133807182312\n",
      "epoch 3900, loss with penalty 0.9093000292778015\n",
      "epoch 3999, loss with penalty 0.9085379242897034\n",
      "epoch 4000, loss with penalty 0.9085407853126526\n",
      "epoch 4099, loss with penalty 0.9109049439430237\n",
      "epoch 4100, loss with penalty 0.9121005535125732\n",
      "epoch 4199, loss with penalty 0.9088535308837891\n",
      "epoch 4200, loss with penalty 0.9088454842567444\n",
      "epoch 4299, loss with penalty 2.6648826599121094\n",
      "epoch 4300, loss with penalty 1.85884428024292\n",
      "epoch 4399, loss with penalty 0.9092894196510315\n",
      "epoch 4400, loss with penalty 0.9092650413513184\n",
      "epoch 4499, loss with penalty 0.9083544015884399\n",
      "epoch 4500, loss with penalty 0.9083473682403564\n",
      "epoch 4599, loss with penalty 0.9209098815917969\n",
      "epoch 4600, loss with penalty 0.9357674717903137\n",
      "epoch 4699, loss with penalty 0.9088664054870605\n",
      "epoch 4700, loss with penalty 0.9088568687438965\n",
      "epoch 4799, loss with penalty 0.9080862402915955\n",
      "epoch 4800, loss with penalty 0.9080790281295776\n",
      "epoch 4899, loss with penalty 0.931796669960022\n",
      "epoch 4900, loss with penalty 0.9163480997085571\n",
      "epoch 4999, loss with penalty 0.9081318974494934\n",
      "epoch 5000, loss with penalty 0.9081237316131592\n",
      "New min loss = tensor([0.9079], grad_fn=<AddBackward0>)\n",
      "epoch 5099, loss with penalty 0.9507496953010559\n",
      "epoch 5100, loss with penalty 0.9804693460464478\n",
      "epoch 5199, loss with penalty 0.9089431166648865\n",
      "epoch 5200, loss with penalty 0.9086980819702148\n",
      "epoch 5299, loss with penalty 0.9074345827102661\n",
      "epoch 5300, loss with penalty 0.9074267148971558\n",
      "epoch 5399, loss with penalty 1.7834630012512207\n",
      "epoch 5400, loss with penalty 3.0921363830566406\n",
      "epoch 5499, loss with penalty 0.9077563881874084\n",
      "epoch 5500, loss with penalty 0.9077181220054626\n",
      "New min loss = tensor([0.9069], grad_fn=<AddBackward0>)\n",
      "epoch 5599, loss with penalty 0.9067666530609131\n",
      "epoch 5600, loss with penalty 0.9067586660385132\n",
      "epoch 5699, loss with penalty 0.9237965941429138\n",
      "epoch 5700, loss with penalty 0.9734729528427124\n",
      "epoch 5799, loss with penalty 0.9066007733345032\n",
      "epoch 5800, loss with penalty 0.906589686870575\n",
      "epoch 5899, loss with penalty 0.9756748676300049\n",
      "epoch 5900, loss with penalty 1.0225238800048828\n",
      "epoch 5999, loss with penalty 0.9071619510650635\n",
      "epoch 6000, loss with penalty 0.9070578813552856\n",
      "New min loss = tensor([0.9059], grad_fn=<AddBackward0>)\n",
      "epoch 6099, loss with penalty 0.9058030247688293\n",
      "epoch 6100, loss with penalty 0.905796468257904\n",
      "epoch 6199, loss with penalty 1.0351331233978271\n",
      "epoch 6200, loss with penalty 0.9779112935066223\n",
      "epoch 6299, loss with penalty 0.9057040214538574\n",
      "epoch 6300, loss with penalty 0.9056961536407471\n",
      "epoch 6399, loss with penalty 2.371699094772339\n",
      "epoch 6400, loss with penalty 2.9250125885009766\n",
      "epoch 6499, loss with penalty 0.9058167934417725\n",
      "epoch 6500, loss with penalty 0.9057512283325195\n",
      "New min loss = tensor([0.9049], grad_fn=<AddBackward0>)\n",
      "epoch 6599, loss with penalty 0.9048740863800049\n",
      "epoch 6600, loss with penalty 0.9048715233802795\n",
      "epoch 6699, loss with penalty 0.9063808917999268\n",
      "epoch 6700, loss with penalty 0.905846357345581\n",
      "epoch 6799, loss with penalty 0.9047545790672302\n",
      "epoch 6800, loss with penalty 0.9047479629516602\n",
      "epoch 6899, loss with penalty 0.9299063086509705\n",
      "epoch 6900, loss with penalty 0.9319600462913513\n",
      "epoch 6999, loss with penalty 0.9047027826309204\n",
      "epoch 7000, loss with penalty 0.9046952128410339\n",
      "epoch 7099, loss with penalty 0.9331467151641846\n",
      "epoch 7100, loss with penalty 0.9286311864852905\n",
      "epoch 7199, loss with penalty 0.9045237302780151\n",
      "epoch 7200, loss with penalty 0.9045140743255615\n",
      "epoch 7299, loss with penalty 0.9253898859024048\n",
      "epoch 7300, loss with penalty 1.0036779642105103\n",
      "epoch 7399, loss with penalty 0.9046831727027893\n",
      "epoch 7400, loss with penalty 0.9046669006347656\n",
      "epoch 7499, loss with penalty 0.9055923223495483\n",
      "epoch 7500, loss with penalty 0.9065567255020142\n",
      "epoch 7599, loss with penalty 0.9049835801124573\n",
      "epoch 7600, loss with penalty 0.9050528407096863\n",
      "epoch 7699, loss with penalty 0.9053716659545898\n",
      "epoch 7700, loss with penalty 0.9062460064888\n",
      "epoch 7799, loss with penalty 0.9045944213867188\n",
      "epoch 7800, loss with penalty 0.9046694040298462\n",
      "New min loss = tensor([0.9039], grad_fn=<AddBackward0>)\n",
      "epoch 7899, loss with penalty 0.9220269918441772\n",
      "epoch 7900, loss with penalty 0.9337553381919861\n",
      "epoch 7999, loss with penalty 0.9045313000679016\n",
      "epoch 8000, loss with penalty 0.9044381380081177\n",
      "epoch 8099, loss with penalty 0.9716232419013977\n",
      "epoch 8100, loss with penalty 0.9134964942932129\n",
      "epoch 8199, loss with penalty 0.9048112034797668\n",
      "epoch 8200, loss with penalty 0.9051124453544617\n",
      "epoch 8299, loss with penalty 0.9043428897857666\n",
      "epoch 8300, loss with penalty 0.9042165875434875\n",
      "epoch 8399, loss with penalty 0.9061325192451477\n",
      "epoch 8400, loss with penalty 0.9070259928703308\n",
      "epoch 8499, loss with penalty 0.9357823729515076\n",
      "epoch 8500, loss with penalty 0.927502453327179\n",
      "epoch 8599, loss with penalty 0.9337846636772156\n",
      "epoch 8600, loss with penalty 0.9193308353424072\n",
      "epoch 8699, loss with penalty 0.9055508375167847\n",
      "epoch 8700, loss with penalty 0.9127662777900696\n",
      "epoch 8799, loss with penalty 0.9272234439849854\n",
      "epoch 8800, loss with penalty 0.9465521574020386\n",
      "epoch 8899, loss with penalty 0.9044711589813232\n",
      "epoch 8900, loss with penalty 0.9102014303207397\n",
      "epoch 8999, loss with penalty 0.9194682240486145\n",
      "epoch 9000, loss with penalty 0.9256190061569214\n",
      "epoch 9099, loss with penalty 0.9705241918563843\n",
      "epoch 9100, loss with penalty 0.952464759349823\n",
      "epoch 9199, loss with penalty 0.9754961133003235\n",
      "epoch 9200, loss with penalty 1.1299042701721191\n",
      "epoch 9299, loss with penalty 0.9042443633079529\n",
      "epoch 9300, loss with penalty 0.9044390320777893\n",
      "epoch 9399, loss with penalty 0.9036144614219666\n",
      "epoch 9400, loss with penalty 0.9036094546318054\n",
      "epoch 9499, loss with penalty 0.9059231281280518\n",
      "epoch 9500, loss with penalty 0.9058864712715149\n",
      "epoch 9599, loss with penalty 0.9043897390365601\n",
      "epoch 9600, loss with penalty 0.9038350582122803\n",
      "epoch 9699, loss with penalty 0.9119880795478821\n",
      "epoch 9700, loss with penalty 0.9124134182929993\n",
      "epoch 9799, loss with penalty 0.9067277908325195\n",
      "epoch 9800, loss with penalty 0.9074746370315552\n",
      "epoch 9899, loss with penalty 0.9102047085762024\n",
      "epoch 9900, loss with penalty 0.9107173681259155\n",
      "epoch 9999, loss with penalty 0.9280734658241272\n",
      "epoch 10000, loss with penalty 0.9634992480278015\n",
      "epoch 10099, loss with penalty 0.9380337595939636\n",
      "epoch 10100, loss with penalty 0.9342839121818542\n",
      "epoch 10199, loss with penalty 1.0566974878311157\n",
      "epoch 10200, loss with penalty 1.0325026512145996\n",
      "epoch 10299, loss with penalty 0.9037404656410217\n",
      "epoch 10300, loss with penalty 0.9043666124343872\n",
      "epoch 10399, loss with penalty 0.9042336940765381\n",
      "epoch 10400, loss with penalty 0.9066017866134644\n",
      "epoch 10499, loss with penalty 1.0116151571273804\n",
      "epoch 10500, loss with penalty 1.0750253200531006\n",
      "epoch 10599, loss with penalty 0.9035376906394958\n",
      "epoch 10600, loss with penalty 0.9035224318504333\n",
      "epoch 10699, loss with penalty 0.9273601770401001\n",
      "epoch 10700, loss with penalty 0.9261608123779297\n",
      "epoch 10799, loss with penalty 0.9030643105506897\n",
      "epoch 10800, loss with penalty 0.9030668139457703\n",
      "epoch 10899, loss with penalty 0.9041436314582825\n",
      "epoch 10900, loss with penalty 0.9040482044219971\n",
      "epoch 10999, loss with penalty 1.1773509979248047\n",
      "epoch 11000, loss with penalty 1.10079026222229\n",
      "epoch 11099, loss with penalty 0.9032346606254578\n",
      "epoch 11100, loss with penalty 0.9032276272773743\n",
      "epoch 11199, loss with penalty 0.910973072052002\n",
      "epoch 11200, loss with penalty 0.9063201546669006\n",
      "New min loss = tensor([0.9029], grad_fn=<AddBackward0>)\n",
      "epoch 11299, loss with penalty 0.9029107689857483\n",
      "epoch 11300, loss with penalty 0.9029035568237305\n",
      "epoch 11399, loss with penalty 0.9038859605789185\n",
      "epoch 11400, loss with penalty 0.9040672183036804\n",
      "epoch 11499, loss with penalty 0.9428725838661194\n",
      "epoch 11500, loss with penalty 0.9781186580657959\n",
      "epoch 11599, loss with penalty 0.9037612080574036\n",
      "epoch 11600, loss with penalty 0.9037447571754456\n",
      "epoch 11699, loss with penalty 1.1152912378311157\n",
      "epoch 11700, loss with penalty 0.930397093296051\n",
      "epoch 11799, loss with penalty 0.9036104679107666\n",
      "epoch 11800, loss with penalty 0.9035801291465759\n",
      "epoch 11899, loss with penalty 0.9071636199951172\n",
      "epoch 11900, loss with penalty 0.9124523997306824\n",
      "epoch 11999, loss with penalty 0.9033990502357483\n",
      "epoch 12000, loss with penalty 0.9033874273300171\n",
      "epoch 12099, loss with penalty 0.9043604731559753\n",
      "epoch 12100, loss with penalty 0.9044296741485596\n",
      "epoch 12199, loss with penalty 1.1763007640838623\n",
      "epoch 12200, loss with penalty 1.0461126565933228\n",
      "epoch 12299, loss with penalty 0.9034634232521057\n",
      "epoch 12300, loss with penalty 0.9034532308578491\n",
      "epoch 12399, loss with penalty 0.9039438962936401\n",
      "epoch 12400, loss with penalty 0.904003381729126\n",
      "epoch 12499, loss with penalty 0.9051162600517273\n",
      "epoch 12500, loss with penalty 0.9052054286003113\n",
      "epoch 12599, loss with penalty 0.9062492251396179\n",
      "epoch 12600, loss with penalty 0.9064518213272095\n",
      "epoch 12699, loss with penalty 0.9060676097869873\n",
      "epoch 12700, loss with penalty 0.9062429666519165\n",
      "epoch 12799, loss with penalty 0.9039261341094971\n",
      "epoch 12800, loss with penalty 0.9038890600204468\n",
      "epoch 12899, loss with penalty 7.031248092651367\n",
      "epoch 12900, loss with penalty 6.432253837585449\n",
      "epoch 12999, loss with penalty 2.6205239295959473\n",
      "epoch 13000, loss with penalty 2.6139159202575684\n",
      "epoch 13099, loss with penalty 2.0738797187805176\n",
      "epoch 13100, loss with penalty 2.0694613456726074\n",
      "epoch 13199, loss with penalty 1.702411413192749\n",
      "epoch 13200, loss with penalty 1.699310064315796\n",
      "epoch 13299, loss with penalty 1.4407925605773926\n",
      "epoch 13300, loss with penalty 1.4386355876922607\n",
      "epoch 13399, loss with penalty 1.2642536163330078\n",
      "epoch 13400, loss with penalty 1.2628549337387085\n",
      "epoch 13499, loss with penalty 1.1531611680984497\n",
      "epoch 13500, loss with penalty 1.152302861213684\n",
      "epoch 13599, loss with penalty 1.0846002101898193\n",
      "epoch 13600, loss with penalty 1.0840609073638916\n",
      "epoch 13699, loss with penalty 1.0403649806976318\n",
      "epoch 13700, loss with penalty 1.0400062799453735\n",
      "epoch 13799, loss with penalty 1.0103776454925537\n",
      "epoch 13800, loss with penalty 1.0101298093795776\n",
      "epoch 13899, loss with penalty 0.9895105361938477\n",
      "epoch 13900, loss with penalty 0.9893377423286438\n",
      "epoch 13999, loss with penalty 0.9748009443283081\n",
      "epoch 14000, loss with penalty 0.9746781587600708\n",
      "epoch 14099, loss with penalty 0.9643039703369141\n",
      "epoch 14100, loss with penalty 0.9642156958580017\n",
      "epoch 14199, loss with penalty 0.9566705822944641\n",
      "epoch 14200, loss with penalty 0.9566061496734619\n",
      "epoch 14299, loss with penalty 0.9509598016738892\n",
      "epoch 14300, loss with penalty 0.9509085416793823\n",
      "epoch 14399, loss with penalty 0.9465129375457764\n",
      "epoch 14400, loss with penalty 0.9464729428291321\n",
      "epoch 14499, loss with penalty 0.9428860545158386\n",
      "epoch 14500, loss with penalty 0.9428524374961853\n",
      "epoch 14599, loss with penalty 0.9397855401039124\n",
      "epoch 14600, loss with penalty 0.9397571682929993\n",
      "epoch 14699, loss with penalty 0.9370289444923401\n",
      "epoch 14700, loss with penalty 0.9370017647743225\n",
      "epoch 14799, loss with penalty 0.9345021843910217\n",
      "epoch 14800, loss with penalty 0.9344770312309265\n",
      "epoch 14899, loss with penalty 0.9321427941322327\n",
      "epoch 14900, loss with penalty 0.9321200251579285\n",
      "epoch 14999, loss with penalty 0.9299196004867554\n",
      "epoch 15000, loss with penalty 0.929899275302887\n",
      "epoch 15099, loss with penalty 0.92781662940979\n",
      "epoch 15100, loss with penalty 0.9277966022491455\n",
      "epoch 15199, loss with penalty 0.9258326888084412\n",
      "epoch 15200, loss with penalty 0.925812304019928\n",
      "epoch 15299, loss with penalty 0.9239664673805237\n",
      "epoch 15300, loss with penalty 0.9239477515220642\n",
      "epoch 15399, loss with penalty 0.9222272038459778\n",
      "epoch 15400, loss with penalty 0.922210156917572\n",
      "epoch 15499, loss with penalty 0.9206148982048035\n",
      "epoch 15500, loss with penalty 0.9205993413925171\n",
      "epoch 15599, loss with penalty 0.9191362261772156\n",
      "epoch 15600, loss with penalty 0.9191224575042725\n",
      "epoch 15699, loss with penalty 0.9177935719490051\n",
      "epoch 15700, loss with penalty 0.9177813529968262\n",
      "epoch 15799, loss with penalty 0.9165844321250916\n",
      "epoch 15800, loss with penalty 0.9165732860565186\n",
      "epoch 15899, loss with penalty 0.9155049324035645\n",
      "epoch 15900, loss with penalty 0.9154950976371765\n",
      "epoch 15999, loss with penalty 0.9145503044128418\n",
      "epoch 16000, loss with penalty 0.9145416021347046\n",
      "epoch 16099, loss with penalty 0.9137107729911804\n",
      "epoch 16100, loss with penalty 0.9137042760848999\n",
      "epoch 16199, loss with penalty 0.9129773378372192\n",
      "epoch 16200, loss with penalty 0.9129707217216492\n",
      "epoch 16299, loss with penalty 0.912337064743042\n",
      "epoch 16300, loss with penalty 0.9123306274414062\n",
      "epoch 16399, loss with penalty 0.9117759466171265\n",
      "epoch 16400, loss with penalty 0.9117699861526489\n",
      "epoch 16499, loss with penalty 0.911282479763031\n",
      "epoch 16500, loss with penalty 0.9112775325775146\n",
      "epoch 16599, loss with penalty 0.9108458161354065\n",
      "epoch 16600, loss with penalty 0.9108415246009827\n",
      "epoch 16699, loss with penalty 0.9104526042938232\n",
      "epoch 16700, loss with penalty 0.9104492664337158\n",
      "epoch 16799, loss with penalty 0.9100940227508545\n",
      "epoch 16800, loss with penalty 0.9100891947746277\n",
      "epoch 16899, loss with penalty 0.9097596406936646\n",
      "epoch 16900, loss with penalty 0.9097567796707153\n",
      "epoch 16999, loss with penalty 0.9094440937042236\n",
      "epoch 17000, loss with penalty 0.9094412922859192\n",
      "epoch 17099, loss with penalty 0.9091421365737915\n",
      "epoch 17100, loss with penalty 0.9091395735740662\n",
      "epoch 17199, loss with penalty 0.9088493585586548\n",
      "epoch 17200, loss with penalty 0.9088466167449951\n",
      "epoch 17299, loss with penalty 0.9085626602172852\n",
      "epoch 17300, loss with penalty 0.9085595607757568\n",
      "epoch 17399, loss with penalty 0.9907370209693909\n",
      "epoch 17400, loss with penalty 0.9231505393981934\n",
      "epoch 17499, loss with penalty 0.9079760313034058\n",
      "epoch 17500, loss with penalty 0.9079731106758118\n",
      "epoch 17599, loss with penalty 0.9236533045768738\n",
      "epoch 17600, loss with penalty 0.933204710483551\n",
      "epoch 17699, loss with penalty 0.9077135920524597\n",
      "epoch 17700, loss with penalty 0.9077107906341553\n",
      "epoch 17799, loss with penalty 0.9072808623313904\n",
      "epoch 17800, loss with penalty 0.9072787761688232\n",
      "epoch 17899, loss with penalty 1.3457061052322388\n",
      "epoch 17900, loss with penalty 1.708733320236206\n",
      "epoch 17999, loss with penalty 0.9073886871337891\n",
      "epoch 18000, loss with penalty 0.9073639512062073\n",
      "epoch 18099, loss with penalty 0.906854510307312\n",
      "epoch 18100, loss with penalty 0.9068517088890076\n",
      "epoch 18199, loss with penalty 0.9284477829933167\n",
      "epoch 18200, loss with penalty 0.9427967071533203\n",
      "epoch 18299, loss with penalty 0.9073922038078308\n",
      "epoch 18300, loss with penalty 0.9073321223258972\n",
      "epoch 18399, loss with penalty 0.9065927267074585\n",
      "epoch 18400, loss with penalty 0.906590461730957\n",
      "epoch 18499, loss with penalty 0.9757769107818604\n",
      "epoch 18500, loss with penalty 1.2997114658355713\n",
      "epoch 18599, loss with penalty 0.9069157242774963\n",
      "epoch 18600, loss with penalty 0.9069118499755859\n",
      "epoch 18699, loss with penalty 0.9063631892204285\n",
      "epoch 18700, loss with penalty 0.9063605070114136\n",
      "epoch 18799, loss with penalty 0.9405970573425293\n",
      "epoch 18800, loss with penalty 0.9226616621017456\n",
      "epoch 18899, loss with penalty 0.9065262675285339\n",
      "epoch 18900, loss with penalty 0.9065195918083191\n",
      "epoch 18999, loss with penalty 0.9061402082443237\n",
      "epoch 19000, loss with penalty 0.9061363339424133\n",
      "epoch 19099, loss with penalty 0.9106773734092712\n",
      "epoch 19100, loss with penalty 0.9085124731063843\n",
      "epoch 19199, loss with penalty 0.9062031507492065\n",
      "epoch 19200, loss with penalty 0.906198263168335\n",
      "epoch 19299, loss with penalty 0.9058678150177002\n",
      "epoch 19300, loss with penalty 0.9058705568313599\n",
      "epoch 19399, loss with penalty 0.9072162508964539\n",
      "epoch 19400, loss with penalty 0.906924843788147\n",
      "epoch 19499, loss with penalty 0.9056856036186218\n",
      "epoch 19500, loss with penalty 0.9056816697120667\n",
      "epoch 19599, loss with penalty 0.9622530341148376\n",
      "epoch 19600, loss with penalty 0.9626664519309998\n",
      "epoch 19699, loss with penalty 0.9056495428085327\n",
      "epoch 19700, loss with penalty 0.9056265950202942\n",
      "epoch 19799, loss with penalty 0.904967188835144\n",
      "epoch 19800, loss with penalty 0.9049617648124695\n",
      "epoch 19899, loss with penalty 0.9044097661972046\n",
      "epoch 19900, loss with penalty 0.9044246673583984\n",
      "epoch 19999, loss with penalty 0.9044496417045593\n",
      "Using potential function defined as class takes 4622.378515958786\n",
      "Before normalization: \n",
      "tensor([[ 0.2191,  0.0109,  0.0694,  0.0271,  0.0748,  0.0103,  0.0114,  0.0010,\n",
      "          0.0217, -0.0156,  0.0227, -0.0377, -0.0870,  0.0113, -0.0197,  0.0107,\n",
      "         -0.0808, -0.0564, -0.1398, -0.0530]])\n",
      "tensor([0.0159])\n",
      "After normalization: \n",
      "tensor([[ 0.2192,  0.0109,  0.0694,  0.0271,  0.0748,  0.0103,  0.0114,  0.0010,\n",
      "          0.0217, -0.0156,  0.0227, -0.0377, -0.0871,  0.0113, -0.0197,  0.0107,\n",
      "         -0.0809, -0.0564, -0.1398, -0.0530]])\n",
      "tensor([0.0159])\n",
      "c value = tensor([1.0005], grad_fn=<MulBackward0>)\n",
      "Before normalization: \n",
      "tensor([[ 0.1239, -0.0003, -0.1047,  0.0017,  0.0764,  0.0007,  0.2147,  0.0011,\n",
      "         -0.0748,  0.0038, -0.0116,  0.0074,  0.1338, -0.0052,  0.0599, -0.0038,\n",
      "         -0.0327, -0.0451,  0.1053, -0.0268]])\n",
      "tensor([0.0917])\n",
      "After normalization: \n",
      "tensor([[ 0.1239, -0.0003, -0.1048,  0.0017,  0.0764,  0.0007,  0.2148,  0.0011,\n",
      "         -0.0748,  0.0038, -0.0116,  0.0074,  0.1338, -0.0052,  0.0600, -0.0038,\n",
      "         -0.0327, -0.0451,  0.1053, -0.0268]])\n",
      "tensor([0.0918])\n",
      "c value = tensor([1.0002], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# model_11_sin(layer_width=20, use_sin=True):\n",
    "res1_11_sin = auto_test_documentation(model_11_sin, adam_opt_11_sin, p_fn1, \n",
    "                                      model_name='model_11_sin_pfn1',\n",
    "                                      res_path=RESULT_PATH,\n",
    "                                      loss_fname='loss_11_sin_pfn1',\n",
    "                                      loss_with_pen_fname = 'loss_pen_11_sin_pfn1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_11_sigmoid = auto_test_documentation(model_11_sigmoid, adam_opt_11_sigmoid, p_fn1, \n",
    "                                      model_name='model_11_sigmoid_pfn1',\n",
    "                                      res_path=RESULT_PATH,\n",
    "                                      loss_fname='loss_11_sigmoid_pfn1',\n",
    "                                      loss_with_pen_fname = 'loss_pen_11_sigmoid_pfn1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time is 2022-08-04 21:19:15.207207-04:00\n",
      "epoch 0, loss with penalty 384.07818603515625\n",
      "epoch 99, loss with penalty 2.7259957790374756\n",
      "epoch 100, loss with penalty 2.718249797821045\n",
      "epoch 199, loss with penalty 2.077763557434082\n",
      "epoch 200, loss with penalty 2.0733699798583984\n",
      "New min loss = tensor([1.9956], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9913], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9871], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9828], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9785], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9742], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9699], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9656], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9613], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9570], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9527], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9484], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9441], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9398], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9354], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9311], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9268], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9224], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9181], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9137], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9093], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9050], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.9006], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8962], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8918], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8874], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8830], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8786], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8742], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8697], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8653], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8608], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8564], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8519], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8474], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8429], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8384], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8339], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8294], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8248], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8203], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8157], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8112], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8066], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.8020], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7975], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7929], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7883], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7836], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7790], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7744], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7697], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7651], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7604], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7558], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7511], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7464], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7417], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7370], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7323], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7276], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7229], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7182], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7134], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7087], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.7040], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6992], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6945], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6897], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6849], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6802], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6754], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6706], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6658], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6611], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6563], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6515], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6467], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6419], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6371], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6324], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6276], grad_fn=<AddBackward0>)\n",
      "epoch 299, loss with penalty 1.6275606155395508\n",
      "New min loss = tensor([1.6228], grad_fn=<AddBackward0>)\n",
      "epoch 300, loss with penalty 1.6227713823318481\n",
      "New min loss = tensor([1.6180], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6132], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6084], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.6036], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5988], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5941], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5893], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5845], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5797], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5750], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5702], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5655], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5607], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5560], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5512], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5465], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5418], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5370], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5323], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5276], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5229], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5183], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5136], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5089], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.5043], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4996], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4950], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4904], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4858], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4812], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4766], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4720], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4675], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4629], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4584], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4539], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4494], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4449], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4405], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4360], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4316], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4272], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4228], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4184], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4141], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4098], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4054], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.4011], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3969], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3926], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3884], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3842], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3800], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3758], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3716], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3675], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3634], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3593], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3553], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3512], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3472], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3432], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3392], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3353], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3314], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3275], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3236], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3198], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3160], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3122], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3084], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3046], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.3009], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2972], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2936], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2899], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2863], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2827], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2792], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2756], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2721], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2686], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2652], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2618], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2584], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2550], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2516], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2483], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2450], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2418], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2385], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2353], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2321], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2290], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2259], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2228], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2197], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2166], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2136], grad_fn=<AddBackward0>)\n",
      "epoch 399, loss with penalty 1.213633418083191\n",
      "New min loss = tensor([1.2106], grad_fn=<AddBackward0>)\n",
      "epoch 400, loss with penalty 1.2106456756591797\n",
      "New min loss = tensor([1.2077], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2048], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.2018], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1990], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1961], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1933], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1905], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1877], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1850], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1823], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1796], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1769], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1743], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1717], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1691], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1665], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1640], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1615], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1590], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1566], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1542], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1518], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1494], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1470], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1447], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1424], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1401], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1379], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1357], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1335], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1313], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1292], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1270], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1249], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1229], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1208], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1188], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1168], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1148], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1128], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1109], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1090], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1071], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1052], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1034], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.1016], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0997], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0980], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0962], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0945], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0928], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0911], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0894], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0877], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0861], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0845], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0829], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0813], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0798], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0782], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0767], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0752], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0738], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0723], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0709], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0694], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0680], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0667], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0653], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0639], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0626], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0613], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0600], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0587], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0575], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0562], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0550], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0538], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0526], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0514], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0502], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0491], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0479], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0468], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0457], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0446], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0436], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0425], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0415], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0396], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0381], grad_fn=<AddBackward0>)\n",
      "epoch 499, loss with penalty 1.050378441810608\n",
      "epoch 500, loss with penalty 1.0646576881408691\n",
      "New min loss = tensor([1.0212], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0201], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0134], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0053], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0023], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([1.0010], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9960], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9932], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9911], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9893], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9880], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9868], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9854], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9844], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9831], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9819], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9808], grad_fn=<AddBackward0>)\n",
      "epoch 599, loss with penalty 0.9804220795631409\n",
      "epoch 600, loss with penalty 0.9800320267677307\n",
      "New min loss = tensor([0.9797], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9787], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9777], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9766], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9756], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9746], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9734], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9724], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9713], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9702], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9691], grad_fn=<AddBackward0>)\n",
      "epoch 699, loss with penalty 1.6130471229553223\n",
      "epoch 700, loss with penalty 0.9961431622505188\n",
      "New min loss = tensor([0.9675], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9662], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9650], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9637], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9627], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9611], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9599], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9589], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9579], grad_fn=<AddBackward0>)\n",
      "epoch 799, loss with penalty 0.9569866061210632\n",
      "epoch 800, loss with penalty 0.956881046295166\n",
      "New min loss = tensor([0.9568], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9557], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9546], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9536], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9525], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9515], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9505], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9494], grad_fn=<AddBackward0>)\n",
      "epoch 899, loss with penalty 5.429451942443848\n",
      "epoch 900, loss with penalty 9.538125038146973\n",
      "epoch 999, loss with penalty 0.9488427042961121\n",
      "epoch 1000, loss with penalty 0.9486737847328186\n",
      "New min loss = tensor([0.9484], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9473], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9463], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9452], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9441], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9431], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9420], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9410], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9399], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9389], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9378], grad_fn=<AddBackward0>)\n",
      "epoch 1099, loss with penalty 0.9375770092010498\n",
      "epoch 1100, loss with penalty 0.9374879598617554\n",
      "New min loss = tensor([0.9368], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9358], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9347], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9336], grad_fn=<AddBackward0>)\n",
      "epoch 1199, loss with penalty 1.2870984077453613\n",
      "epoch 1200, loss with penalty 0.9839164018630981\n",
      "New min loss = tensor([0.9326], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9315], grad_fn=<AddBackward0>)\n",
      "epoch 1299, loss with penalty 0.9311190843582153\n",
      "epoch 1300, loss with penalty 0.9309925436973572\n",
      "New min loss = tensor([0.9304], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9293], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9283], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9272], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9262], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9251], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9241], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9231], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9221], grad_fn=<AddBackward0>)\n",
      "epoch 1399, loss with penalty 0.9214664101600647\n",
      "epoch 1400, loss with penalty 0.9213835000991821\n",
      "New min loss = tensor([0.9210], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9199], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9189], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9179], grad_fn=<AddBackward0>)\n",
      "epoch 1499, loss with penalty 1.375474452972412\n",
      "epoch 1500, loss with penalty 0.9744950532913208\n",
      "New min loss = tensor([0.9169], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9158], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9147], grad_fn=<AddBackward0>)\n",
      "epoch 1599, loss with penalty 0.9144038558006287\n",
      "epoch 1600, loss with penalty 0.9142773151397705\n",
      "New min loss = tensor([0.9137], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9126], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9116], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9106], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9095], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9085], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9074], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9064], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9053], grad_fn=<AddBackward0>)\n",
      "epoch 1699, loss with penalty 0.9048202037811279\n",
      "epoch 1700, loss with penalty 0.904736340045929\n",
      "New min loss = tensor([0.9043], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9033], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9022], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9012], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.9002], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8992], grad_fn=<AddBackward0>)\n",
      "epoch 1799, loss with penalty 3.3621737957000732\n",
      "epoch 1800, loss with penalty 1.075022578239441\n",
      "New min loss = tensor([0.8980], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8970], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8958], grad_fn=<AddBackward0>)\n",
      "epoch 1899, loss with penalty 0.8958107233047485\n",
      "epoch 1900, loss with penalty 0.8957219123840332\n",
      "New min loss = tensor([0.8947], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8937], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8926], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8916], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8906], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8895], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8885], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8875], grad_fn=<AddBackward0>)\n",
      "epoch 1999, loss with penalty 0.8865686058998108\n",
      "epoch 2000, loss with penalty 0.8864872455596924\n",
      "New min loss = tensor([0.8864], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8854], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8843], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8833], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8822], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8812], grad_fn=<AddBackward0>)\n",
      "epoch 2099, loss with penalty 3.112062931060791\n",
      "epoch 2100, loss with penalty 4.252709865570068\n",
      "New min loss = tensor([0.8798], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8787], grad_fn=<AddBackward0>)\n",
      "epoch 2199, loss with penalty 0.8778156042098999\n",
      "New min loss = tensor([0.8777], grad_fn=<AddBackward0>)\n",
      "epoch 2200, loss with penalty 0.8776724934577942\n",
      "New min loss = tensor([0.8767], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8756], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8746], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8736], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8725], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8714], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8704], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8693], grad_fn=<AddBackward0>)\n",
      "epoch 2299, loss with penalty 0.8689603805541992\n",
      "epoch 2300, loss with penalty 0.8688862323760986\n",
      "New min loss = tensor([0.8683], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8672], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8662], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8652], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8642], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8632], grad_fn=<AddBackward0>)\n",
      "epoch 2399, loss with penalty 0.8622996211051941\n",
      "epoch 2400, loss with penalty 0.8622390627861023\n",
      "New min loss = tensor([0.8621], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8611], grad_fn=<AddBackward0>)\n",
      "epoch 2499, loss with penalty 0.8620776534080505\n",
      "epoch 2500, loss with penalty 0.8672164678573608\n",
      "New min loss = tensor([0.8599], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8587], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8577], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8566], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8556], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8546], grad_fn=<AddBackward0>)\n",
      "epoch 2599, loss with penalty 0.8542043566703796\n",
      "epoch 2600, loss with penalty 0.8541479706764221\n",
      "New min loss = tensor([0.8536], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8526], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8515], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8505], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8495], grad_fn=<AddBackward0>)\n",
      "epoch 2699, loss with penalty 0.8494550585746765\n",
      "epoch 2700, loss with penalty 0.8494147062301636\n",
      "epoch 2799, loss with penalty 0.8546351194381714\n",
      "epoch 2800, loss with penalty 0.8492225408554077\n",
      "New min loss = tensor([0.8483], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8471], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8461], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8451], grad_fn=<AddBackward0>)\n",
      "epoch 2899, loss with penalty 0.8443516492843628\n",
      "epoch 2900, loss with penalty 0.8443190455436707\n",
      "New min loss = tensor([0.8441], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8431], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8420], grad_fn=<AddBackward0>)\n",
      "epoch 2999, loss with penalty 0.8417830467224121\n",
      "epoch 3000, loss with penalty 0.841768741607666\n",
      "epoch 3099, loss with penalty 0.8423311114311218\n",
      "epoch 3100, loss with penalty 0.8419492244720459\n",
      "New min loss = tensor([0.8410], grad_fn=<AddBackward0>)\n",
      "New min loss = tensor([0.8400], grad_fn=<AddBackward0>)\n",
      "epoch 3199, loss with penalty 0.8394137620925903\n",
      "epoch 3200, loss with penalty 0.8393973112106323\n",
      "New min loss = tensor([0.8390], grad_fn=<AddBackward0>)\n",
      "epoch 3299, loss with penalty 0.8382306098937988\n",
      "epoch 3300, loss with penalty 0.8382523655891418\n",
      "epoch 3399, loss with penalty 0.8394390940666199\n",
      "epoch 3400, loss with penalty 0.8390206098556519\n",
      "New min loss = tensor([0.8380], grad_fn=<AddBackward0>)\n",
      "epoch 3499, loss with penalty 0.8373029232025146\n",
      "epoch 3500, loss with penalty 0.8372937440872192\n",
      "New min loss = tensor([0.8370], grad_fn=<AddBackward0>)\n",
      "epoch 3599, loss with penalty 0.836617648601532\n",
      "epoch 3600, loss with penalty 0.8366124033927917\n",
      "epoch 3699, loss with penalty 0.839927077293396\n",
      "epoch 3700, loss with penalty 0.8378062844276428\n",
      "epoch 3799, loss with penalty 0.8362662196159363\n",
      "epoch 3800, loss with penalty 0.8362596035003662\n",
      "New min loss = tensor([0.8360], grad_fn=<AddBackward0>)\n",
      "epoch 3899, loss with penalty 0.835815966129303\n",
      "epoch 3900, loss with penalty 0.8358142375946045\n",
      "epoch 3999, loss with penalty 0.8378467559814453\n",
      "epoch 4000, loss with penalty 0.8367916345596313\n",
      "epoch 4099, loss with penalty 0.8356968760490417\n",
      "epoch 4100, loss with penalty 0.8356928825378418\n",
      "epoch 4199, loss with penalty 0.8353994488716125\n",
      "epoch 4200, loss with penalty 0.835401713848114\n",
      "epoch 4299, loss with penalty 0.8367496728897095\n",
      "epoch 4300, loss with penalty 0.8375106453895569\n",
      "epoch 4399, loss with penalty 0.8352759480476379\n",
      "epoch 4400, loss with penalty 0.8352730870246887\n",
      "epoch 4499, loss with penalty 0.8821183443069458\n",
      "epoch 4500, loss with penalty 0.8551861047744751\n",
      "epoch 4599, loss with penalty 0.8351980447769165\n",
      "epoch 4600, loss with penalty 0.8351946473121643\n",
      "epoch 4699, loss with penalty 0.8905460238456726\n",
      "epoch 4700, loss with penalty 1.0267447233200073\n",
      "epoch 4799, loss with penalty 0.8351434469223022\n",
      "epoch 4800, loss with penalty 0.8351409435272217\n",
      "epoch 4899, loss with penalty 1.0487970113754272\n",
      "epoch 4900, loss with penalty 1.1992849111557007\n",
      "epoch 4999, loss with penalty 0.835075318813324\n",
      "epoch 5000, loss with penalty 0.8350724577903748\n",
      "epoch 5099, loss with penalty 0.8890895247459412\n",
      "epoch 5100, loss with penalty 0.8637875914573669\n",
      "epoch 5199, loss with penalty 0.8358586430549622\n",
      "epoch 5200, loss with penalty 0.8362031579017639\n",
      "epoch 5299, loss with penalty 0.8355991840362549\n",
      "epoch 5300, loss with penalty 0.835631787776947\n",
      "epoch 5399, loss with penalty 0.8431553244590759\n",
      "epoch 5400, loss with penalty 0.8374354839324951\n",
      "epoch 5499, loss with penalty 0.9794842600822449\n",
      "epoch 5500, loss with penalty 1.0248395204544067\n",
      "epoch 5599, loss with penalty 0.8413263559341431\n",
      "epoch 5600, loss with penalty 0.8437501192092896\n",
      "epoch 5699, loss with penalty 0.8356451988220215\n",
      "epoch 5700, loss with penalty 0.8356932997703552\n",
      "epoch 5799, loss with penalty 0.8351721167564392\n",
      "epoch 5800, loss with penalty 0.8350799679756165\n",
      "epoch 5899, loss with penalty 0.8389599323272705\n",
      "epoch 5900, loss with penalty 0.8387757539749146\n",
      "epoch 5999, loss with penalty 0.8383384346961975\n",
      "epoch 6000, loss with penalty 0.8394376635551453\n",
      "epoch 6099, loss with penalty 0.8615023493766785\n",
      "epoch 6100, loss with penalty 0.8616291284561157\n",
      "epoch 6199, loss with penalty 0.850896418094635\n",
      "epoch 6200, loss with penalty 0.8457721471786499\n",
      "epoch 6299, loss with penalty 0.8350785970687866\n",
      "epoch 6300, loss with penalty 0.8350749611854553\n",
      "epoch 6399, loss with penalty 0.8415355682373047\n",
      "epoch 6400, loss with penalty 0.8407407402992249\n",
      "epoch 6499, loss with penalty 0.8397007584571838\n",
      "epoch 6500, loss with penalty 0.8396380543708801\n",
      "epoch 6599, loss with penalty 0.8354899883270264\n",
      "epoch 6600, loss with penalty 0.8358403444290161\n",
      "epoch 6699, loss with penalty 0.9407323598861694\n",
      "epoch 6700, loss with penalty 0.9654074907302856\n",
      "epoch 6799, loss with penalty 0.9013580679893494\n",
      "epoch 6800, loss with penalty 0.8989804983139038\n",
      "epoch 6899, loss with penalty 1.1451667547225952\n",
      "epoch 6900, loss with penalty 1.0465285778045654\n",
      "epoch 6999, loss with penalty 0.8384384512901306\n",
      "epoch 7000, loss with penalty 0.8408839106559753\n",
      "epoch 7099, loss with penalty 0.8716935515403748\n",
      "epoch 7100, loss with penalty 0.8589450716972351\n",
      "epoch 7199, loss with penalty 0.8625620603561401\n",
      "epoch 7200, loss with penalty 0.8660008311271667\n",
      "epoch 7299, loss with penalty 0.8513343930244446\n",
      "epoch 7300, loss with penalty 0.8440468311309814\n",
      "epoch 7399, loss with penalty 0.8564289808273315\n",
      "epoch 7400, loss with penalty 0.8389388918876648\n",
      "epoch 7499, loss with penalty 1.1352016925811768\n",
      "epoch 7500, loss with penalty 1.076981782913208\n",
      "epoch 7599, loss with penalty 0.8590799570083618\n",
      "epoch 7600, loss with penalty 0.8547777533531189\n",
      "epoch 7699, loss with penalty 0.8360079526901245\n",
      "epoch 7700, loss with penalty 0.83907151222229\n",
      "epoch 7799, loss with penalty 0.8351196646690369\n",
      "epoch 7800, loss with penalty 0.8351165652275085\n",
      "epoch 7899, loss with penalty 0.8648846745491028\n",
      "epoch 7900, loss with penalty 0.8383365273475647\n",
      "epoch 7999, loss with penalty 1.2022387981414795\n",
      "epoch 8000, loss with penalty 1.2209618091583252\n",
      "New min loss = tensor([0.8350], grad_fn=<AddBackward0>)\n",
      "epoch 8099, loss with penalty 0.8381258249282837\n",
      "epoch 8100, loss with penalty 0.8353258371353149\n",
      "epoch 8199, loss with penalty 0.8627532720565796\n",
      "epoch 8200, loss with penalty 0.862629771232605\n",
      "epoch 8299, loss with penalty 1.003417730331421\n",
      "epoch 8300, loss with penalty 1.0147128105163574\n",
      "epoch 8399, loss with penalty 0.8350250720977783\n",
      "epoch 8400, loss with penalty 0.8350495100021362\n",
      "epoch 8499, loss with penalty 1.008307933807373\n",
      "epoch 8500, loss with penalty 1.0417149066925049\n",
      "epoch 8599, loss with penalty 1.4348201751708984\n",
      "epoch 8600, loss with penalty 1.3730883598327637\n",
      "epoch 8699, loss with penalty 0.8349015712738037\n",
      "epoch 8700, loss with penalty 0.8350083827972412\n",
      "epoch 8799, loss with penalty 0.8351154327392578\n",
      "epoch 8800, loss with penalty 0.834990918636322\n",
      "epoch 8899, loss with penalty 0.8353188037872314\n",
      "epoch 8900, loss with penalty 0.840646505355835\n",
      "epoch 8999, loss with penalty 0.8357487916946411\n",
      "epoch 9000, loss with penalty 0.8377866744995117\n",
      "epoch 9099, loss with penalty 0.9631239771842957\n",
      "epoch 9100, loss with penalty 0.9680060744285583\n",
      "epoch 9199, loss with penalty 0.8553197383880615\n",
      "epoch 9200, loss with penalty 0.8611252903938293\n",
      "epoch 9299, loss with penalty 0.9527169466018677\n",
      "epoch 9300, loss with penalty 0.9315439462661743\n",
      "epoch 9399, loss with penalty 0.9187393188476562\n",
      "epoch 9400, loss with penalty 0.9222936630249023\n",
      "epoch 9499, loss with penalty 0.8528394103050232\n",
      "epoch 9500, loss with penalty 0.8481830954551697\n",
      "epoch 9599, loss with penalty 0.8400325179100037\n",
      "epoch 9600, loss with penalty 0.8373811841011047\n",
      "epoch 9699, loss with penalty 0.8476890921592712\n",
      "epoch 9700, loss with penalty 0.8503968119621277\n",
      "epoch 9799, loss with penalty 0.8462409377098083\n",
      "epoch 9800, loss with penalty 0.8518177270889282\n",
      "epoch 9899, loss with penalty 0.8445616364479065\n",
      "epoch 9900, loss with penalty 0.8509484529495239\n",
      "epoch 9999, loss with penalty 0.8677758574485779\n",
      "epoch 10000, loss with penalty 0.8770439028739929\n",
      "epoch 10099, loss with penalty 0.8612426519393921\n",
      "epoch 10100, loss with penalty 0.8529966473579407\n",
      "epoch 10199, loss with penalty 0.860505998134613\n",
      "epoch 10200, loss with penalty 0.844028651714325\n",
      "epoch 10299, loss with penalty 0.8654308319091797\n",
      "epoch 10300, loss with penalty 0.8755725026130676\n",
      "epoch 10399, loss with penalty 0.8634663224220276\n",
      "epoch 10400, loss with penalty 0.8456165790557861\n",
      "epoch 10499, loss with penalty 0.874535083770752\n",
      "epoch 10500, loss with penalty 0.8910648822784424\n",
      "epoch 10599, loss with penalty 0.8385066390037537\n",
      "epoch 10600, loss with penalty 0.8413109183311462\n",
      "epoch 10699, loss with penalty 0.8365486264228821\n",
      "epoch 10700, loss with penalty 0.8391003012657166\n",
      "epoch 10799, loss with penalty 0.8630841970443726\n",
      "epoch 10800, loss with penalty 0.8599976301193237\n",
      "epoch 10899, loss with penalty 5.6195268630981445\n",
      "epoch 10900, loss with penalty 5.741830825805664\n",
      "epoch 10999, loss with penalty 3.6344308853149414\n",
      "epoch 11000, loss with penalty 3.62214732170105\n",
      "epoch 11099, loss with penalty 2.324507474899292\n",
      "epoch 11100, loss with penalty 2.313047409057617\n",
      "epoch 11199, loss with penalty 1.5089646577835083\n",
      "epoch 11200, loss with penalty 1.503782868385315\n",
      "epoch 11299, loss with penalty 1.1670477390289307\n",
      "epoch 11300, loss with penalty 1.164947748184204\n",
      "epoch 11399, loss with penalty 1.0246238708496094\n",
      "epoch 11400, loss with penalty 1.0236995220184326\n",
      "epoch 11499, loss with penalty 0.9582840800285339\n",
      "epoch 11500, loss with penalty 0.9578193426132202\n",
      "epoch 11599, loss with penalty 0.9227578043937683\n",
      "epoch 11600, loss with penalty 0.922490119934082\n",
      "epoch 11699, loss with penalty 0.9012237787246704\n",
      "epoch 11700, loss with penalty 0.9010531902313232\n",
      "epoch 11799, loss with penalty 0.8869614005088806\n",
      "epoch 11800, loss with penalty 0.8868427276611328\n",
      "epoch 11899, loss with penalty 0.8769627809524536\n",
      "epoch 11900, loss with penalty 0.8768781423568726\n",
      "epoch 11999, loss with penalty 0.8696906566619873\n",
      "epoch 12000, loss with penalty 0.8696284890174866\n",
      "epoch 12099, loss with penalty 0.864264965057373\n",
      "epoch 12100, loss with penalty 0.8642179369926453\n",
      "epoch 12199, loss with penalty 0.8971186280250549\n",
      "epoch 12200, loss with penalty 0.8676692843437195\n",
      "epoch 12299, loss with penalty 0.8589569330215454\n",
      "epoch 12300, loss with penalty 0.858913779258728\n",
      "epoch 12399, loss with penalty 0.8555468320846558\n",
      "epoch 12400, loss with penalty 0.8555269241333008\n",
      "epoch 12499, loss with penalty 0.8585878610610962\n",
      "epoch 12500, loss with penalty 0.8593470454216003\n",
      "epoch 12599, loss with penalty 0.853185772895813\n",
      "epoch 12600, loss with penalty 0.8531547784805298\n",
      "epoch 12699, loss with penalty 0.8506963849067688\n",
      "epoch 12700, loss with penalty 0.8506827354431152\n",
      "epoch 12799, loss with penalty 0.8616024851799011\n",
      "epoch 12800, loss with penalty 0.862173855304718\n",
      "epoch 12899, loss with penalty 0.851982057094574\n",
      "epoch 12900, loss with penalty 0.8519338369369507\n",
      "epoch 12999, loss with penalty 0.8486685752868652\n",
      "epoch 13000, loss with penalty 0.8486462831497192\n",
      "epoch 13099, loss with penalty 0.8468520045280457\n",
      "epoch 13100, loss with penalty 0.84683758020401\n",
      "epoch 13199, loss with penalty 0.8606103658676147\n",
      "epoch 13200, loss with penalty 0.8629860877990723\n",
      "epoch 13299, loss with penalty 0.8484350442886353\n",
      "epoch 13300, loss with penalty 0.8483911156654358\n",
      "epoch 13399, loss with penalty 0.8456076383590698\n",
      "epoch 13400, loss with penalty 0.8455892205238342\n",
      "epoch 13499, loss with penalty 4.394296169281006\n",
      "epoch 13500, loss with penalty 2.1477205753326416\n",
      "epoch 13599, loss with penalty 0.8487288951873779\n",
      "epoch 13600, loss with penalty 0.8486619591712952\n",
      "epoch 13699, loss with penalty 0.8444926738739014\n",
      "epoch 13700, loss with penalty 0.844470202922821\n",
      "epoch 13799, loss with penalty 1.0031720399856567\n",
      "epoch 13800, loss with penalty 1.1039484739303589\n",
      "epoch 13899, loss with penalty 0.8488759994506836\n",
      "epoch 13900, loss with penalty 0.8488805294036865\n",
      "epoch 13999, loss with penalty 0.8435677289962769\n",
      "epoch 14000, loss with penalty 0.8435414433479309\n",
      "epoch 14099, loss with penalty 0.8418887853622437\n",
      "epoch 14100, loss with penalty 0.8419259190559387\n",
      "epoch 14199, loss with penalty 0.8498008251190186\n",
      "epoch 14200, loss with penalty 0.8492435216903687\n",
      "epoch 14299, loss with penalty 0.8426575660705566\n",
      "epoch 14300, loss with penalty 0.8426289558410645\n",
      "epoch 14399, loss with penalty 0.8407868146896362\n",
      "epoch 14400, loss with penalty 0.840776264667511\n",
      "epoch 14499, loss with penalty 0.8482608795166016\n",
      "epoch 14500, loss with penalty 0.8498117327690125\n",
      "epoch 14599, loss with penalty 0.8412017822265625\n",
      "epoch 14600, loss with penalty 0.8411765694618225\n",
      "epoch 14699, loss with penalty 0.8507501482963562\n",
      "epoch 14700, loss with penalty 0.8570581078529358\n",
      "epoch 14799, loss with penalty 0.8430997729301453\n",
      "epoch 14800, loss with penalty 0.8427858948707581\n"
     ]
    }
   ],
   "source": [
    "res1_21_sin = auto_test_documentation(model_21_sin, adam_opt_21_sin, p_fn1, \n",
    "                                      model_name='model_21_sin_pfn1',\n",
    "                                      res_path=RESULT_PATH,\n",
    "                                      loss_fname='loss_21_sin_pfn1',\n",
    "                                      loss_with_pen_fname = 'loss_pen_21_sin_pfn1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9031], grad_fn=<MulBackward0>),\n",
       " tensor([0.9031], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon_Loss_penalty(p_fn1, res1_11_sin[3], LAMBDA_PEN, U_BOUND, DISCRETE_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values = np.linspace(L_BOUND, U_BOUND, N_POINTS)\n",
    "x_vals = [torch.tensor([i], requires_grad=True, dtype=torch.float) for i in np.linspace(L_BOUND, U_BOUND, N_POINTS)]\n",
    "\n",
    "model_output = model_on_interval(model_21_sin, DISCRETE_POINTS)\n",
    "\n",
    "plot_figure(x_values, model_output)\n",
    "# plot_figure(x_values, model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrbOIPSxK5LM"
   },
   "source": [
    "### The Finite Difference Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "qkg6Zfo7u8Fq"
   },
   "outputs": [],
   "source": [
    "def finite_diff_method(discrete_points, given_fn):\n",
    "    N_points = len(discrete_points)\n",
    "    h = discrete_points[1]-discrete_points[0]\n",
    "    c = np.sqrt(1/h)\n",
    "\n",
    "    H = np.zeros((N_points,N_points))\n",
    "    # Question: Why H has dimension (N-2) x (N-2)?\n",
    "    V = np.zeros((N_points,N_points))\n",
    "\n",
    "    for i in range (N_points):\n",
    "        for j in range (N_points):\n",
    "            if i == j:\n",
    "                H[i][j] = -2\n",
    "            elif abs(i-j) == 1:\n",
    "                H[i][j] = 1\n",
    "    for i in range (N_points):\n",
    "        for j in range (N_points):\n",
    "            if i == j:\n",
    "                V[i][j] = given_fn.forward(discrete_points[i])\n",
    "\n",
    "    A = -H/(h**2) + V\n",
    "    \n",
    "    eig_val, eig_vec = np.linalg.eig(A)\n",
    "    sorted_id_eig_val = np.argsort(eig_val)\n",
    "    # Get the indices that would sort eig_val\n",
    "    z = sorted_id_eig_val[0:1]\n",
    "    # eig_val[z] will return sorted first [T] value of eigenvalues.\n",
    "    energies = eig_val[z]/eig_val[z][0]\n",
    "\n",
    "    ground_state = c * eig_vec[:,z[0]]\n",
    "\n",
    "    return (eig_val[z], ground_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lIGR6Gohu8Fr"
   },
   "outputs": [],
   "source": [
    "def plot_finite_diff_sol(eig_val, ground_state, discrete_points, nn_model=None):\n",
    "    plt.figure(figsize=(12,10))\n",
    "    for i in range(len(eig_val)):\n",
    "        y = []\n",
    "        y = np.append(y, (-1) * ground_state)\n",
    "\n",
    "        plt.plot(discrete_points,y,lw=3, label=\"{} \".format(i))\n",
    "        plt.xlabel('x', size=14)\n",
    "        plt.ylabel('$\\psi$(x)',size=14)\n",
    "    if nn_model != None:\n",
    "        model_output = model_on_interval(nn_model, DISCRETE_POINTS)\n",
    "        plt.plot(discrete_points, model_output)\n",
    "    plt.legend()\n",
    "    plt.title('normalized wavefunctions for a harmonic oscillator using finite difference method',size=14)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xx7UHtNBK5LO"
   },
   "outputs": [],
   "source": [
    "loss_val = epsilon_Loss_penalty(p_fn1, res_1_1_sin[3], LAMBDA_PEN, U_BOUND, DISCRETE_POINTS)[0]\n",
    "print(\"The FD method eigenvalue: \" + str(eig_val[0]))\n",
    "print(\"The NN method loss value: \" + str(loss_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8500041525597394\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eig_val, ground_state = finite_diff_method(DISCRETE_POINTS, p_fn1)\n",
    "print(eig_val[0])\n",
    "plot_finite_diff_sol(eig_val, ground_state, DISCRETE_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrbOIPSxK5LM"
   },
   "source": [
    "### Load Results for Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Data-running version.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
