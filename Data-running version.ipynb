{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "# from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS FOR \n",
    "LAMBDA_PEN = 1000\n",
    "L_BOUND = -10\n",
    "U_BOUND = 10\n",
    "N_POINTS = 512\n",
    "\n",
    "NUM_EPOCHS = 5000\n",
    "\n",
    "M_POINTS = 10\n",
    "ALPHA = 2\n",
    "\n",
    "SELECTION_RATE = 0.01\n",
    "\n",
    "DISCRETE_POINTS = np.linspace(L_BOUND, U_BOUND, N_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sinFN(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(input):\n",
    "        return torch.sin(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# PLOT DATA\n",
    "def plot_figure(x_val, y_val, x_test=None, predicted=None, log_scale=False):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 6), dpi=80)\n",
    "    if log_scale==True:\n",
    "        plt.yscale('log')\n",
    "    plt.plot(x_val, y_val, '--', label='True data', alpha=0.5)\n",
    "    if predicted != None:\n",
    "        plt.plot(x_test, predicted, '--', label='Predictions', alpha=0.5)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class X_Square_sin:\n",
    "    def __init__(self, const, use_sin=False):\n",
    "        self.const = const\n",
    "        if use_sin == True:\n",
    "            self.sin_const = np.random.choice(np.arange(10, 50))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x**2 + self.const + self.sin_const*math.sin(x)\n",
    "    \n",
    "class Potential_Function:\n",
    "    def __init__(self, c_0=0,\n",
    "                 M_points=M_POINTS,\n",
    "                 L_endpoint=U_BOUND, \n",
    "                 alpha=ALPHA, \n",
    "                 rescale=1):\n",
    "        self.M_points = M_points\n",
    "        self.L_endpoint = L_endpoint\n",
    "        self.alpha = alpha\n",
    "        self.c_0 = c_0\n",
    "        self.rescale = 1\n",
    "        \n",
    "        self.ti_list = np.random.normal(loc=0, scale=1.0, size= self.M_points)\n",
    "        self.ci_list = [(self.L_endpoint/(i * math.pi))**self.alpha \n",
    "                        for i in range(1, self.M_points+1)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        f_value = 0\n",
    "        summation = 0\n",
    "        \n",
    "        #Iterative method:\n",
    "\n",
    "        for i in range(1, self.M_points+1):\n",
    "            cos_val = np.cos((i * math.pi * x)/self.L_endpoint)\n",
    "            summation += self.ti_list[i-1] * self.ci_list[i-1] * cos_val\n",
    "\n",
    "        f_value += summation\n",
    "        f_value += self.c_0\n",
    "        return self.rescale * f_value\n",
    "    \n",
    "    def set_c0_value(self, val):\n",
    "        self.c_0 = val\n",
    "        return\n",
    "    \n",
    "    def set_rescale_factor(self, val):\n",
    "        self.rescale = val\n",
    "        return \n",
    "    \n",
    "    def plot_function(self, discrete_points):\n",
    "        y_values = [self.forward(i) for i in discrete_points]\n",
    "        plt.plot(discrete_points, y_values)\n",
    "    \n",
    "    def update_potential_fn(self, discrete_points):\n",
    "        # (1) Check lowest value to set appropriate c_0 value:\n",
    "        y_values = [self.forward(i) for i in discrete_points]\n",
    "        min_val = min(y_values)\n",
    "        print(\"Original min val = \" + str(min_val))\n",
    "        # plt.plot(DISCRETE_POINTS, y_values)\n",
    "\n",
    "        # (1) Set c_0 value s.t. every value in y_values is > 0: \n",
    "        if min_val < 0:\n",
    "            c_0 = math.ceil(abs(min_val))\n",
    "            self.set_c0_value(c_0)\n",
    "            new_y = [self.forward(i) for i in discrete_points]\n",
    "            print(\"Min val with c_0 updated = \" + str(min(new_y)))\n",
    "        # plt.plot(DISCRETE_POINTS, new_y)\n",
    "\n",
    "        # (2) Rescale potential function s.t. every value lies between 0 and 5:\n",
    "        max_v = max(new_y)\n",
    "#         min_v = min(new_y)\n",
    "#         print(max_v, min_v)\n",
    "#         max_min = max_v-min_v\n",
    "\n",
    "#         rescaled_y = [(i-min_v)/(max_min) for i in new_y]\n",
    "#         plt.plot(DISCRETE_POINTS, rescaled_y)\n",
    "\n",
    "        c = 5/max_v\n",
    "        self.set_rescale_factor(c)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_Loss_penalty(v_x, model_u, \n",
    "                         lambda_pen,\n",
    "                         upper_bound, \n",
    "                         discrete_points):\n",
    "    eps_sum = 0\n",
    "    pen = 0\n",
    "\n",
    "    h = (2*upper_bound)/(len(discrete_points)-1)\n",
    "#     print(\"h = \"+ str(h))\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "\n",
    "        u_prime_square = torch.square(model_u.u_prime(x_i))\n",
    "        u_xi_square = torch.square(model_u(x_i))\n",
    "        \n",
    "        v_xi = v_x.forward(i)\n",
    "        if v_xi < 0:\n",
    "            raise ValueError('Potential Function value is < 0')\n",
    "\n",
    "        t = u_prime_square + v_xi*u_xi_square\n",
    "        eps_sum += t\n",
    "        \n",
    "        pen+= u_xi_square\n",
    "        \n",
    "    epsilon_fn = h*eps_sum\n",
    "    \n",
    "    penalty = lambda_pen * torch.square(h*pen-1)\n",
    "#     print(\"epsilon_fn value = \" + str(epsilon_fn))\n",
    "#     print(\"penalty value = \" + str(penalty))\n",
    "    return (epsilon_fn, epsilon_fn + penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nonlinear_2(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network with 2 layers\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_width, use_sin=False):\n",
    "        super(Nonlinear, self).__init__()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = nn.sinFn()\n",
    "#         self.tanh = nn.Tanh()\n",
    "        self.use_sin = use_sin\n",
    "        \n",
    "        self.hidden1 = nn.Linear(1, layer_width)\n",
    "        self.hidden2 = nn.Linear(layer_width, layer_width)\n",
    "        self.output = nn.Linear(layer_width, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.use_sin == True:\n",
    "            x = self.hidden1(x)\n",
    "            x = self.sin(x)\n",
    "            \n",
    "            x = self.hidden2(x)\n",
    "            x = self.sin(x)\n",
    "            \n",
    "            x = self.output(x)\n",
    "            \n",
    "        else:\n",
    "            x = self.hidden1(x)\n",
    "            x = self.sigmoid(x)\n",
    "            \n",
    "            x = self.hidden2(x)\n",
    "            x = self.sigmoid(x)\n",
    "            \n",
    "            x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def normalize_model(self, upper_bound, discrete_points):\n",
    "        \"\"\"\n",
    "        GOAL: Normalize the output weight layer\n",
    "        model.output *= c\n",
    "        where,\n",
    "        scalar c = 1/denom\n",
    "        \"\"\"\n",
    "        \n",
    "        h = (2*upper_bound)/(len(discrete_points)-1)\n",
    "        if h != (discrete_points[1]-discrete_points[0]):\n",
    "            raise ValueError(\"h is wrong!\")\n",
    "        s = 0\n",
    "        for i in discrete_points:\n",
    "            x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "            s += self(x_i)**2\n",
    "        denom = math.sqrt(h) * torch.sqrt(s)\n",
    "        c = 1/denom\n",
    "\n",
    "        print(\"Before normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        \n",
    "        self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
    "        self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
    "\n",
    "        print(\"After normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        print(\"c value = \" + str(c))\n",
    "\n",
    "        return \n",
    "\n",
    "    def u_prime(self, x_in):\n",
    "        y = self(x_in)\n",
    "        y_prime = torch.autograd.grad(y.sum(), x_in, create_graph=True)\n",
    "        return y_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING MODEL CLASS\n",
    "class Nonlinear_1(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network with 1 layers\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_width, use_sin=False):\n",
    "        super(Nonlinear, self).__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = nn.sinFn()\n",
    "        self.use_sin = use_sin\n",
    "#         self.tanh = nn.Tanh()\n",
    "\n",
    "        self.hidden = nn.Linear(1, layer_width)\n",
    "        self.output = nn.Linear(layer_width, 1)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if self.use_sin == True:\n",
    "            x = self.hidden(x)\n",
    "            x = self.sin(x)\n",
    "            x = self.output(x)\n",
    "        else:\n",
    "            x = self.hidden(x)\n",
    "            x = self.sigmoid(x)\n",
    "            x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def normalize_model(self, upper_bound, discrete_points):\n",
    "        \"\"\"\n",
    "        GOAL: Normalize the output weight layer\n",
    "        model.output *= c\n",
    "        where,\n",
    "        scalar c = 1/denom\n",
    "        \"\"\"\n",
    "        \n",
    "        h = (2*upper_bound)/(len(discrete_points)-1)\n",
    "        if h != (discrete_points[1]-discrete_points[0]):\n",
    "            raise ValueError(\"h is wrong!\")\n",
    "        s = 0\n",
    "        for i in discrete_points:\n",
    "            x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "            s += self(x_i)**2\n",
    "        denom = math.sqrt(h) * torch.sqrt(s)\n",
    "        c = 1/denom\n",
    "\n",
    "        print(\"Before normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        \n",
    "        self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
    "        self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
    "\n",
    "        print(\"After normalization: \")\n",
    "        print(self.output.weight.data)\n",
    "        print(self.output.bias.data)\n",
    "        print(\"c value = \" + str(c))\n",
    "\n",
    "        return \n",
    "\n",
    "    def u_prime(self, x_in):\n",
    "        y = self(x_in)\n",
    "        y_prime = torch.autograd.grad(y.sum(), x_in, create_graph=True)\n",
    "        return y_prime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANING MODEL\n",
    "def minibatch_train_with_penalty(model, \n",
    "                               num_epochs, \n",
    "                               v_x, \n",
    "                               optimizer, \n",
    "                               lambda_pen,\n",
    "                               discrete_points,\n",
    "                               batch_size=32):\n",
    "    # For plotting loss value over epochs:\n",
    "    x_epochs = []\n",
    "    y_loss = []\n",
    "    y_loss_pen = []\n",
    "\n",
    "    m = len(discrete_points)\n",
    "    num_batches = int(m/batch_size)\n",
    "#         print(\"Number of batches \" + str(num_batches))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        perm = torch.randperm(m)\n",
    "        avg_loss_pen = 0\n",
    "        avg_loss = 0\n",
    "\n",
    "#                 start1 = time.time()\n",
    "        for i in range(0, m, batch_size):\n",
    "#                 print(\"i = \" + str(i))\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            indices = perm[i: i+batch_size]\n",
    "\n",
    "            loss_values = epsilon_Loss_penalty(v_x, model, lambda_pen,\n",
    "                                            U_BOUND, \n",
    "                                            discrete_points[indices])\n",
    "\n",
    "            loss = loss_values[0]\n",
    "            loss_pen = loss_values[1]\n",
    "\n",
    "            avg_loss_pen += loss_pen.item()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            loss_pen.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss_pen = avg_loss_pen/num_batches\n",
    "        avg_loss = avg_loss/num_batches\n",
    "\n",
    "        y_loss_pen.append(avg_loss_pen)\n",
    "        y_loss.append(avg_loss)\n",
    "        x_epochs.append(epoch)\n",
    "\n",
    "        if epoch % 100 == 0 or epoch < 30:\n",
    "            print('epoch {}, loss with penalty {}'.format(epoch, avg_loss_pen))\n",
    "\n",
    "#                 if epoch == 0:\n",
    "#                     end1 = time.time()\n",
    "#                     time1 = end1-start1\n",
    "#                     print(\"One epoch takes \" + str(time1))\n",
    "\n",
    "    print('Please normalize after training')\n",
    "    return (x_epochs, y_loss_pen, y_loss)\n",
    "\n",
    "def batch_train_with_penalty(model, \n",
    "                         num_epochs, \n",
    "                         v_x, \n",
    "                         optimizer, \n",
    "                         lambda_pen, \n",
    "                         discrete_points):\n",
    "    # For plotting loss value over epochs:\n",
    "    x_epochs = []\n",
    "    y_loss = []\n",
    "    y_loss_pen = []\n",
    "\n",
    "    #Early stopping criteria:\n",
    "    last_min_loss = 2.0\n",
    "    patience = 4\n",
    "    min_delta = 1e-3\n",
    "    stop_counter = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss_values = epsilon_Loss_penalty(v_x, model, lambda_pen,\n",
    "                                    U_BOUND, discrete_points)\n",
    "\n",
    "        loss = loss_values[0]\n",
    "        loss_pen = loss_values[1]\n",
    "\n",
    "        y_loss_pen.append(loss_pen.detach().numpy().item())\n",
    "        y_loss.append(loss.detach().numpy().item())\n",
    "        x_epochs.append(epoch)\n",
    "\n",
    "#             if epoch == 0:\n",
    "#                 last_min_loss = loss_pen\n",
    "#             else:\n",
    "#             #Early stopping\n",
    "#                 if abs(loss_pen-last_min_loss) <= min_delta:\n",
    "#                     stop_counter += 1\n",
    "#                     print(\"Stop counter + 1, current stop counter = \" + str(stop_counter))\n",
    "#                     if loss_pen < last_min_loss:\n",
    "#                         last_min_loss = loss_pen\n",
    "#                         best_model = copy.deepcopy(self.state_dict())\n",
    "\n",
    "#                     if stop_counter >= patience:\n",
    "#                         print(\"Early stopping. Training is complete\")\n",
    "#                         print('Please normalize after training')\n",
    "#                         return (x_epochs, y_loss_pen, y_loss)\n",
    "#                 else:\n",
    "#                     stop_counter = 0\n",
    "\n",
    "        if loss_pen < last_min_loss and abs(loss_pen-last_min_loss) >= min_delta:\n",
    "            print(\"New min loss = \" + str(loss_pen))\n",
    "            best_model = copy.deepcopy(model)\n",
    "            last_min_loss = loss_pen\n",
    "\n",
    "#             if epoch % 100 == 0 or epoch < 30:\n",
    "        print('epoch {}, loss with penalty {}'.format(epoch, loss_pen.item()))\n",
    "        loss_pen.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Please normalize after training')\n",
    "    return (x_epochs, y_loss_pen, y_loss, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.01\n",
    "\n",
    "#INIT MODEL\n",
    "batch_model = Nonlinear(40)\n",
    "model_1 = Nonlinear_1(20)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_32.cuda()\n",
    "    model_64.cuda()\n",
    "\n",
    "# INIT OPTIMIZER CLASS\n",
    "# SGD:\n",
    "# sgd_optimizer = torch.optim.SGD(model_32.parameters(), \n",
    "#                                 lr=learningRate, \n",
    "#                                 momentum=0.9)\n",
    "\n",
    "# Adam:\n",
    "adam_optimizer = torch.optim.Adam(batch_model.parameters(), \n",
    "                                    lr=learningRate, \n",
    "                                    betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, \n",
    "                                    weight_decay=0, \n",
    "                                    amsgrad=False)\n",
    "\n",
    "adam_optimizer_2 = torch.optim.Adam(model_64.parameters(), \n",
    "                                    lr=learningRate, \n",
    "                                    betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, \n",
    "                                    weight_decay=0, \n",
    "                                    amsgrad=False)\n",
    "\n",
    "adam_optimizer_3 = torch.optim.Adam(model_128.parameters(), \n",
    "                                    lr=learningRate, \n",
    "                                    betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, \n",
    "                                    weight_decay=0, \n",
    "                                    amsgrad=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Finite Difference Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qkg6Zfo7u8Fq"
   },
   "outputs": [],
   "source": [
    "def finite_diff_method(lower_bound, upper_bound, N_points, given_fn):\n",
    "    discrete_points = np.linspace(lower_bound, upper_bound, N_points)\n",
    "    h = discrete_points[1]-discrete_points[0]\n",
    "    c = np.sqrt(1/h)\n",
    "\n",
    "    H = np.zeros((N_points,N_points))\n",
    "    # Question: Why H has dimension (N-2) x (N-2)?\n",
    "    V = np.zeros((N_points,N_points))\n",
    "\n",
    "    for i in range (N_points):\n",
    "        for j in range (N_points):\n",
    "            if i == j:\n",
    "                H[i][j] = -2\n",
    "            elif abs(i-j) == 1:\n",
    "                H[i][j] = 1\n",
    "    for i in range (N_points):\n",
    "        for j in range (N_points):\n",
    "            if i == j:\n",
    "                V[i][j] = given_fn.forward(discrete_points[i])\n",
    "\n",
    "    A = -H/(h**2) + V\n",
    "    \n",
    "    eig_val, eig_vec = np.linalg.eig(A)\n",
    "    sorted_id_eig_val = np.argsort(eig_val)\n",
    "    # Get the indices that would sort eig_val\n",
    "    z = sorted_id_eig_val[0:1]\n",
    "    # eig_val[z] will return sorted first [T] value of eigenvalues.\n",
    "    energies = eig_val[z]/eig_val[z][0]\n",
    "\n",
    "    ground_state = c * eig_vec[:,z[0]]\n",
    "\n",
    "    return (eig_val[z], ground_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lIGR6Gohu8Fr"
   },
   "outputs": [],
   "source": [
    "def plot_finite_diff_sol(eig_val, ground_state, discrete_points, nn_model=None):\n",
    "    plt.figure(figsize=(12,10))\n",
    "    for i in range(len(eig_val)):\n",
    "        y = []\n",
    "        y = np.append(y, (-1) * -ground_state)\n",
    "\n",
    "        plt.plot(discrete_points,y,lw=3, label=\"{} \".format(i))\n",
    "        plt.xlabel('x', size=14)\n",
    "        plt.ylabel('$\\psi$(x)',size=14)\n",
    "    if nn_model != None:\n",
    "        model_output = model_on_interval(nn_model, DISCRETE_POINTS)\n",
    "        plt.plot(discrete_points, model_output)\n",
    "    plt.legend()\n",
    "#     plt.title('normalized wavefunctions for a harmonic oscillator using finite difference method',size=14)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val, ground_state = finite_diff_method(L_BOUND, U_BOUND, N_POINTS, pt_fn)\n",
    "plot_finite_diff_sol(eig_val, ground_state, DISCRETE_POINTS, nn_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
