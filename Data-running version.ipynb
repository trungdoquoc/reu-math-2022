{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "U3Z0FXavK5LD"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from pytz import timezone\n",
        "\n",
        "# from collections import OrderedDict\n",
        "from copy import deepcopy\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "P5y5jqCDK5LG"
      },
      "outputs": [],
      "source": [
        "# PARAMETERS FOR \n",
        "LAMBDA_PEN = 1000\n",
        "L_BOUND = -10\n",
        "U_BOUND = 10\n",
        "N_POINTS = 512\n",
        "\n",
        "NUM_EPOCHS = 5000\n",
        "\n",
        "M_POINTS = 10\n",
        "ALPHA = 2\n",
        "\n",
        "SELECTION_RATE = 0.01\n",
        "\n",
        "DISCRETE_POINTS = np.linspace(L_BOUND, U_BOUND, N_POINTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "03rjt3_6K5LG"
      },
      "outputs": [],
      "source": [
        "class sinFn(torch.nn.Module):\n",
        "    @staticmethod\n",
        "    def forward(input):\n",
        "        return torch.sin(input)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bIUFbmpwcCeA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "5O3XwauEK5LH"
      },
      "outputs": [],
      "source": [
        "# PLOT DATA\n",
        "def plot_figure(x_val, y_val, x_test=None, predicted=None, log_scale=False):\n",
        "    plt.clf()\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    if log_scale==True:\n",
        "        plt.yscale('log')\n",
        "    plt.plot(x_val, y_val, '--', label='True data', alpha=0.5)\n",
        "    if predicted != None:\n",
        "        plt.plot(x_test, predicted, '--', label='Predictions', alpha=0.5)\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "P1WpAhwrK5LI"
      },
      "outputs": [],
      "source": [
        "class X_Square_sin:\n",
        "    def __init__(self, const, use_sin=False):\n",
        "        self.const = const\n",
        "        if use_sin == True:\n",
        "            self.sin_const = np.random.choice(np.arange(10, 50))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x**2 + self.const + self.sin_const*math.sin(x)\n",
        "    \n",
        "class Potential_Function:\n",
        "    def __init__(self, c_0=0,\n",
        "                 M_points=M_POINTS,\n",
        "                 L_endpoint=U_BOUND, \n",
        "                 alpha=ALPHA, \n",
        "                 rescale=1):\n",
        "        self.M_points = M_points\n",
        "        self.L_endpoint = L_endpoint\n",
        "        self.alpha = alpha\n",
        "        self.c_0 = c_0\n",
        "        self.rescale = 1\n",
        "        \n",
        "        self.ti_list = np.random.normal(loc=0, scale=1.0, size= self.M_points)\n",
        "        self.ci_list = [(self.L_endpoint/(i * math.pi))**self.alpha \n",
        "                        for i in range(1, self.M_points+1)]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        f_value = 0\n",
        "        summation = 0\n",
        "        \n",
        "        #Iterative method:\n",
        "\n",
        "        for i in range(1, self.M_points+1):\n",
        "            cos_val = np.cos((i * math.pi * x)/self.L_endpoint)\n",
        "            summation += self.ti_list[i-1] * self.ci_list[i-1] * cos_val\n",
        "\n",
        "        f_value += summation\n",
        "        f_value += self.c_0\n",
        "        return self.rescale * f_value\n",
        "    \n",
        "    def set_c0_value(self, val):\n",
        "        self.c_0 = val\n",
        "        return\n",
        "    \n",
        "    def set_rescale_factor(self, val):\n",
        "        self.rescale = val\n",
        "        return \n",
        "    \n",
        "    def plot_function(self, discrete_points):\n",
        "        y_values = [self.forward(i) for i in discrete_points]\n",
        "        plt.plot(discrete_points, y_values)\n",
        "    \n",
        "    def update_potential_fn(self, discrete_points):\n",
        "        # (1) Check lowest value to set appropriate c_0 value:\n",
        "        y_values = [self.forward(i) for i in discrete_points]\n",
        "        min_val = min(y_values)\n",
        "        print(\"Original min val = \" + str(min_val))\n",
        "        # plt.plot(DISCRETE_POINTS, y_values)\n",
        "\n",
        "        # (1) Set c_0 value s.t. every value in y_values is > 0: \n",
        "        if min_val < 0:\n",
        "            c_0 = math.ceil(abs(min_val))\n",
        "            self.set_c0_value(c_0)\n",
        "            new_y = [self.forward(i) for i in discrete_points]\n",
        "            print(\"Min val with c_0 updated = \" + str(min(new_y)))\n",
        "        # plt.plot(DISCRETE_POINTS, new_y)\n",
        "\n",
        "        # (2) Rescale potential function s.t. every value lies between 0 and 5:\n",
        "        max_v = max(new_y)\n",
        "#         min_v = min(new_y)\n",
        "#         print(max_v, min_v)\n",
        "#         max_min = max_v-min_v\n",
        "\n",
        "#         rescaled_y = [(i-min_v)/(max_min) for i in new_y]\n",
        "#         plt.plot(DISCRETE_POINTS, rescaled_y)\n",
        "\n",
        "        c = 5/max_v\n",
        "        self.set_rescale_factor(c)\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gwppbWioK5LJ"
      },
      "outputs": [],
      "source": [
        "def epsilon_Loss_penalty(v_x, model_u, \n",
        "                         lambda_pen,\n",
        "                         upper_bound, \n",
        "                         discrete_points):\n",
        "    eps_sum = 0\n",
        "    pen = 0\n",
        "\n",
        "    h = (2*upper_bound)/(len(discrete_points)-1)\n",
        "#     print(\"h = \"+ str(h))\n",
        "    for i in discrete_points:\n",
        "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
        "\n",
        "        u_prime_square = torch.square(model_u.u_prime(x_i))\n",
        "        u_xi_square = torch.square(model_u(x_i))\n",
        "        \n",
        "        v_xi = v_x.forward(i)\n",
        "        if v_xi < 0:\n",
        "            raise ValueError('Potential Function value is < 0')\n",
        "\n",
        "        t = u_prime_square + v_xi*u_xi_square\n",
        "        eps_sum += t\n",
        "        \n",
        "        pen+= u_xi_square\n",
        "        \n",
        "    epsilon_fn = h*eps_sum\n",
        "    \n",
        "    penalty = lambda_pen * torch.square(h*pen-1)\n",
        "#     print(\"epsilon_fn value = \" + str(epsilon_fn))\n",
        "#     print(\"penalty value = \" + str(penalty))\n",
        "    return (epsilon_fn, epsilon_fn + penalty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "i0X45z26K5LJ"
      },
      "outputs": [],
      "source": [
        "class Nonlinear_2(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural Network with 2 layers\n",
        "    \"\"\"\n",
        "    def __init__(self, layer_width, use_sin=False):\n",
        "        super(Nonlinear_2, self).__init__()\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.sin = sinFn()\n",
        "#         self.tanh = nn.Tanh()\n",
        "        self.use_sin = use_sin\n",
        "        \n",
        "        self.hidden1 = nn.Linear(1, layer_width)\n",
        "        self.hidden2 = nn.Linear(layer_width, layer_width)\n",
        "        self.output = nn.Linear(layer_width, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.use_sin == True:\n",
        "            x = self.hidden1(x)\n",
        "            x = self.sin(x)\n",
        "            \n",
        "            x = self.hidden2(x)\n",
        "            x = self.sin(x)\n",
        "            \n",
        "            x = self.output(x)\n",
        "            \n",
        "        else:\n",
        "            x = self.hidden1(x)\n",
        "            x = self.sigmoid(x)\n",
        "            \n",
        "            x = self.hidden2(x)\n",
        "            x = self.sigmoid(x)\n",
        "            \n",
        "            x = self.output(x)\n",
        "        return x\n",
        "\n",
        "    def normalize_model(self, upper_bound, discrete_points):\n",
        "        \"\"\"\n",
        "        GOAL: Normalize the output weight layer\n",
        "        model.output *= c\n",
        "        where,\n",
        "        scalar c = 1/denom\n",
        "        \"\"\"\n",
        "        \n",
        "        h = (2*upper_bound)/(len(discrete_points)-1)\n",
        "        if h != (discrete_points[1]-discrete_points[0]):\n",
        "            raise ValueError(\"h is wrong!\")\n",
        "        s = 0\n",
        "        for i in discrete_points:\n",
        "            x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
        "            s += self(x_i)**2\n",
        "        denom = math.sqrt(h) * torch.sqrt(s)\n",
        "        c = 1/denom\n",
        "\n",
        "        print(\"Before normalization: \")\n",
        "        print(self.output.weight.data)\n",
        "        print(self.output.bias.data)\n",
        "        \n",
        "        self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
        "        self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
        "\n",
        "        print(\"After normalization: \")\n",
        "        print(self.output.weight.data)\n",
        "        print(self.output.bias.data)\n",
        "        print(\"c value = \" + str(c))\n",
        "\n",
        "        return \n",
        "\n",
        "    def u_prime(self, x_in):\n",
        "        y = self(x_in)\n",
        "        y_prime = torch.autograd.grad(y.sum(), x_in, create_graph=True)\n",
        "        return y_prime[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "clBtQzFcK5LK"
      },
      "outputs": [],
      "source": [
        "# CREATING MODEL CLASS\n",
        "class Nonlinear_1(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural Network with 1 layers\n",
        "    \"\"\"\n",
        "    def __init__(self, layer_width, use_sin=False):\n",
        "        super(Nonlinear_1, self).__init__()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.sin = sinFn()\n",
        "        self.use_sin = use_sin\n",
        "#         self.tanh = nn.Tanh()\n",
        "\n",
        "        self.hidden = nn.Linear(1, layer_width)\n",
        "        self.output = nn.Linear(layer_width, 1)\n",
        "\n",
        "            \n",
        "    def forward(self, x):\n",
        "        if self.use_sin == True:\n",
        "            x = self.hidden(x)\n",
        "            x = self.sin(x)\n",
        "            x = self.output(x)\n",
        "        else:\n",
        "            x = self.hidden(x)\n",
        "            x = self.sigmoid(x)\n",
        "            x = self.output(x)\n",
        "        return x\n",
        "\n",
        "    def normalize_model(self, upper_bound, discrete_points):\n",
        "        \"\"\"\n",
        "        GOAL: Normalize the output weight layer\n",
        "        model.output *= c\n",
        "        where,\n",
        "        scalar c = 1/denom\n",
        "        \"\"\"\n",
        "        \n",
        "        h = (2*upper_bound)/(len(discrete_points)-1)\n",
        "        if h != (discrete_points[1]-discrete_points[0]):\n",
        "            raise ValueError(\"h is wrong!\")\n",
        "        s = 0\n",
        "        for i in discrete_points:\n",
        "            x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
        "            s += self(x_i)**2\n",
        "        denom = math.sqrt(h) * torch.sqrt(s)\n",
        "        c = 1/denom\n",
        "\n",
        "        print(\"Before normalization: \")\n",
        "        print(self.output.weight.data)\n",
        "        print(self.output.bias.data)\n",
        "        \n",
        "        self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
        "        self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
        "\n",
        "        print(\"After normalization: \")\n",
        "        print(self.output.weight.data)\n",
        "        print(self.output.bias.data)\n",
        "        print(\"c value = \" + str(c))\n",
        "\n",
        "        return \n",
        "\n",
        "    def u_prime(self, x_in):\n",
        "        y = self(x_in)\n",
        "        y_prime = torch.autograd.grad(y.sum(), x_in, create_graph=True)\n",
        "        return y_prime[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-yehv8TrK5LL"
      },
      "outputs": [],
      "source": [
        "# TRANING MODEL\n",
        "def minibatch_train_with_penalty(model, \n",
        "                               num_epochs, \n",
        "                               v_x, \n",
        "                               optimizer, \n",
        "                               lambda_pen,\n",
        "                               discrete_points,\n",
        "                               batch_size=32):\n",
        "    # For plotting loss value over epochs:\n",
        "    x_epochs = []\n",
        "    y_loss = []\n",
        "    y_loss_pen = []\n",
        "\n",
        "    m = len(discrete_points)\n",
        "    num_batches = int(m/batch_size)\n",
        "#         print(\"Number of batches \" + str(num_batches))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        perm = torch.randperm(m)\n",
        "        avg_loss_pen = 0\n",
        "        avg_loss = 0\n",
        "\n",
        "#                 start1 = time.time()\n",
        "        for i in range(0, m, batch_size):\n",
        "#                 print(\"i = \" + str(i))\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            indices = perm[i: i+batch_size]\n",
        "\n",
        "            loss_values = epsilon_Loss_penalty(v_x, model, lambda_pen,\n",
        "                                            U_BOUND, \n",
        "                                            discrete_points[indices])\n",
        "\n",
        "            loss = loss_values[0]\n",
        "            loss_pen = loss_values[1]\n",
        "\n",
        "            avg_loss_pen += loss_pen.item()\n",
        "            avg_loss += loss.item()\n",
        "\n",
        "            loss_pen.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_loss_pen = avg_loss_pen/num_batches\n",
        "        avg_loss = avg_loss/num_batches\n",
        "\n",
        "        y_loss_pen.append(avg_loss_pen)\n",
        "        y_loss.append(avg_loss)\n",
        "        x_epochs.append(epoch)\n",
        "\n",
        "        if epoch % 100 == 0 or epoch < 30:\n",
        "            print('epoch {}, loss with penalty {}'.format(epoch, avg_loss_pen))\n",
        "\n",
        "#                 if epoch == 0:\n",
        "#                     end1 = time.time()\n",
        "#                     time1 = end1-start1\n",
        "#                     print(\"One epoch takes \" + str(time1))\n",
        "\n",
        "    print('Please normalize after training')\n",
        "    return (x_epochs, y_loss_pen, y_loss)\n",
        "\n",
        "def batch_train_with_penalty(model, \n",
        "                         num_epochs, \n",
        "                         v_x, \n",
        "                         optimizer, \n",
        "                         lambda_pen, \n",
        "                         discrete_points):\n",
        "    # For plotting loss value over epochs:\n",
        "    x_epochs = []\n",
        "    y_loss = []\n",
        "    y_loss_pen = []\n",
        "\n",
        "    #Early stopping criteria:\n",
        "    last_min_loss = 2.0\n",
        "    patience = 4\n",
        "    min_delta = 1e-3 #minimum difference to be considered better loss\n",
        "    stop_counter = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        loss_values = epsilon_Loss_penalty(v_x, model, lambda_pen,\n",
        "                                    U_BOUND, discrete_points)\n",
        "\n",
        "        loss = loss_values[0]\n",
        "        loss_pen = loss_values[1]\n",
        "\n",
        "        y_loss_pen.append(loss_pen.detach().numpy().item())\n",
        "        y_loss.append(loss.detach().numpy().item())\n",
        "        x_epochs.append(epoch)\n",
        "\n",
        "#             if epoch == 0:\n",
        "#                 last_min_loss = loss_pen\n",
        "#             else:\n",
        "#             #Early stopping\n",
        "#                 if abs(loss_pen-last_min_loss) <= min_delta:\n",
        "#                     stop_counter += 1\n",
        "#                     print(\"Stop counter + 1, current stop counter = \" + str(stop_counter))\n",
        "#                     if loss_pen < last_min_loss:\n",
        "#                         last_min_loss = loss_pen\n",
        "#                         best_model = copy.deepcopy(self.state_dict())\n",
        "\n",
        "#                     if stop_counter >= patience:\n",
        "#                         print(\"Early stopping. Training is complete\")\n",
        "#                         print('Please normalize after training')\n",
        "#                         return (x_epochs, y_loss_pen, y_loss)\n",
        "#                 else:\n",
        "#                     stop_counter = 0\n",
        "\n",
        "        if loss_pen < last_min_loss and abs(loss_pen-last_min_loss) >= min_delta:\n",
        "            print(\"New min loss = \" + str(loss_pen))\n",
        "            best_model = copy.deepcopy(model)\n",
        "            last_min_loss = loss_pen\n",
        "\n",
        "#             if epoch % 100 == 0 or epoch < 30:\n",
        "        print('epoch {}, loss with penalty {}'.format(epoch, loss_pen.item()))\n",
        "        loss_pen.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Please normalize after training')\n",
        "    return (x_epochs, y_loss_pen, y_loss, best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "8Ph_ZvmuK5LL"
      },
      "outputs": [],
      "source": [
        "learningRate = 0.01\n",
        "\n",
        "#INIT MODEL\n",
        "model_1_1_sin = Nonlinear_1(20, use_sin=True)\n",
        "model_1_1_sigmoid = Nonlinear_1(20)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model_1_1_sin.cuda()\n",
        "    model_1_1_sigmoid.cuda()\n",
        "\n",
        "# INIT OPTIMIZER CLASS\n",
        "# Adam:\n",
        "adam_opt_11_sin = torch.optim.Adam(model_1_1_sin.parameters(), \n",
        "                                    lr=learningRate, \n",
        "                                    betas=(0.9, 0.999), \n",
        "                                    eps=1e-08, \n",
        "                                    weight_decay=0, \n",
        "                                    amsgrad=False)\n",
        "\n",
        "adam_opt_11_sigmoid = torch.optim.Adam(model_1_1_sigmoid.parameters(), \n",
        "                                    lr=learningRate, \n",
        "                                    betas=(0.9, 0.999), \n",
        "                                    eps=1e-08, \n",
        "                                    weight_decay=0, \n",
        "                                    amsgrad=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INIT POTENTIAL FUNCTIONS\n",
        "p_fn1 = Potential_Function()\n",
        "p_fn1.plot_function(DISCRETE_POINTS)"
      ],
      "metadata": {
        "id": "88EiIGL3cvoA",
        "outputId": "7865ac7f-7d7b-4529-a045-6bfea26c1775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_fn1.update_potential_fn(DISCRETE_POINTS)\n",
        "p_fn1.plot_function(DISCRETE_POINTS)"
      ],
      "metadata": {
        "id": "3rg8s_yZdCXR",
        "outputId": "496ee383-4f59-4f82-a249-cf1e5fea0460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original min val = -17.070161763267283\n",
            "Min val with c_0 updated = 0.9298382367327171\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZd7/8fedTnpCEiCkF3onFClKExAsuHYXUaSs2F3b+ttdVx91d3VXH0GxoNj7rl0UpAmIUkINkEBCQoBACgmkF5Lcvz8y2SeyiZmEmTlTvq/rykXITGY+OZl88s2Zc+5RWmuEEELYLzejAwghhPh1UtRCCGHnpKiFEMLOSVELIYSdk6IWQgg752GNGw0LC9NxcXHWuGkhhHBKO3bsOKW1Dm/tMqsUdVxcHKmpqda4aSGEcEpKqdy2LpNdH0IIYeekqIUQws5JUQshhJ2TohZCCDsnRS2EEHbOrKM+lFJHgHKgAajXWqdYM5QQQoj/05HD8yZqrU9ZLYkQQohWWeU4aiHsSX5pDVuyizl+ugqlFNGhvoyODyUi0MfoaEKYxdyi1sD3SikNvKq1XnbuFZRSC4GFADExMZZLKEQn7Tx6msVrMtlwqOi/LnNTMLF3BPdd3IsBPYMMSCeE+ZQ5LxyglOqptc5TSkUAq4G7tNYb27p+SkqKljMThVHq6ht5asUB3v45lzB/L24aHcfF/bqREO4HQGZBBav25/Pe1lxKq8+y8MIE7r+4N14e8ty6MI5Sakdbz/+ZNVFrrfNM/xYqpT4HRgJtFrUQRikqr2XReztIzT3NLWPieGBab/y9f/kwHxgVxMCoIBZcmMDfv0vn1Q3Z7Msr5eXZwwn08TQouRBta3eEUEr5KaUCmt8HpgL7rB1MiI4qLK/huld/Zt+JUl64YSiPXd7/v0q6paAunvztN4N49prBbM0u4dpXfqa4otaGiYUwjzl/63UDflRK7QG2ASu01iutG0uIjimprOO3r23lZGkN784bxWWDI83+3KuGR/HGLSPIOVXJ3Le2U1Fbb8WkQnRcu0Wttc7WWg82vfXXWj9li2BCmKuuvpHb3t3B0ZIqlt+Swoi40A7fxoW9wll64zD2nyhj0Xs7qG9otEJSITpHnj0RDk1rzV++2se2IyU8c/UgxiSGdfq2pvTrxl+vHMCmzFP84/uDFkwpxPmRohYO7bOdeXy47RiLJiRyxZCe5317142I4bejYnh1QzbfpZ20QEIhzp8UtXBYx0qq+MtX+xkZF8oDU3tb7HYfvawfg6ODefjTveSX1ljsdoXoLClq4ZDqGxq57+PdKOC56wbj7qYsdtveHu48f90QzjZoHvp0L+acayCENUlRC4f06sZsUnNP88SsAUSF+Fr89uPD/HhkRh82Hirig21HLX77QnSEFLVwOEdOVbJ4bSaXDOjOrKHnv1+6LbNHxTI2qStPrUjn+Okqq92PEO2RohYORWvNH79Iw9vdjccu72/V+3JzUzx91SC0hse/PmDV+xLi10hRC4fyxe48NmcV89Alfehmg9XvokJ8uXtyMqsPFLDmQIHV70+I1khRC4dRVnOWJ79JZ0h0ML8dabsVGueNiyc5wp/Hvt5PdV2Dze5XiGZS1MJhLF2XRUlVHU/OGoCbBY/yaI+XhxtPzBrA8dPVvPxDls3uV4hmUtTCIeQWV/LG5hyuHhZlyPrRoxO6cvngSJZtyubEmWqb379wbVLUwiH87dsMPN3deHCa5U5s6aiHpvemUcM/V8np5cK2pKiF3duSXczK/fksuijR0JfPigrxZf64eD7blcfe42cMyyFcjxS1sGuNjZonVxwgMsiHBRcmGB2HRRMSCfP34slv0uWMRWEzUtTCrn22K499eWU8fEkffDzdjY5DgI8n913ci21HSli1P9/oOMJFSFELu1Vb38D/rj7E4KggLu/ACwFY23Up0SRH+PPMqoOybrWwCSlqYbc+2HqUvDPVPDitD0rZ7nC89ni4u3H/1N5kF1Xy2c48o+MIFyBFLexSZW09S9dncUFCV8YmdTU6zn+Z1r8bg6ODeX7NIWrOykkwwrqkqIVdenNzDqcq6nhwem+7mqabKaV4aFpvTpTW8P5WWV1PWJcUtbA7Z6rqeHVjNlP6dmNYTIjRcdo0NimMsUldWbo+S14QV1iVFLWwO69uzKaitp77p/YyOkq7HpzWh5LKOpZvyjE6inBiUtTCrhSW1/Dm5hwuHxxJ3x6BRsdp15DoYKb178Zrm7IpqawzOo5wUlLUwq68uC6L+gbNfVPsf5pu9sDU3lTV1cuCTcJqpKiF3ThWUsWH245y7Yho4sL8jI5jtuRuAcwa2pN3fs6lsExeDFdYnhS1sBvPr8lEKcXdk5KNjtJhd09Kpr5R8/KGw0ZHEU5IilrYhcNFFXy+6zhzRsfSPci4hZc6Ky7Mj98M7cn7W49SIFO1sDApamEXXlyXhZeHG7dNSDQ6SqfdNSmZxkbNS+tlX7WwLClqYbjDRRV8uTuPORfEEebvbXScTovp6svVw6P4cNsxTpbKiwsIy5GiFoZrnqYX2sEypufrjolJNGrNUpmqhQVJUQtDOcs03Sw61JdrR0Tz8fZj5MlLdgkLkaIWhnphbabTTNPN7piYhELx4jqZqoVlSFELw2QVVvDVnhNOM0036xnchetGRPOv1GMcK6kyOo5wAmYXtVLKXSm1Syn1jTUDCdfx4rpMvD3cnWqabnb7xETc3GSqFpbRkYn6HiDdWkGEa/m/aTrWqabpZj2CunDjyBj+vfM4ucWVRscRDs6solZKRQEzgdetG0e4iuZp2h5esNZaFk1IxMNN8YJM1eI8mTtRPw88BLT5AnFKqYVKqVSlVGpRUZFFwgnn5OzTdLNugT78dlQsn+/KI+eUTNWi89otaqXUpUCh1nrHr11Pa71Ma52itU4JDw+3WEDhfF5wgWm62W0TEvB0V7ywNtPoKMKBmTNRjwUuV0odAT4CJiml3rNqKuG0/jNNj3HuabpZRIAPN42O5YvdeRwuqjA6jnBQ7Ra11voRrXWU1joOuB5Yp7WebfVkwim9sC4THw93Fo53/mm62e8uSsTbw12matFpchy1sJmW03RXF5imm4X5ezPngli+2nOCrEKZqkXHdaiotdY/aK0vtVYY4dxccZputuDChKapep1M1aLjZKIWNpFVWO6S03SzMH9v5oxpnqrLjY4jHIwUtbCJJWuz6OLpmtN0s99dmEgXT3cWr5XjqkXHSFELq8sqLOfrvU1rerjiNN0s1M+Lm8fE8c3eE2QWyFQtzCdFLayueZpeMD7e6CiGWzA+AV9PdxbLESCiA6SohVXJNP1LzVP1irSTHJKpWphJilpYlUzT/23B+AT8vDxYvEamamEeKWphNZkFMk23JsTPi1tMU/XBfJmqRfukqIXVLFlnOtLDBdb06Kj54+Px9/Zg8dpDRkcRDkCKWlhFZkE53+w9wc1j4gj18zI6jt0J9vVi7tg4vk3LJ/1kmdFxhJ2TohZW0TxNL3Dh46bbM39cAgHesq9atE+KWljcIZmmzRLk68nccfGs3J/PgRMyVYu2SVELi1uyNhNfmabNMm9sPAE+sq9a/DopamFRhwrKWZF2UqZpMwX5enLr2HhW7S9g/4lSo+MIOyVFLSxqsUzTHXbruKap+nnZVy3aIEUtLOZgfjnfpp3klrFxhMg0bbagLp7MH5fA6gMF7MuTqVr8NylqYTFL1mbi5+XB/HEyTXfU3HFxBMpULdogRS0s4mB+077pW8bINN0ZgT6ezB+fwJr0AtKOy1QtfkmKWljEkrWZ+Ht7MF/W9Oi0uWPjCOriyfNr5AgQ8UtS1OK8ZeSXsSLtJHPHxhHsK9N0ZwX4eLJgfDxrMwrZe/yM0XGEHZGiFudtydpMArw9mDdOpunzdfOYOIJ9PWVftfgFKWpxXtJPlvFtWr5M0xbSNFUnsC6jkN3HZKoWTaSoxXl59vtDBPh4ME+O9LCYm8fEEeIr+6rF/5GiFp22I7eENekF3HZRIkG+nkbHcRr+3h4suDCBHw4WsevoaaPjCDsgRS06RWvNMysPEubvzdyxcUbHcTpzLmieqmVftZCiFp20MfMUW3NKuGtSEr5eHkbHcTr+3h4svDCRDYeK2JErU7Wrk6IWHdbYqPnHqgyiQrpww8gYo+M4rTkXxBLq58Vzqw+itTY6jjCQFLXosO/25bMvr4z7pvTCy0MeQtbi5+3BnROT2JxVzIZDRUbHEQaSnzLRIWcbGnn2+4P06ubPrKE9jY7j9GaPjiW2qy9/+zaDhkaZql2VFLXokPe25JJ9qpKHpvXB3U0ZHcfpeXm48dC0PhwsKOfTHceNjiMMIkUtzHa6so7n12QyPjmMyX0jjI7jMmYM7M7QmGCeXX2Qqrp6o+MIA0hRC7MtXptJec1Z/jSzH0rJNG0rSin+NLMvBWW1vLYxx+g4wgDtFrVSykcptU0ptUcptV8p9bgtggn7klVYzrtbcrlxVAy9uwcYHcflDI8NZebAHry8IYtjJVVGxxE2Zs5EXQtM0loPBoYA05VSo60bS9gTrTVPfJOOr5c7903pZXQcl/XHmX1xU4rHvz5gdBRhY+0WtW5SYfqvp+lNnn52Id/ty2fDoSLumZxMV39vo+O4rMjgLtw9OZk16QWsTS8wOo6wIbP2USul3JVSu4FCYLXWemsr11molEpVSqUWFckxn86irOYsj321nwE9A7llTJzRcVzerWPjSYrw57Gv91NztsHoOMJGzCpqrXWD1noIEAWMVEoNaOU6y7TWKVrrlPDwcEvnFAZ5ZmUGpypq+duVg/Bwl+eejebl4cYTVwzgWEk1L6yTdUBcRYd+8rTWZ4D1wHTrxBH2ZPuREt7fepSbx8QxMCrI6DjC5ILErlw1LIpXNmSzR9asdgnmHPURrpQKNr3fBbgYyLB2MGGsspqz3PfxbqJDfLl/am+j44hzPHpZPyICvPn9J7tlF4gLMGei7gGsV0rtBbbTtI/6G+vGEkb7y5f7OVlaw/9eNwR/b1kdz94EdfHkH1cP5nBRJU+vlLnJ2bX7E6i13gsMtUEWYSe+3J3H57vyuHdKMsNjQ4yOI9owLjmMmy+I5c3NR7ggoStT+3c3OpKwEnl2SPxCRn4Zj3yWxvDYEO6cmGR0HNGOR2b0ZVBUEL//ZA+Hiyra/wThkKSoxX+cqapj4Ts78Pf24KXfDpOjPByAj6c7L88ejpeHG797dwcVtbIWiDOSn0QBQFVdPfPeTiW/tIZXbhpOt0AfoyMJM/UM7sKLNw4l51Qli97bQW29PLnobKSoBXX1jSx6bye7jp7m+euHMCxG9ks7mjGJYTx91SA2ZZ7i3o92U9/QaHQkYUFS1C6usraeeW9vZ8OhIp66ciAzBvYwOpLopKuHR/GnmX35bl8+i97fKYftOREpahd2srSaG1/bwk+Hi3nm6kHy+odOYP74BB6/vD9r0guYs3wbReW1RkcSFiBF7aLWZxQyY/EmsgoreGX2cK5NiTY6krCQm8fEseT6oew5foYZSzbx0+FTRkcS50mK2sWcLK3mzg92Mvet7UQE+PDVXeO4uF83o2MJC7tscCRf3DGWAG8PbnxtK/d/skemawemrPEy9CkpKTo1NdXitys672B+Oe/8fIR/pTa97t7tExO57aJEfDzdjQ0mrKqqrp4la7N4fVM2Hu6KG0bGMHt0LInh/kZHE+dQSu3QWqe0epkUtXOqrW8g7XgpW7KLWZGWT/rJMjzdFdekRLPookSiQ32NjihsKOdUJUvXZ/H5rjwaGjWDooKYMbAHYxPD6BcZKC9UbAekqF1AbX0Du46eYUt2MVuzS9h59DS19U2HaA2OCuLKoT25bHCkLPzv4grLavhqzwk+35XH/hNlQNO6IaMTQhkV35VRCaH07R6ImxS3zUlRO6mTpdV8s+ckGzOL2H6khJqzjSgFfbsHMsr0gzcyPpRQPy+jowo7VFhWw0+Hi9mcdYotOcUcK6kGmop7RFwoY5O6MnNQDyIC5OQnW5CidiKNjZrV6QW8+3Mumw+fQmtIjvBnbFIYY5PCGBkfSlAXT6NjCgeUd6aaraa/yLbkFJNbXIW7m2J8chg3j4ljQq9wefV5K5KidhKbMot4akU6Gfnl9AzuwtXDo7hyaE/iwvyMjiacUFZhBZ/vOs6nO/LIL6thYM8g/nxpP0bGhxodzSlJUTu40qqz/OnLfXy95wSxXX25d0oylw2KlEWThE3U1Tfyxa48Fq/NJO9MNdcMj+LRy/oR4CN/uVnSrxW1rAhv53YdPc2dH+yisLyGe6ckyyF1wua8PNy4dkQ0lw7uwZK1WSzbeJjU3NMsvXEY/SIDjY7nEmQks2Mr953kumVbUAr+fdsY7p3SS0paGMbXy4M/XNKHDxeMpqqunqtf+Yn1BwuNjuUSpKjt1IfbjrLo/Z0MiAzk6zvHMTg62OhIQgAwKqErX981jvgwP+a/ncoXu/KMjuT0pKjt0L93HOeRz9KY0Cuc9+ePJkQOrxN2JiLAh48WjmZEXAi//2Q336WdNDqSU5OitjPf78/noX/vYVxSGC/PHk4XL9nVIexTgI8ny28ewdCYEO7+aBebMouMjuS0pKjtyL68Uu75aDcDo4JZNme47I8Wds/P24M3544gMdyf29/fSVahvG6jNUhR24lTFbUseCeVYF9PXpszHF8vOSBHOIZAH09evzkFL3c35r+9ndLqs0ZHcjpS1HagsVFz/yd7KK6s47U5KXLKrnA4USG+vHrTcI6frub/fZaGNc7PcGVS1HZg+Y85bDhUxJ8v7ceAnkFGxxGiU1LiQrl/am9WpJ3ko+3HjI7jVKSoDbb3+BmeWZXBtP7dmD1KXgpLOLbfXZjA+OQwHv96P4cKyo2O4zSkqA1UW9/A7z/ZQ5i/N09fNUgWvBEOz81N8ey1g/Hz8uCBf+2RV0O3EClqAy1df5iswgr+9puBBPvKsdLCOUQE+PD4Ff3Ze7yUNzbnGB3HKUhRG+Rgfjkv/5DFlUN7MqF3hNFxhLComQN7MKVvN55bfYjc4kqj4zg8KWoDNDZqHv50LwE+nvz50n5GxxHC4pRSPDlrAJ5ubjwiR4GcNylqA/x7x3F2HzvDny/tK6++IpxW9yAfHrqkDz8dLmaFnGJ+XqSobay85izPrMogJTaEWUN6Gh1HCKu6cWQM/XoE8tcV6VTXNRgdx2FJUdvYC+uyKK6s4y+X9ZejPITTc3dTPHZ5f06U1vDyhsNGx3FY7Ra1UipaKbVeKXVAKbVfKXWPLYI5o+yiCt7cnMM1w6MYGCUntgjXMDI+lMsHR/LKhsMcK6kyOo5DMmeirgfu11r3A0YDdyil5BmwTnhqRTreHu48OK2P0VGEsKlHZvRBAc9+f9DoKA6p3aLWWp/UWu80vV8OpAOyc7WDtmQXszajkDsnJREe4G10HCFsqkdQF24dF8+Xe06w/0Sp0XEcTof2USul4oChwNZWLluolEpVSqUWFcm6tC1prfnHqoN0C/TmljFxRscRwhC3XZRIoI8nz6yUqbqjzC5qpZQ/8Clwr9a67NzLtdbLtNYpWuuU8PBwS2Z0eOsyCtmRe5q7JyfLGtPCZQV18eTOiUlsOFTET1mnjI7jUMwqaqWUJ00l/b7W+jPrRnIujY1N03RcV1+uTYk2Oo4Qhrrpglgig3z4+8oMOQmmA8w56kMBy4F0rfVz1o/kXL7ee4KM/HLuu7gXnu5yNKRwbT6e7vx+am/2Hi9l5b58o+M4DHOaYyxwEzBJKbXb9DbDyrmcwtmGRp5bfYi+PQK5bFCk0XGEsAtXDu1JYrgfz6/JpLFRpmpzmHPUx49aa6W1HqS1HmJ6+9YW4RzdJ6nHyC2u4sFpvXBzk5NbhICmk2DunpzMwYJyVu6Xqdoc8re4lZxtaOSl9YcZGhPMRFkdT4hfuHRQJEkR/iyWqdosUtRW8sWuPPLOVHP3pGQ5VVyIc7Scqr+TfdXtkqK2goZGzUs/HKZ/ZCATesuhikK0ZubAHiRH+LN47SGZqtshRW0FK9JOknOqkrsmJck0LUQbmqfqQwUVsgxqO6SoLayxUbN0XRbJEf5M7dfd6DhC2LXmqXrJWtlX/WukqC1sdXoBBwvKuWNikhzpIUQ73NwUd01OJrOwgu8PyL7qtkhRW5DWmhfXZRHb1ZdLB/UwOo4QDmHmwB7EdfXlxfVZcrZiG6SoLWjDoSLS8kq5fUIiHnIWohBmcXdTLJqQyL68MjZmyhogrZE2sRCtNS+syyIyyIcrh0YZHUcIh3Ll0Cgig3xYui7L6Ch2SYraQrZkl7Aj9zS3TUjEy0M2qxAd4eXhxsILE9h2pIRtOSVGx7E70igW8uL6TMIDvGWFPCE66fqRMYT5e/HiepmqzyVFbQE7ck+zOauYheMTZL1pITrJx9OdeeMS2HioiL3Hzxgdx65IUVvA0vVZhPh6cuOoGKOjCOHQZo+OIdDHg6UyVf+CFPV52pdXyrqMQuaNi8fP28PoOEI4tAAfT24ZE8eq/QVkFpQbHcduSFGfp6Xrswjw8WCOvBaiEBYxd2w8vl7uvPTDYaOj2A0p6vOQaVr565YxcQT6eBodRwinEOLnxW9HxfDVnhMcLa4yOo5dkKI+Dy/9cBhfL3fmjo03OooQTmXB+ATcleLlDTJVgxR1p+UWV/Ll7jxmj44l1M/L6DhCOJWIQB+uSYni0x3HyS+tMTqO4aSoO+nlHw7j4e7G/PEyTQthDbddlEiD1izbmG10FMNJUXdC3plqPt15nOtHRBMR4GN0HCGcUnSoL1cMieSDbbkUV9QaHcdQUtSd8Irp2ejbLko0OIkQzu32CUnU1jfy+o85RkcxlBR1BxWU1fBx6jGuHh5FZHAXo+MI4dSSIvy5dFAk7/x0hNOVdUbHMYwUdQe9uiGbhkbNoouSjI4ihEu4a1ISlXUNvLnZdadqKeoOKCqv5YNtucwa0pOYrr5GxxHCJfTqFsAlA7rz5uYjlFafNTqOIaSoO+D1TdnU1Tdyx0TZNy2ELd05KYny2nre/umI0VEMIUVtppLKOt7dkstlgyNJCPc3Oo4QLqV/ZBBT+nZj+Y85lNe43lQtRW2mN37MofpsA3dOlH3TQhjh7slJlFaf5d0tuUZHsTkpajOUVp/l7Z+OcMmA7iR3CzA6jhAuaVBUMBN7h/P6phwqa+uNjmNTUtRmeGvzEcpr67lzYrLRUYRwaXdNTqakso73t7rWVC1F3Y7S6rMs/zGbKX270S8y0Og4Qri0YTEhjE8OY9nGHKrrGoyOYzNS1O1Yvimbspp67rtYpmkh7MFdk5I5VVHrUlN1u0WtlHpDKVWolNpni0D2pLiiluU/5jBzUA/6RwYZHUcIAYyMD2V8chgv/XCYChfZV23ORP0WMN3KOezSqxuzqT7bwH1TZJoWwp48MLU3JZV1vOEia4C0W9Ra641AiQ2y2JXCshre/ukIs4b2JClCjvQQwp4Mjg5mar9uvLYxmzNVzr8GiMX2USulFiqlUpVSqUVFRZa6WcMsXZ9FQ6Pm3sm9jI4ihGjF/VN7U1FXzysbnH+9aosVtdZ6mdY6RWudEh4ebqmbNcTx01V8sO0o146IljU9hLBTvbsHMGtIT976KYfCMud+FRg56qMVz60+hFJKzkIUws7dOyWZ+gbNi+uzjI5iVVLU59iXV8rnu/K4dWy8rDcthJ2L7erHdSOi+WDrUXJOVRodx2rMOTzvQ+BnoLdS6rhSap71YxlDa81TK9IJ7uLJ7bJCnhAO4Z4pyXh7uPH379KNjmI15hz1cYPWuofW2lNrHaW1Xm6LYEZYf7CQn7OLuXdKLwJ9PI2OI4QwQ0SAD4smJLJqfwFbsouNjmMVsuvDpL6hkb99m0F8mB83jooxOo4QogPmj08gMsiHJ1ccoLFRGx3H4qSoTT7cfozMwgoent4HT3fZLEI4Eh9Pdx6c3pt9eWV8vivP6DgWJ41E04sC/HPVQUYnhDKtfzej4wghOuGKwT0ZFBXEM6synO7Ucilq4JmVGVTW1vM/VwxAKWV0HCFEJ7i5Kf5yWX8KympZsjbT6DgW5fJFvevoaT7afoy5Y+PoJS8KIIRDGx4bwvUjoln+Yw4H88uNjmMxLl3UDY2aR7/cT0SAN/dMkVPFhXAGD0/vQ6CPB3/6Ig2tneOJRZcu6ve25JKWV8ofZ/bF39vD6DhCCAsI8fPiD5f0YfuR03y60zmeWHTZoj5WUsXTKzMYnxzG5YMjjY4jhLCga4ZHMywmmL9+m86pilqj45w3lyxqrTWPfJaGAv5+1SB5AlEIJ+Pmpnj6qkFU1NTz5y/2OfwuEJcs6o+3H+PHrFM8MqMvPWU9DyGcUnK3AO67uBff7cvn670njY5zXlyuqE+cqeapFemMTgjlxpFyBqIQzmzB+HiGRAfz6Jf7KCx33KVQXaqoGxo19328mwatefqqQbi5yS4PIZyZh7sb/7xmMFV1Dfy/zxz3KBCXKuqX1mexNaeE/7liALFd/YyOI4SwgaQIfx6e3oc16YW8/dMRo+N0issUdeqREp5fm8kVQyK5alhPo+MIIWzo1rFxTO4TwV+/zSDteKnRcTrMJYq6qLyWuz7cRc/gLjw5S04TF8LVKKX45zWDCfP34o4PdlJWc9boSB3i9EVdV9/IHe/v5HRVHS/PHkaArDMthEsK8fNiyQ1DOXGmmrs/3EWDAy2H6vRF/eSKA2w7UsLTVw2if2SQ0XGEEAZKiQvlscv788PBIp5emWF0HLM59XnTb/yYwzs/57LwwgSuGCL7pYUQMHt0LAfzy1m2MZvkCH+uSYk2OlK7nLaov95zgidWHGB6/+48PL2P0XGEEHbk0cv6kX2qgj98lkaonxeT+9r3OvROuevjx8xT3P/JHkbEhvL89UNwl+OlhRAteLq78crs4fTrEcjt7+9kW06J0ZF+ldMV9cZDRcx7ezsJ4X68NicFH093oyMJIexQgI8nb80dQc+QLsx7azupR+y3rJ2qqH84WMj8d1JJCPfngwWjCfKVIzyEEG3r6u/Ne/NGERbgzU3Lt7E565TRkVrlNEX9yfZjzH87leQIfz6YP4pQPy+jIwkhHEBkcIXw6nsAAAjCSURBVBc+/t1oYkJ9mfvWdr5Ns78FnBy+qBsaNX//LoOHPt3LBYld+XDhaEKkpIUQHRAR4MNHC0fTP7Jpn/X/rj5Eox0dZ+3QRZ1fWsPs17fyyobD3DgqhjduGUGgnNAihOiEED8vPlwwmquGRbF4bSYL391BsZ286IBDFrXWmq/3nGD64o3sPnaGv/9mIE/NGoCnu0N+OUIIO+Hj6c4/rxnEny/tx8ZDRUx7fiPf7883OpbjHUedkV/G418d4OfsYgb2DOL564eQGO5vdCwhhJNQSjFvXDxjk7ry+4/3sPDdHUzsHc4jM/rSq1uAMZmssT5rSkqKTk1Ntdjtaa3ZefQ0r2/KYeX+fIK6ePLA1N7cMDJGjpEWQlhNXX0jb27O4cX1WVTW1jNzUCRzLoglJTbE4ou7KaV2aK1TWr3MHotaa01xZR0HTpSx+fApVu3L50hxFYE+Hsy5II554+LlCUMhhM2crqzjpR+y+Gj7Mcpr6kmO8Ofift0YmxRG/8hAgn3Pv48cpqhnLtnEmaqzlFWfpby2HgBPd8WIuFBmDe3JjIE98Pd2uL01QggnUVVXz5e7T/Dl7jy2Hzn9nxX4/L09CPXzonugD5/cdkGnbvvXitquWq9XtwCUgkAfT2JCfUnu5s/w2BB8vewqphDCRfl6eXDDyBhuGBlDafVZ0o6XcuBkKSfO1FBSWYevl3XOhLariVoIIVzVr03UZh3PppSarpQ6qJTKUkr9wbLxhBBC/Jp2i1op5Q4sBS4B+gE3KKX6WTuYEEKIJuZM1COBLK11tta6DvgIuMK6sYQQQjQzp6h7Asda/P+46WO/oJRaqJRKVUqlFhUVWSqfEEK4PIudc621Xqa1TtFap4SHh1vqZoUQwuWZU9R5QMsXFYsyfUwIIYQNmFPU24FkpVS8UsoLuB74yrqxhBBCNGv3TBKtdb1S6k5gFeAOvKG13m/1ZEIIIQArnfCilCoCcjv56WGAPb4ejuTqGMnVMZKrY5wxV6zWutUn+KxS1OdDKZXa1tk5RpJcHSO5OkZydYyr5ZKV9oUQws5JUQshhJ2zx6JeZnSANkiujpFcHSO5OsalctndPmohhBC/ZI8TtRBCiBakqIUQws4ZUtRKqWuUUvuVUo1KqZRzLnvEtO71QaXUtDY+P14ptdV0vY9NZ0xaOuPHSqndprcjSqndbVzviFIqzXQ9q79aglLqMaVUXotsM9q4nk3XEFdK/UMplaGU2quU+lwpFdzG9Wyyvdr7+pVS3qbvcZbpsRRnrSwt7jNaKbVeKXXA9Pi/p5XrTFBKlbb4/j5q7Vym+/3V74tqssS0vfYqpYbZIFPvFttht1KqTCl17znXscn2Ukq9oZQqVErta/GxUKXUaqVUpunfkDY+92bTdTKVUjd3KoDW2uZvQF+gN/ADkNLi4/2APYA3EA8cBtxb+fxPgOtN778CLLJy3meBR9u47AgQZsNt9xjwQDvXcTdtuwTAy7RN+1k511TAw/T+08DTRm0vc75+4HbgFdP71wMf2+B71wMYZno/ADjUSq4JwDe2ejyZ+30BZgDfAQoYDWy1cT53IJ+mk0Jsvr2AC4FhwL4WH3sG+IPp/T+09pgHQoFs078hpvdDOnr/hkzUWut0rfXBVi66AvhIa12rtc4BsmhaD/s/VNNrtE8C/m360NvALGtlNd3ftcCH1roPK7D5GuJa6++11vWm/26hafEuo5jz9V9B02MHmh5Lk03fa6vRWp/UWu80vV8OpNPKksF26grgHd1kCxCslOphw/ufDBzWWnf2jOfzorXeCJSc8+GWj6G2emgasFprXaK1Pg2sBqZ39P7tbR+1OWtfdwXOtCiFVtfHtqDxQIHWOrONyzXwvVJqh1JqoRVztHSn6c/PN9r4c8usNcSt6Faapq/W2GJ7mfP1/+c6psdSKU2PLZsw7WoZCmxt5eILlFJ7lFLfKaX62yhSe98Xox9T19P2sGTE9gLoprU+aXo/H+jWynUsst2s9vLeSqk1QPdWLvqj1vpLa91vR5iZ8QZ+fZoep7XOU0pFAKuVUhmm375WyQW8DDxB0w/WEzTtlrn1fO7PErmat5dS6o9APfB+Gzdj8e3laJRS/sCnwL1a67JzLt5J05/3FabnH74Akm0Qy26/L6bnoC4HHmnlYqO21y9orbVSymrHOlutqLXWUzrxaeasfV1M059dHqZJqNPrY7eXUSnlAfwGGP4rt5Fn+rdQKfU5TX92n9cD3Nxtp5R6DfimlYussoa4GdvrFuBSYLI27aBr5TYsvr1aYc7X33yd46bvcxBNjy2rUkp50lTS72utPzv38pbFrbX+Vin1klIqTGtt1QWIzPi+GLku/SXATq11wbkXGLW9TAqUUj201idNu4EKW7lOHk370ZtF0fTcXIfY266Pr4DrTc/Ix9P0m3FbyyuYCmA9cLXpQzcD1prQpwAZWuvjrV2olPJTSgU0v0/TE2r7WruupZyzX/DKNu7P5muIK6WmAw8Bl2utq9q4jq22lzlf/1c0PXag6bG0rq1fLpZi2ge+HEjXWj/XxnW6N+8rV0qNpOln1Kq/QMz8vnwFzDEd/TEaKG3xZ7+1tflXrRHbq4WWj6G2emgVMFUpFWLaTTnV9LGOsfazpW08g3olTftqaoECYFWLy/5I0zP2B4FLWnz8WyDS9H4CTQWeBfwL8LZSzreA2875WCTwbYsce0xv+2naBWDtbfcukAbsNT1Qepyby/T/GTQdVXDYRrmyaNoXt9v09sq5uWy5vVr7+oH/oekXCYCP6bGTZXosJdhgG42jaZfV3hbbaQZwW/PjDLjTtG320PSk7Bgb5Gr1+3JOLgUsNW3PNFocrWXlbH40FW9Qi4/ZfHvR9IviJHDW1F3zaHpOYy2QCawBQk3XTQFeb/G5t5oeZ1nA3M7cv5xCLoQQds7edn0IIYQ4hxS1EELYOSlqIYSwc1LUQghh56SohRDCzklRCyGEnZOiFkIIO/f/AWLLSS09sLseAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_fn2 = Potential_Function()\n",
        "p_fn2.plot_function(DISCRETE_POINTS)"
      ],
      "metadata": {
        "id": "a_beaqAjdi8w",
        "outputId": "991de568-a73d-48ba-bcd1-b8d407662c7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_fn2.update_potential_fn(DISCRETE_POINTS)\n",
        "p_fn2.plot_function(DISCRETE_POINTS)"
      ],
      "metadata": {
        "id": "HhHflb5adobv",
        "outputId": "72c70e93-99ca-436f-a846-ed980a8e0ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original min val = -4.394594810319068\n",
            "Min val with c_0 updated = 0.6054051896809316\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_train_with_penalty(model_1_1_sin, \n",
        "                         NUM_EPOCHS,\n",
        "                         p_fn1,\n",
        "                         adam_opt_11_sin,\n",
        "                         LAMBDA_PEN,\n",
        "                         DISCRETE_POINTS)"
      ],
      "metadata": {
        "id": "6OoKNxtrcTIv",
        "outputId": "8b289643-1cfd-4864-c6e8-a5af88592b91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss with penalty 12746.349609375\n",
            "epoch 1, loss with penalty 4580.22509765625\n",
            "epoch 2, loss with penalty 1351.51171875\n",
            "epoch 3, loss with penalty 285.04583740234375\n",
            "epoch 4, loss with penalty 22.618019104003906\n",
            "epoch 5, loss with penalty 13.852798461914062\n",
            "epoch 6, loss with penalty 56.586063385009766\n",
            "epoch 7, loss with penalty 80.75733947753906\n",
            "epoch 8, loss with penalty 70.0332260131836\n",
            "epoch 9, loss with penalty 36.110408782958984\n",
            "epoch 10, loss with penalty 6.257083892822266\n",
            "epoch 11, loss with penalty 8.442904472351074\n",
            "epoch 12, loss with penalty 48.077674865722656\n",
            "epoch 13, loss with penalty 94.04878997802734\n",
            "epoch 14, loss with penalty 108.0234146118164\n",
            "epoch 15, loss with penalty 82.69295501708984\n",
            "epoch 16, loss with penalty 40.4708251953125\n",
            "epoch 17, loss with penalty 9.125713348388672\n",
            "New min loss = tensor([1.9426], grad_fn=<AddBackward0>)\n",
            "epoch 18, loss with penalty 1.942589282989502\n",
            "epoch 19, loss with penalty 14.156079292297363\n",
            "epoch 20, loss with penalty 32.04784393310547\n",
            "epoch 21, loss with penalty 43.630531311035156\n",
            "epoch 22, loss with penalty 43.58428955078125\n",
            "epoch 23, loss with penalty 33.09456253051758\n",
            "epoch 24, loss with penalty 17.817155838012695\n",
            "epoch 25, loss with penalty 5.26151704788208\n",
            "New min loss = tensor([1.4046], grad_fn=<AddBackward0>)\n",
            "epoch 26, loss with penalty 1.4046229124069214\n",
            "epoch 27, loss with penalty 7.076959133148193\n",
            "epoch 28, loss with penalty 16.738920211791992\n",
            "epoch 29, loss with penalty 22.287996292114258\n",
            "epoch 30, loss with penalty 19.623149871826172\n",
            "epoch 31, loss with penalty 11.452452659606934\n",
            "epoch 32, loss with penalty 3.915755271911621\n",
            "New min loss = tensor([1.3016], grad_fn=<AddBackward0>)\n",
            "epoch 33, loss with penalty 1.3016185760498047\n",
            "epoch 34, loss with penalty 3.6490678787231445\n",
            "epoch 35, loss with penalty 7.9757304191589355\n",
            "epoch 36, loss with penalty 10.929679870605469\n",
            "epoch 37, loss with penalty 10.739524841308594\n",
            "epoch 38, loss with penalty 7.773253917694092\n",
            "epoch 39, loss with penalty 3.962808609008789\n",
            "epoch 40, loss with penalty 1.552590012550354\n",
            "epoch 41, loss with penalty 1.7102478742599487\n",
            "epoch 42, loss with penalty 3.7684621810913086\n",
            "epoch 43, loss with penalty 5.767210960388184\n",
            "epoch 44, loss with penalty 6.040953159332275\n",
            "epoch 45, loss with penalty 4.498719692230225\n",
            "epoch 46, loss with penalty 2.4555277824401855\n",
            "epoch 47, loss with penalty 1.3510805368423462\n",
            "epoch 48, loss with penalty 1.651607632637024\n",
            "epoch 49, loss with penalty 2.744703769683838\n",
            "epoch 50, loss with penalty 3.6151185035705566\n",
            "epoch 51, loss with penalty 3.612147331237793\n",
            "epoch 52, loss with penalty 2.804600477218628\n",
            "epoch 53, loss with penalty 1.8141555786132812\n",
            "epoch 54, loss with penalty 1.3132224082946777\n",
            "epoch 55, loss with penalty 1.5341792106628418\n",
            "epoch 56, loss with penalty 2.1343696117401123\n",
            "epoch 57, loss with penalty 2.524000406265259\n",
            "epoch 58, loss with penalty 2.3745203018188477\n",
            "epoch 59, loss with penalty 1.8519783020019531\n",
            "epoch 60, loss with penalty 1.39803147315979\n",
            "epoch 61, loss with penalty 1.3192334175109863\n",
            "epoch 62, loss with penalty 1.5698192119598389\n",
            "epoch 63, loss with penalty 1.8594162464141846\n",
            "epoch 64, loss with penalty 1.9255812168121338\n",
            "epoch 65, loss with penalty 1.7292007207870483\n",
            "epoch 66, loss with penalty 1.4464049339294434\n",
            "New min loss = tensor([1.2971], grad_fn=<AddBackward0>)\n",
            "epoch 67, loss with penalty 1.2970701456069946\n",
            "epoch 68, loss with penalty 1.3613111972808838\n",
            "epoch 69, loss with penalty 1.5309118032455444\n",
            "epoch 70, loss with penalty 1.6263179779052734\n",
            "epoch 71, loss with penalty 1.562140703201294\n",
            "epoch 72, loss with penalty 1.4071828126907349\n",
            "epoch 73, loss with penalty 1.2979333400726318\n",
            "epoch 74, loss with penalty 1.3074679374694824\n",
            "epoch 75, loss with penalty 1.3941996097564697\n",
            "epoch 76, loss with penalty 1.4590308666229248\n",
            "epoch 77, loss with penalty 1.4405336380004883\n",
            "epoch 78, loss with penalty 1.3606951236724854\n",
            "New min loss = tensor([1.2920], grad_fn=<AddBackward0>)\n",
            "epoch 79, loss with penalty 1.29201078414917\n",
            "New min loss = tensor([1.2860], grad_fn=<AddBackward0>)\n",
            "epoch 80, loss with penalty 1.286045789718628\n",
            "epoch 81, loss with penalty 1.3304603099822998\n",
            "epoch 82, loss with penalty 1.3697047233581543\n",
            "epoch 83, loss with penalty 1.362686276435852\n",
            "epoch 84, loss with penalty 1.318510890007019\n",
            "New min loss = tensor([1.2795], grad_fn=<AddBackward0>)\n",
            "epoch 85, loss with penalty 1.2794909477233887\n",
            "New min loss = tensor([1.2760], grad_fn=<AddBackward0>)\n",
            "epoch 86, loss with penalty 1.2759501934051514\n",
            "epoch 87, loss with penalty 1.3001450300216675\n",
            "epoch 88, loss with penalty 1.3203201293945312\n",
            "epoch 89, loss with penalty 1.314470648765564\n",
            "epoch 90, loss with penalty 1.288999080657959\n",
            "New min loss = tensor([1.2679], grad_fn=<AddBackward0>)\n",
            "epoch 91, loss with penalty 1.267945647239685\n",
            "epoch 92, loss with penalty 1.2672995328903198\n",
            "epoch 93, loss with penalty 1.2809721231460571\n",
            "epoch 94, loss with penalty 1.290209412574768\n",
            "epoch 95, loss with penalty 1.283928632736206\n",
            "epoch 96, loss with penalty 1.2683601379394531\n",
            "New min loss = tensor([1.2579], grad_fn=<AddBackward0>)\n",
            "epoch 97, loss with penalty 1.257887363433838\n",
            "epoch 98, loss with penalty 1.2594016790390015\n",
            "epoch 99, loss with penalty 1.2668107748031616\n",
            "epoch 100, loss with penalty 1.2693865299224854\n",
            "epoch 101, loss with penalty 1.2631993293762207\n",
            "New min loss = tensor([1.2539], grad_fn=<AddBackward0>)\n",
            "epoch 102, loss with penalty 1.2538607120513916\n",
            "New min loss = tensor([1.2493], grad_fn=<AddBackward0>)\n",
            "epoch 103, loss with penalty 1.2492557764053345\n",
            "epoch 104, loss with penalty 1.25118088722229\n",
            "epoch 105, loss with penalty 1.2545095682144165\n",
            "epoch 106, loss with penalty 1.253617763519287\n",
            "epoch 107, loss with penalty 1.2482658624649048\n",
            "New min loss = tensor([1.2429], grad_fn=<AddBackward0>)\n",
            "epoch 108, loss with penalty 1.2429275512695312\n",
            "New min loss = tensor([1.2413], grad_fn=<AddBackward0>)\n",
            "epoch 109, loss with penalty 1.2412984371185303\n",
            "epoch 110, loss with penalty 1.2425750494003296\n",
            "epoch 111, loss with penalty 1.2430835962295532\n",
            "epoch 112, loss with penalty 1.2406504154205322\n",
            "New min loss = tensor([1.2366], grad_fn=<AddBackward0>)\n",
            "epoch 113, loss with penalty 1.236628770828247\n",
            "New min loss = tensor([1.2338], grad_fn=<AddBackward0>)\n",
            "epoch 114, loss with penalty 1.2338340282440186\n",
            "epoch 115, loss with penalty 1.2333149909973145\n",
            "epoch 116, loss with penalty 1.2335089445114136\n",
            "New min loss = tensor([1.2324], grad_fn=<AddBackward0>)\n",
            "epoch 117, loss with penalty 1.232376217842102\n",
            "New min loss = tensor([1.2297], grad_fn=<AddBackward0>)\n",
            "epoch 118, loss with penalty 1.229689359664917\n",
            "New min loss = tensor([1.2270], grad_fn=<AddBackward0>)\n",
            "epoch 119, loss with penalty 1.2269636392593384\n",
            "New min loss = tensor([1.2255], grad_fn=<AddBackward0>)\n",
            "epoch 120, loss with penalty 1.225498914718628\n",
            "epoch 121, loss with penalty 1.224989891052246\n",
            "New min loss = tensor([1.2241], grad_fn=<AddBackward0>)\n",
            "epoch 122, loss with penalty 1.2241430282592773\n",
            "New min loss = tensor([1.2223], grad_fn=<AddBackward0>)\n",
            "epoch 123, loss with penalty 1.2222851514816284\n",
            "New min loss = tensor([1.2200], grad_fn=<AddBackward0>)\n",
            "epoch 124, loss with penalty 1.2199956178665161\n",
            "New min loss = tensor([1.2182], grad_fn=<AddBackward0>)\n",
            "epoch 125, loss with penalty 1.2182401418685913\n",
            "New min loss = tensor([1.2172], grad_fn=<AddBackward0>)\n",
            "epoch 126, loss with penalty 1.2172260284423828\n",
            "epoch 127, loss with penalty 1.2163000106811523\n",
            "New min loss = tensor([1.2148], grad_fn=<AddBackward0>)\n",
            "epoch 128, loss with penalty 1.2148247957229614\n",
            "New min loss = tensor([1.2129], grad_fn=<AddBackward0>)\n",
            "epoch 129, loss with penalty 1.2129087448120117\n",
            "New min loss = tensor([1.2111], grad_fn=<AddBackward0>)\n",
            "epoch 130, loss with penalty 1.2111424207687378\n",
            "New min loss = tensor([1.2098], grad_fn=<AddBackward0>)\n",
            "epoch 131, loss with penalty 1.209843635559082\n",
            "New min loss = tensor([1.2087], grad_fn=<AddBackward0>)\n",
            "epoch 132, loss with penalty 1.2087458372116089\n",
            "New min loss = tensor([1.2074], grad_fn=<AddBackward0>)\n",
            "epoch 133, loss with penalty 1.2073945999145508\n",
            "New min loss = tensor([1.2057], grad_fn=<AddBackward0>)\n",
            "epoch 134, loss with penalty 1.2057253122329712\n",
            "New min loss = tensor([1.2041], grad_fn=<AddBackward0>)\n",
            "epoch 135, loss with penalty 1.2040530443191528\n",
            "New min loss = tensor([1.2026], grad_fn=<AddBackward0>)\n",
            "epoch 136, loss with penalty 1.2026461362838745\n",
            "New min loss = tensor([1.2014], grad_fn=<AddBackward0>)\n",
            "epoch 137, loss with penalty 1.201418399810791\n",
            "New min loss = tensor([1.2001], grad_fn=<AddBackward0>)\n",
            "epoch 138, loss with penalty 1.2000925540924072\n",
            "New min loss = tensor([1.1986], grad_fn=<AddBackward0>)\n",
            "epoch 139, loss with penalty 1.1985559463500977\n",
            "New min loss = tensor([1.1970], grad_fn=<AddBackward0>)\n",
            "epoch 140, loss with penalty 1.1969701051712036\n",
            "New min loss = tensor([1.1955], grad_fn=<AddBackward0>)\n",
            "epoch 141, loss with penalty 1.1955262422561646\n",
            "New min loss = tensor([1.1942], grad_fn=<AddBackward0>)\n",
            "epoch 142, loss with penalty 1.1942124366760254\n",
            "New min loss = tensor([1.1929], grad_fn=<AddBackward0>)\n",
            "epoch 143, loss with penalty 1.1928702592849731\n",
            "New min loss = tensor([1.1914], grad_fn=<AddBackward0>)\n",
            "epoch 144, loss with penalty 1.1914026737213135\n",
            "New min loss = tensor([1.1899], grad_fn=<AddBackward0>)\n",
            "epoch 145, loss with penalty 1.1898881196975708\n",
            "New min loss = tensor([1.1884], grad_fn=<AddBackward0>)\n",
            "epoch 146, loss with penalty 1.18844735622406\n",
            "New min loss = tensor([1.1871], grad_fn=<AddBackward0>)\n",
            "epoch 147, loss with penalty 1.187096357345581\n",
            "New min loss = tensor([1.1857], grad_fn=<AddBackward0>)\n",
            "epoch 148, loss with penalty 1.1857383251190186\n",
            "New min loss = tensor([1.1843], grad_fn=<AddBackward0>)\n",
            "epoch 149, loss with penalty 1.184306263923645\n",
            "New min loss = tensor([1.1828], grad_fn=<AddBackward0>)\n",
            "epoch 150, loss with penalty 1.1828360557556152\n",
            "New min loss = tensor([1.1814], grad_fn=<AddBackward0>)\n",
            "epoch 151, loss with penalty 1.1814069747924805\n",
            "New min loss = tensor([1.1800], grad_fn=<AddBackward0>)\n",
            "epoch 152, loss with penalty 1.1800341606140137\n",
            "New min loss = tensor([1.1787], grad_fn=<AddBackward0>)\n",
            "epoch 153, loss with penalty 1.178663969039917\n",
            "New min loss = tensor([1.1772], grad_fn=<AddBackward0>)\n",
            "epoch 154, loss with penalty 1.1772493124008179\n",
            "New min loss = tensor([1.1758], grad_fn=<AddBackward0>)\n",
            "epoch 155, loss with penalty 1.1758127212524414\n",
            "New min loss = tensor([1.1744], grad_fn=<AddBackward0>)\n",
            "epoch 156, loss with penalty 1.1744033098220825\n",
            "New min loss = tensor([1.1730], grad_fn=<AddBackward0>)\n",
            "epoch 157, loss with penalty 1.1730254888534546\n",
            "New min loss = tensor([1.1717], grad_fn=<AddBackward0>)\n",
            "epoch 158, loss with penalty 1.171651840209961\n",
            "New min loss = tensor([1.1703], grad_fn=<AddBackward0>)\n",
            "epoch 159, loss with penalty 1.1702520847320557\n",
            "New min loss = tensor([1.1688], grad_fn=<AddBackward0>)\n",
            "epoch 160, loss with penalty 1.1688381433486938\n",
            "New min loss = tensor([1.1674], grad_fn=<AddBackward0>)\n",
            "epoch 161, loss with penalty 1.1674398183822632\n",
            "New min loss = tensor([1.1661], grad_fn=<AddBackward0>)\n",
            "epoch 162, loss with penalty 1.166064977645874\n",
            "New min loss = tensor([1.1647], grad_fn=<AddBackward0>)\n",
            "epoch 163, loss with penalty 1.1646901369094849\n",
            "New min loss = tensor([1.1633], grad_fn=<AddBackward0>)\n",
            "epoch 164, loss with penalty 1.1633021831512451\n",
            "New min loss = tensor([1.1619], grad_fn=<AddBackward0>)\n",
            "epoch 165, loss with penalty 1.16190767288208\n",
            "New min loss = tensor([1.1605], grad_fn=<AddBackward0>)\n",
            "epoch 166, loss with penalty 1.1605249643325806\n",
            "New min loss = tensor([1.1592], grad_fn=<AddBackward0>)\n",
            "epoch 167, loss with penalty 1.1591567993164062\n",
            "New min loss = tensor([1.1578], grad_fn=<AddBackward0>)\n",
            "epoch 168, loss with penalty 1.157788634300232\n",
            "New min loss = tensor([1.1564], grad_fn=<AddBackward0>)\n",
            "epoch 169, loss with penalty 1.1564114093780518\n",
            "New min loss = tensor([1.1550], grad_fn=<AddBackward0>)\n",
            "epoch 170, loss with penalty 1.155033826828003\n",
            "New min loss = tensor([1.1537], grad_fn=<AddBackward0>)\n",
            "epoch 171, loss with penalty 1.1536632776260376\n",
            "New min loss = tensor([1.1523], grad_fn=<AddBackward0>)\n",
            "epoch 172, loss with penalty 1.1523019075393677\n",
            "New min loss = tensor([1.1509], grad_fn=<AddBackward0>)\n",
            "epoch 173, loss with penalty 1.1509416103363037\n",
            "New min loss = tensor([1.1496], grad_fn=<AddBackward0>)\n",
            "epoch 174, loss with penalty 1.1495767831802368\n",
            "New min loss = tensor([1.1482], grad_fn=<AddBackward0>)\n",
            "epoch 175, loss with penalty 1.1482138633728027\n",
            "New min loss = tensor([1.1469], grad_fn=<AddBackward0>)\n",
            "epoch 176, loss with penalty 1.1468578577041626\n",
            "New min loss = tensor([1.1455], grad_fn=<AddBackward0>)\n",
            "epoch 177, loss with penalty 1.1455069780349731\n",
            "New min loss = tensor([1.1442], grad_fn=<AddBackward0>)\n",
            "epoch 178, loss with penalty 1.1441577672958374\n",
            "New min loss = tensor([1.1428], grad_fn=<AddBackward0>)\n",
            "epoch 179, loss with penalty 1.142806053161621\n",
            "New min loss = tensor([1.1415], grad_fn=<AddBackward0>)\n",
            "epoch 180, loss with penalty 1.1414573192596436\n",
            "New min loss = tensor([1.1401], grad_fn=<AddBackward0>)\n",
            "epoch 181, loss with penalty 1.1401129961013794\n",
            "New min loss = tensor([1.1388], grad_fn=<AddBackward0>)\n",
            "epoch 182, loss with penalty 1.138774037361145\n",
            "New min loss = tensor([1.1374], grad_fn=<AddBackward0>)\n",
            "epoch 183, loss with penalty 1.137436032295227\n",
            "New min loss = tensor([1.1361], grad_fn=<AddBackward0>)\n",
            "epoch 184, loss with penalty 1.1360979080200195\n",
            "New min loss = tensor([1.1348], grad_fn=<AddBackward0>)\n",
            "epoch 185, loss with penalty 1.1347646713256836\n",
            "New min loss = tensor([1.1334], grad_fn=<AddBackward0>)\n",
            "epoch 186, loss with penalty 1.1334346532821655\n",
            "New min loss = tensor([1.1321], grad_fn=<AddBackward0>)\n",
            "epoch 187, loss with penalty 1.1321086883544922\n",
            "New min loss = tensor([1.1308], grad_fn=<AddBackward0>)\n",
            "epoch 188, loss with penalty 1.1307835578918457\n",
            "New min loss = tensor([1.1295], grad_fn=<AddBackward0>)\n",
            "epoch 189, loss with penalty 1.129460334777832\n",
            "New min loss = tensor([1.1281], grad_fn=<AddBackward0>)\n",
            "epoch 190, loss with penalty 1.1281405687332153\n",
            "New min loss = tensor([1.1268], grad_fn=<AddBackward0>)\n",
            "epoch 191, loss with penalty 1.1268253326416016\n",
            "New min loss = tensor([1.1255], grad_fn=<AddBackward0>)\n",
            "epoch 192, loss with penalty 1.1255125999450684\n",
            "New min loss = tensor([1.1242], grad_fn=<AddBackward0>)\n",
            "epoch 193, loss with penalty 1.1242002248764038\n",
            "New min loss = tensor([1.1229], grad_fn=<AddBackward0>)\n",
            "epoch 194, loss with penalty 1.1228922605514526\n",
            "New min loss = tensor([1.1216], grad_fn=<AddBackward0>)\n",
            "epoch 195, loss with penalty 1.1215882301330566\n",
            "New min loss = tensor([1.1203], grad_fn=<AddBackward0>)\n",
            "epoch 196, loss with penalty 1.1202869415283203\n",
            "New min loss = tensor([1.1190], grad_fn=<AddBackward0>)\n",
            "epoch 197, loss with penalty 1.1189879179000854\n",
            "New min loss = tensor([1.1177], grad_fn=<AddBackward0>)\n",
            "epoch 198, loss with penalty 1.1176915168762207\n",
            "New min loss = tensor([1.1164], grad_fn=<AddBackward0>)\n",
            "epoch 199, loss with penalty 1.1163990497589111\n",
            "New min loss = tensor([1.1151], grad_fn=<AddBackward0>)\n",
            "epoch 200, loss with penalty 1.1151098012924194\n",
            "New min loss = tensor([1.1138], grad_fn=<AddBackward0>)\n",
            "epoch 201, loss with penalty 1.1138226985931396\n",
            "New min loss = tensor([1.1125], grad_fn=<AddBackward0>)\n",
            "epoch 202, loss with penalty 1.112539291381836\n",
            "New min loss = tensor([1.1113], grad_fn=<AddBackward0>)\n",
            "epoch 203, loss with penalty 1.1112587451934814\n",
            "New min loss = tensor([1.1100], grad_fn=<AddBackward0>)\n",
            "epoch 204, loss with penalty 1.1099815368652344\n",
            "New min loss = tensor([1.1087], grad_fn=<AddBackward0>)\n",
            "epoch 205, loss with penalty 1.1087079048156738\n",
            "New min loss = tensor([1.1074], grad_fn=<AddBackward0>)\n",
            "epoch 206, loss with penalty 1.107437014579773\n",
            "New min loss = tensor([1.1062], grad_fn=<AddBackward0>)\n",
            "epoch 207, loss with penalty 1.1061689853668213\n",
            "New min loss = tensor([1.1049], grad_fn=<AddBackward0>)\n",
            "epoch 208, loss with penalty 1.1049038171768188\n",
            "New min loss = tensor([1.1036], grad_fn=<AddBackward0>)\n",
            "epoch 209, loss with penalty 1.1036434173583984\n",
            "New min loss = tensor([1.1024], grad_fn=<AddBackward0>)\n",
            "epoch 210, loss with penalty 1.1023850440979004\n",
            "New min loss = tensor([1.1011], grad_fn=<AddBackward0>)\n",
            "epoch 211, loss with penalty 1.101130723953247\n",
            "New min loss = tensor([1.0999], grad_fn=<AddBackward0>)\n",
            "epoch 212, loss with penalty 1.0998786687850952\n",
            "New min loss = tensor([1.0986], grad_fn=<AddBackward0>)\n",
            "epoch 213, loss with penalty 1.098630666732788\n",
            "New min loss = tensor([1.0974], grad_fn=<AddBackward0>)\n",
            "epoch 214, loss with penalty 1.0973848104476929\n",
            "New min loss = tensor([1.0961], grad_fn=<AddBackward0>)\n",
            "epoch 215, loss with penalty 1.096143364906311\n",
            "New min loss = tensor([1.0949], grad_fn=<AddBackward0>)\n",
            "epoch 216, loss with penalty 1.0949046611785889\n",
            "New min loss = tensor([1.0937], grad_fn=<AddBackward0>)\n",
            "epoch 217, loss with penalty 1.0936704874038696\n",
            "New min loss = tensor([1.0924], grad_fn=<AddBackward0>)\n",
            "epoch 218, loss with penalty 1.0924381017684937\n",
            "New min loss = tensor([1.0912], grad_fn=<AddBackward0>)\n",
            "epoch 219, loss with penalty 1.0912086963653564\n",
            "New min loss = tensor([1.0900], grad_fn=<AddBackward0>)\n",
            "epoch 220, loss with penalty 1.0899841785430908\n",
            "New min loss = tensor([1.0888], grad_fn=<AddBackward0>)\n",
            "epoch 221, loss with penalty 1.0887640714645386\n",
            "New min loss = tensor([1.0875], grad_fn=<AddBackward0>)\n",
            "epoch 222, loss with penalty 1.087544560432434\n",
            "New min loss = tensor([1.0863], grad_fn=<AddBackward0>)\n",
            "epoch 223, loss with penalty 1.086330533027649\n",
            "New min loss = tensor([1.0851], grad_fn=<AddBackward0>)\n",
            "epoch 224, loss with penalty 1.0851188898086548\n",
            "New min loss = tensor([1.0839], grad_fn=<AddBackward0>)\n",
            "epoch 225, loss with penalty 1.0839104652404785\n",
            "New min loss = tensor([1.0827], grad_fn=<AddBackward0>)\n",
            "epoch 226, loss with penalty 1.0827057361602783\n",
            "New min loss = tensor([1.0815], grad_fn=<AddBackward0>)\n",
            "epoch 227, loss with penalty 1.0815045833587646\n",
            "New min loss = tensor([1.0803], grad_fn=<AddBackward0>)\n",
            "epoch 228, loss with penalty 1.0803073644638062\n",
            "New min loss = tensor([1.0791], grad_fn=<AddBackward0>)\n",
            "epoch 229, loss with penalty 1.079111933708191\n",
            "New min loss = tensor([1.0779], grad_fn=<AddBackward0>)\n",
            "epoch 230, loss with penalty 1.0779213905334473\n",
            "New min loss = tensor([1.0767], grad_fn=<AddBackward0>)\n",
            "epoch 231, loss with penalty 1.0767337083816528\n",
            "New min loss = tensor([1.0756], grad_fn=<AddBackward0>)\n",
            "epoch 232, loss with penalty 1.0755503177642822\n",
            "New min loss = tensor([1.0744], grad_fn=<AddBackward0>)\n",
            "epoch 233, loss with penalty 1.074369192123413\n",
            "New min loss = tensor([1.0732], grad_fn=<AddBackward0>)\n",
            "epoch 234, loss with penalty 1.07319176197052\n",
            "New min loss = tensor([1.0720], grad_fn=<AddBackward0>)\n",
            "epoch 235, loss with penalty 1.0720181465148926\n",
            "New min loss = tensor([1.0708], grad_fn=<AddBackward0>)\n",
            "epoch 236, loss with penalty 1.0708476305007935\n",
            "New min loss = tensor([1.0697], grad_fn=<AddBackward0>)\n",
            "epoch 237, loss with penalty 1.0696812868118286\n",
            "New min loss = tensor([1.0685], grad_fn=<AddBackward0>)\n",
            "epoch 238, loss with penalty 1.0685185194015503\n",
            "New min loss = tensor([1.0674], grad_fn=<AddBackward0>)\n",
            "epoch 239, loss with penalty 1.0673589706420898\n",
            "New min loss = tensor([1.0662], grad_fn=<AddBackward0>)\n",
            "epoch 240, loss with penalty 1.0662027597427368\n",
            "New min loss = tensor([1.0651], grad_fn=<AddBackward0>)\n",
            "epoch 241, loss with penalty 1.0650502443313599\n",
            "New min loss = tensor([1.0639], grad_fn=<AddBackward0>)\n",
            "epoch 242, loss with penalty 1.0639021396636963\n",
            "New min loss = tensor([1.0628], grad_fn=<AddBackward0>)\n",
            "epoch 243, loss with penalty 1.0627552270889282\n",
            "New min loss = tensor([1.0616], grad_fn=<AddBackward0>)\n",
            "epoch 244, loss with penalty 1.0616132020950317\n",
            "New min loss = tensor([1.0605], grad_fn=<AddBackward0>)\n",
            "epoch 245, loss with penalty 1.0604751110076904\n",
            "New min loss = tensor([1.0593], grad_fn=<AddBackward0>)\n",
            "epoch 246, loss with penalty 1.059340476989746\n",
            "New min loss = tensor([1.0582], grad_fn=<AddBackward0>)\n",
            "epoch 247, loss with penalty 1.0582082271575928\n",
            "New min loss = tensor([1.0571], grad_fn=<AddBackward0>)\n",
            "epoch 248, loss with penalty 1.0570796728134155\n",
            "New min loss = tensor([1.0560], grad_fn=<AddBackward0>)\n",
            "epoch 249, loss with penalty 1.0559552907943726\n",
            "New min loss = tensor([1.0548], grad_fn=<AddBackward0>)\n",
            "epoch 250, loss with penalty 1.0548348426818848\n",
            "New min loss = tensor([1.0537], grad_fn=<AddBackward0>)\n",
            "epoch 251, loss with penalty 1.053717851638794\n",
            "New min loss = tensor([1.0526], grad_fn=<AddBackward0>)\n",
            "epoch 252, loss with penalty 1.052604079246521\n",
            "New min loss = tensor([1.0515], grad_fn=<AddBackward0>)\n",
            "epoch 253, loss with penalty 1.051493525505066\n",
            "New min loss = tensor([1.0504], grad_fn=<AddBackward0>)\n",
            "epoch 254, loss with penalty 1.0503860712051392\n",
            "New min loss = tensor([1.0493], grad_fn=<AddBackward0>)\n",
            "epoch 255, loss with penalty 1.0492833852767944\n",
            "New min loss = tensor([1.0482], grad_fn=<AddBackward0>)\n",
            "epoch 256, loss with penalty 1.0481821298599243\n",
            "New min loss = tensor([1.0471], grad_fn=<AddBackward0>)\n",
            "epoch 257, loss with penalty 1.0470874309539795\n",
            "New min loss = tensor([1.0460], grad_fn=<AddBackward0>)\n",
            "epoch 258, loss with penalty 1.0459948778152466\n",
            "New min loss = tensor([1.0449], grad_fn=<AddBackward0>)\n",
            "epoch 259, loss with penalty 1.0449061393737793\n",
            "New min loss = tensor([1.0438], grad_fn=<AddBackward0>)\n",
            "epoch 260, loss with penalty 1.0438185930252075\n",
            "New min loss = tensor([1.0427], grad_fn=<AddBackward0>)\n",
            "epoch 261, loss with penalty 1.0427378416061401\n",
            "New min loss = tensor([1.0417], grad_fn=<AddBackward0>)\n",
            "epoch 262, loss with penalty 1.0416591167449951\n",
            "New min loss = tensor([1.0406], grad_fn=<AddBackward0>)\n",
            "epoch 263, loss with penalty 1.0405828952789307\n",
            "New min loss = tensor([1.0395], grad_fn=<AddBackward0>)\n",
            "epoch 264, loss with penalty 1.03951096534729\n",
            "New min loss = tensor([1.0384], grad_fn=<AddBackward0>)\n",
            "epoch 265, loss with penalty 1.038443684577942\n",
            "New min loss = tensor([1.0374], grad_fn=<AddBackward0>)\n",
            "epoch 266, loss with penalty 1.0373787879943848\n",
            "New min loss = tensor([1.0363], grad_fn=<AddBackward0>)\n",
            "epoch 267, loss with penalty 1.0363173484802246\n",
            "New min loss = tensor([1.0353], grad_fn=<AddBackward0>)\n",
            "epoch 268, loss with penalty 1.0352596044540405\n",
            "New min loss = tensor([1.0342], grad_fn=<AddBackward0>)\n",
            "epoch 269, loss with penalty 1.0342061519622803\n",
            "New min loss = tensor([1.0332], grad_fn=<AddBackward0>)\n",
            "epoch 270, loss with penalty 1.0331542491912842\n",
            "New min loss = tensor([1.0321], grad_fn=<AddBackward0>)\n",
            "epoch 271, loss with penalty 1.0321080684661865\n",
            "New min loss = tensor([1.0311], grad_fn=<AddBackward0>)\n",
            "epoch 272, loss with penalty 1.0310630798339844\n",
            "New min loss = tensor([1.0300], grad_fn=<AddBackward0>)\n",
            "epoch 273, loss with penalty 1.0300235748291016\n",
            "New min loss = tensor([1.0290], grad_fn=<AddBackward0>)\n",
            "epoch 274, loss with penalty 1.0289868116378784\n",
            "New min loss = tensor([1.0280], grad_fn=<AddBackward0>)\n",
            "epoch 275, loss with penalty 1.0279539823532104\n",
            "New min loss = tensor([1.0269], grad_fn=<AddBackward0>)\n",
            "epoch 276, loss with penalty 1.0269231796264648\n",
            "New min loss = tensor([1.0259], grad_fn=<AddBackward0>)\n",
            "epoch 277, loss with penalty 1.0258969068527222\n",
            "New min loss = tensor([1.0249], grad_fn=<AddBackward0>)\n",
            "epoch 278, loss with penalty 1.0248738527297974\n",
            "New min loss = tensor([1.0239], grad_fn=<AddBackward0>)\n",
            "epoch 279, loss with penalty 1.0238549709320068\n",
            "New min loss = tensor([1.0228], grad_fn=<AddBackward0>)\n",
            "epoch 280, loss with penalty 1.0228383541107178\n",
            "New min loss = tensor([1.0218], grad_fn=<AddBackward0>)\n",
            "epoch 281, loss with penalty 1.021824836730957\n",
            "New min loss = tensor([1.0208], grad_fn=<AddBackward0>)\n",
            "epoch 282, loss with penalty 1.0208160877227783\n",
            "New min loss = tensor([1.0198], grad_fn=<AddBackward0>)\n",
            "epoch 283, loss with penalty 1.0198099613189697\n",
            "New min loss = tensor([1.0188], grad_fn=<AddBackward0>)\n",
            "epoch 284, loss with penalty 1.0188074111938477\n",
            "epoch 285, loss with penalty 1.0178085565567017\n",
            "New min loss = tensor([1.0168], grad_fn=<AddBackward0>)\n",
            "epoch 286, loss with penalty 1.0168124437332153\n",
            "epoch 287, loss with penalty 1.015820026397705\n",
            "New min loss = tensor([1.0148], grad_fn=<AddBackward0>)\n",
            "epoch 288, loss with penalty 1.0148309469223022\n",
            "epoch 289, loss with penalty 1.0138450860977173\n",
            "New min loss = tensor([1.0129], grad_fn=<AddBackward0>)\n",
            "epoch 290, loss with penalty 1.0128624439239502\n",
            "epoch 291, loss with penalty 1.0118839740753174\n",
            "New min loss = tensor([1.0109], grad_fn=<AddBackward0>)\n",
            "epoch 292, loss with penalty 1.010909080505371\n",
            "epoch 293, loss with penalty 1.0099365711212158\n",
            "New min loss = tensor([1.0090], grad_fn=<AddBackward0>)\n",
            "epoch 294, loss with penalty 1.008967638015747\n",
            "epoch 295, loss with penalty 1.0080018043518066\n",
            "New min loss = tensor([1.0070], grad_fn=<AddBackward0>)\n",
            "epoch 296, loss with penalty 1.007039189338684\n",
            "epoch 297, loss with penalty 1.0060808658599854\n",
            "New min loss = tensor([1.0051], grad_fn=<AddBackward0>)\n",
            "epoch 298, loss with penalty 1.0051238536834717\n",
            "epoch 299, loss with penalty 1.00417160987854\n",
            "New min loss = tensor([1.0032], grad_fn=<AddBackward0>)\n",
            "epoch 300, loss with penalty 1.0032232999801636\n",
            "epoch 301, loss with penalty 1.0022765398025513\n",
            "New min loss = tensor([1.0013], grad_fn=<AddBackward0>)\n",
            "epoch 302, loss with penalty 1.0013337135314941\n",
            "epoch 303, loss with penalty 1.0003951787948608\n",
            "New min loss = tensor([0.9995], grad_fn=<AddBackward0>)\n",
            "epoch 304, loss with penalty 0.9994591474533081\n",
            "epoch 305, loss with penalty 0.9985254406929016\n",
            "New min loss = tensor([0.9976], grad_fn=<AddBackward0>)\n",
            "epoch 306, loss with penalty 0.9975966215133667\n",
            "epoch 307, loss with penalty 0.996669590473175\n",
            "New min loss = tensor([0.9957], grad_fn=<AddBackward0>)\n",
            "epoch 308, loss with penalty 0.9957461953163147\n",
            "epoch 309, loss with penalty 0.9948266744613647\n",
            "New min loss = tensor([0.9939], grad_fn=<AddBackward0>)\n",
            "epoch 310, loss with penalty 0.9939091801643372\n",
            "epoch 311, loss with penalty 0.9929952025413513\n",
            "New min loss = tensor([0.9921], grad_fn=<AddBackward0>)\n",
            "epoch 312, loss with penalty 0.9920846223831177\n",
            "epoch 313, loss with penalty 0.9911777377128601\n",
            "New min loss = tensor([0.9903], grad_fn=<AddBackward0>)\n",
            "epoch 314, loss with penalty 0.990272581577301\n",
            "epoch 315, loss with penalty 0.9893717765808105\n",
            "New min loss = tensor([0.9885], grad_fn=<AddBackward0>)\n",
            "epoch 316, loss with penalty 0.9884731769561768\n",
            "epoch 317, loss with penalty 0.9875781536102295\n",
            "New min loss = tensor([0.9867], grad_fn=<AddBackward0>)\n",
            "epoch 318, loss with penalty 0.9866859316825867\n",
            "epoch 319, loss with penalty 0.9857977628707886\n",
            "New min loss = tensor([0.9849], grad_fn=<AddBackward0>)\n",
            "epoch 320, loss with penalty 0.9849120378494263\n",
            "epoch 321, loss with penalty 0.9840288758277893\n",
            "New min loss = tensor([0.9831], grad_fn=<AddBackward0>)\n",
            "epoch 322, loss with penalty 0.983148992061615\n",
            "epoch 323, loss with penalty 0.9822721481323242\n",
            "New min loss = tensor([0.9814], grad_fn=<AddBackward0>)\n",
            "epoch 324, loss with penalty 0.9813985228538513\n",
            "epoch 325, loss with penalty 0.9805276989936829\n",
            "New min loss = tensor([0.9797], grad_fn=<AddBackward0>)\n",
            "epoch 326, loss with penalty 0.9796595573425293\n",
            "epoch 327, loss with penalty 0.978794515132904\n",
            "New min loss = tensor([0.9779], grad_fn=<AddBackward0>)\n",
            "epoch 328, loss with penalty 0.977933406829834\n",
            "epoch 329, loss with penalty 0.9770745635032654\n",
            "New min loss = tensor([0.9762], grad_fn=<AddBackward0>)\n",
            "epoch 330, loss with penalty 0.9762188196182251\n",
            "epoch 331, loss with penalty 0.9753661155700684\n",
            "New min loss = tensor([0.9745], grad_fn=<AddBackward0>)\n",
            "epoch 332, loss with penalty 0.9745165109634399\n",
            "epoch 333, loss with penalty 0.9736689925193787\n",
            "New min loss = tensor([0.9728], grad_fn=<AddBackward0>)\n",
            "epoch 334, loss with penalty 0.9728252291679382\n",
            "epoch 335, loss with penalty 0.971983790397644\n",
            "New min loss = tensor([0.9711], grad_fn=<AddBackward0>)\n",
            "epoch 336, loss with penalty 0.9711454510688782\n",
            "epoch 337, loss with penalty 0.9703102111816406\n",
            "New min loss = tensor([0.9695], grad_fn=<AddBackward0>)\n",
            "epoch 338, loss with penalty 0.9694770574569702\n",
            "epoch 339, loss with penalty 0.9686477184295654\n",
            "New min loss = tensor([0.9678], grad_fn=<AddBackward0>)\n",
            "epoch 340, loss with penalty 0.9678201079368591\n",
            "epoch 341, loss with penalty 0.966995894908905\n",
            "New min loss = tensor([0.9662], grad_fn=<AddBackward0>)\n",
            "epoch 342, loss with penalty 0.9661744832992554\n",
            "epoch 343, loss with penalty 0.9653559327125549\n",
            "New min loss = tensor([0.9645], grad_fn=<AddBackward0>)\n",
            "epoch 344, loss with penalty 0.9645400643348694\n",
            "epoch 345, loss with penalty 0.9637269377708435\n",
            "New min loss = tensor([0.9629], grad_fn=<AddBackward0>)\n",
            "epoch 346, loss with penalty 0.962917685508728\n",
            "epoch 347, loss with penalty 0.9621090888977051\n",
            "New min loss = tensor([0.9613], grad_fn=<AddBackward0>)\n",
            "epoch 348, loss with penalty 0.9613048434257507\n",
            "epoch 349, loss with penalty 0.9605013132095337\n",
            "New min loss = tensor([0.9597], grad_fn=<AddBackward0>)\n",
            "epoch 350, loss with penalty 0.9597026109695435\n",
            "epoch 351, loss with penalty 0.9589057564735413\n",
            "New min loss = tensor([0.9581], grad_fn=<AddBackward0>)\n",
            "epoch 352, loss with penalty 0.9581120014190674\n",
            "epoch 353, loss with penalty 0.9573207497596741\n",
            "New min loss = tensor([0.9565], grad_fn=<AddBackward0>)\n",
            "epoch 354, loss with penalty 0.9565325975418091\n",
            "epoch 355, loss with penalty 0.9557464122772217\n",
            "New min loss = tensor([0.9550], grad_fn=<AddBackward0>)\n",
            "epoch 356, loss with penalty 0.9549632668495178\n",
            "epoch 357, loss with penalty 0.9541828036308289\n",
            "New min loss = tensor([0.9534], grad_fn=<AddBackward0>)\n",
            "epoch 358, loss with penalty 0.953403890132904\n",
            "epoch 359, loss with penalty 0.9526283740997314\n",
            "New min loss = tensor([0.9519], grad_fn=<AddBackward0>)\n",
            "epoch 360, loss with penalty 0.9518555402755737\n",
            "epoch 361, loss with penalty 0.9510849714279175\n",
            "New min loss = tensor([0.9503], grad_fn=<AddBackward0>)\n",
            "epoch 362, loss with penalty 0.9503167867660522\n",
            "epoch 363, loss with penalty 0.9495521187782288\n",
            "New min loss = tensor([0.9488], grad_fn=<AddBackward0>)\n",
            "epoch 364, loss with penalty 0.948789119720459\n",
            "epoch 365, loss with penalty 0.9480288624763489\n",
            "New min loss = tensor([0.9473], grad_fn=<AddBackward0>)\n",
            "epoch 366, loss with penalty 0.9472705721855164\n",
            "epoch 367, loss with penalty 0.946516215801239\n",
            "New min loss = tensor([0.9458], grad_fn=<AddBackward0>)\n",
            "epoch 368, loss with penalty 0.9457628130912781\n",
            "epoch 369, loss with penalty 0.945013165473938\n",
            "New min loss = tensor([0.9443], grad_fn=<AddBackward0>)\n",
            "epoch 370, loss with penalty 0.9442654848098755\n",
            "epoch 371, loss with penalty 0.9435195922851562\n",
            "New min loss = tensor([0.9428], grad_fn=<AddBackward0>)\n",
            "epoch 372, loss with penalty 0.9427772760391235\n",
            "epoch 373, loss with penalty 0.9420362710952759\n",
            "New min loss = tensor([0.9413], grad_fn=<AddBackward0>)\n",
            "epoch 374, loss with penalty 0.9412990808486938\n",
            "epoch 375, loss with penalty 0.9405625462532043\n",
            "New min loss = tensor([0.9398], grad_fn=<AddBackward0>)\n",
            "epoch 376, loss with penalty 0.9398286938667297\n",
            "epoch 377, loss with penalty 0.9390983581542969\n",
            "New min loss = tensor([0.9384], grad_fn=<AddBackward0>)\n",
            "epoch 378, loss with penalty 0.9383695721626282\n",
            "epoch 379, loss with penalty 0.9376438856124878\n",
            "New min loss = tensor([0.9369], grad_fn=<AddBackward0>)\n",
            "epoch 380, loss with penalty 0.9369191527366638\n",
            "epoch 381, loss with penalty 0.9361979961395264\n",
            "New min loss = tensor([0.9355], grad_fn=<AddBackward0>)\n",
            "epoch 382, loss with penalty 0.9354783296585083\n",
            "epoch 383, loss with penalty 0.9347618818283081\n",
            "New min loss = tensor([0.9340], grad_fn=<AddBackward0>)\n",
            "epoch 384, loss with penalty 0.9340470433235168\n",
            "epoch 385, loss with penalty 0.9333338141441345\n",
            "New min loss = tensor([0.9326], grad_fn=<AddBackward0>)\n",
            "epoch 386, loss with penalty 0.9326239228248596\n",
            "epoch 387, loss with penalty 0.9319161772727966\n",
            "New min loss = tensor([0.9312], grad_fn=<AddBackward0>)\n",
            "epoch 388, loss with penalty 0.9312118887901306\n",
            "epoch 389, loss with penalty 0.9305068254470825\n",
            "New min loss = tensor([0.9298], grad_fn=<AddBackward0>)\n",
            "epoch 390, loss with penalty 0.9298065304756165\n",
            "epoch 391, loss with penalty 0.9291073083877563\n",
            "New min loss = tensor([0.9284], grad_fn=<AddBackward0>)\n",
            "epoch 392, loss with penalty 0.9284099340438843\n",
            "epoch 393, loss with penalty 0.927716076374054\n",
            "New min loss = tensor([0.9270], grad_fn=<AddBackward0>)\n",
            "epoch 394, loss with penalty 0.9270229935646057\n",
            "epoch 395, loss with penalty 0.9263338446617126\n",
            "New min loss = tensor([0.9256], grad_fn=<AddBackward0>)\n",
            "epoch 396, loss with penalty 0.9256450533866882\n",
            "epoch 397, loss with penalty 0.9249584078788757\n",
            "New min loss = tensor([0.9243], grad_fn=<AddBackward0>)\n",
            "epoch 398, loss with penalty 0.924275279045105\n",
            "epoch 399, loss with penalty 0.9235923290252686\n",
            "New min loss = tensor([0.9229], grad_fn=<AddBackward0>)\n",
            "epoch 400, loss with penalty 0.9229130744934082\n",
            "epoch 401, loss with penalty 0.922234833240509\n",
            "New min loss = tensor([0.9216], grad_fn=<AddBackward0>)\n",
            "epoch 402, loss with penalty 0.9215601682662964\n",
            "epoch 403, loss with penalty 0.9208872318267822\n",
            "New min loss = tensor([0.9202], grad_fn=<AddBackward0>)\n",
            "epoch 404, loss with penalty 0.9202155470848083\n",
            "epoch 405, loss with penalty 0.9195455312728882\n",
            "New min loss = tensor([0.9189], grad_fn=<AddBackward0>)\n",
            "epoch 406, loss with penalty 0.9188777208328247\n",
            "epoch 407, loss with penalty 0.9182134866714478\n",
            "New min loss = tensor([0.9175], grad_fn=<AddBackward0>)\n",
            "epoch 408, loss with penalty 0.9175488352775574\n",
            "epoch 409, loss with penalty 0.916887640953064\n",
            "New min loss = tensor([0.9162], grad_fn=<AddBackward0>)\n",
            "epoch 410, loss with penalty 0.9162290096282959\n",
            "epoch 411, loss with penalty 0.9155716300010681\n",
            "New min loss = tensor([0.9149], grad_fn=<AddBackward0>)\n",
            "epoch 412, loss with penalty 0.9149161577224731\n",
            "epoch 413, loss with penalty 0.914262592792511\n",
            "New min loss = tensor([0.9136], grad_fn=<AddBackward0>)\n",
            "epoch 414, loss with penalty 0.9136112928390503\n",
            "epoch 415, loss with penalty 0.9129611849784851\n",
            "New min loss = tensor([0.9123], grad_fn=<AddBackward0>)\n",
            "epoch 416, loss with penalty 0.9123134613037109\n",
            "epoch 417, loss with penalty 0.9116686582565308\n",
            "New min loss = tensor([0.9110], grad_fn=<AddBackward0>)\n",
            "epoch 418, loss with penalty 0.9110230207443237\n",
            "epoch 419, loss with penalty 0.9103823304176331\n",
            "New min loss = tensor([0.9097], grad_fn=<AddBackward0>)\n",
            "epoch 420, loss with penalty 0.9097413420677185\n",
            "epoch 421, loss with penalty 0.9091034531593323\n",
            "New min loss = tensor([0.9085], grad_fn=<AddBackward0>)\n",
            "epoch 422, loss with penalty 0.9084661602973938\n",
            "epoch 423, loss with penalty 0.9078324437141418\n",
            "New min loss = tensor([0.9072], grad_fn=<AddBackward0>)\n",
            "epoch 424, loss with penalty 0.9071996212005615\n",
            "epoch 425, loss with penalty 0.9065680503845215\n",
            "New min loss = tensor([0.9059], grad_fn=<AddBackward0>)\n",
            "epoch 426, loss with penalty 0.9059389233589172\n",
            "epoch 427, loss with penalty 0.9053110480308533\n",
            "New min loss = tensor([0.9047], grad_fn=<AddBackward0>)\n",
            "epoch 428, loss with penalty 0.9046865105628967\n",
            "epoch 429, loss with penalty 0.9040625691413879\n",
            "New min loss = tensor([0.9034], grad_fn=<AddBackward0>)\n",
            "epoch 430, loss with penalty 0.9034407734870911\n",
            "epoch 431, loss with penalty 0.9028199911117554\n",
            "New min loss = tensor([0.9022], grad_fn=<AddBackward0>)\n",
            "epoch 432, loss with penalty 0.9022014141082764\n",
            "epoch 433, loss with penalty 0.9015846848487854\n",
            "New min loss = tensor([0.9010], grad_fn=<AddBackward0>)\n",
            "epoch 434, loss with penalty 0.9009696245193481\n",
            "epoch 435, loss with penalty 0.900356650352478\n",
            "New min loss = tensor([0.8997], grad_fn=<AddBackward0>)\n",
            "epoch 436, loss with penalty 0.8997454643249512\n",
            "epoch 437, loss with penalty 0.8991350531578064\n",
            "New min loss = tensor([0.8985], grad_fn=<AddBackward0>)\n",
            "epoch 438, loss with penalty 0.898526668548584\n",
            "epoch 439, loss with penalty 0.8979200124740601\n",
            "New min loss = tensor([0.8973], grad_fn=<AddBackward0>)\n",
            "epoch 440, loss with penalty 0.8973155617713928\n",
            "epoch 441, loss with penalty 0.896712601184845\n",
            "New min loss = tensor([0.8961], grad_fn=<AddBackward0>)\n",
            "epoch 442, loss with penalty 0.8961107730865479\n",
            "epoch 443, loss with penalty 0.8955112099647522\n",
            "New min loss = tensor([0.8949], grad_fn=<AddBackward0>)\n",
            "epoch 444, loss with penalty 0.894912600517273\n",
            "epoch 445, loss with penalty 0.8943159580230713\n",
            "New min loss = tensor([0.8937], grad_fn=<AddBackward0>)\n",
            "epoch 446, loss with penalty 0.8937207460403442\n",
            "epoch 447, loss with penalty 0.8931270837783813\n",
            "New min loss = tensor([0.8925], grad_fn=<AddBackward0>)\n",
            "epoch 448, loss with penalty 0.8925352692604065\n",
            "epoch 449, loss with penalty 0.891944408416748\n",
            "New min loss = tensor([0.8914], grad_fn=<AddBackward0>)\n",
            "epoch 450, loss with penalty 0.8913560509681702\n",
            "epoch 451, loss with penalty 0.8907687067985535\n",
            "New min loss = tensor([0.8902], grad_fn=<AddBackward0>)\n",
            "epoch 452, loss with penalty 0.8901837468147278\n",
            "epoch 453, loss with penalty 0.8895995020866394\n",
            "New min loss = tensor([0.8890], grad_fn=<AddBackward0>)\n",
            "epoch 454, loss with penalty 0.8890154361724854\n",
            "epoch 455, loss with penalty 0.888435423374176\n",
            "New min loss = tensor([0.8879], grad_fn=<AddBackward0>)\n",
            "epoch 456, loss with penalty 0.8878565430641174\n",
            "epoch 457, loss with penalty 0.8872777819633484\n",
            "New min loss = tensor([0.8867], grad_fn=<AddBackward0>)\n",
            "epoch 458, loss with penalty 0.8867015838623047\n",
            "epoch 459, loss with penalty 0.8861269950866699\n",
            "New min loss = tensor([0.8856], grad_fn=<AddBackward0>)\n",
            "epoch 460, loss with penalty 0.8855525851249695\n",
            "epoch 461, loss with penalty 0.8849815726280212\n",
            "New min loss = tensor([0.8844], grad_fn=<AddBackward0>)\n",
            "epoch 462, loss with penalty 0.884410560131073\n",
            "epoch 463, loss with penalty 0.8838412761688232\n",
            "New min loss = tensor([0.8833], grad_fn=<AddBackward0>)\n",
            "epoch 464, loss with penalty 0.8832735419273376\n",
            "epoch 465, loss with penalty 0.8827081918716431\n",
            "New min loss = tensor([0.8821], grad_fn=<AddBackward0>)\n",
            "epoch 466, loss with penalty 0.8821440935134888\n",
            "epoch 467, loss with penalty 0.881579577922821\n",
            "New min loss = tensor([0.8810], grad_fn=<AddBackward0>)\n",
            "epoch 468, loss with penalty 0.8810174465179443\n",
            "epoch 469, loss with penalty 0.88045734167099\n",
            "New min loss = tensor([0.8799], grad_fn=<AddBackward0>)\n",
            "epoch 470, loss with penalty 0.8798978328704834\n",
            "epoch 471, loss with penalty 0.8793413043022156\n",
            "New min loss = tensor([0.8788], grad_fn=<AddBackward0>)\n",
            "epoch 472, loss with penalty 0.8787842392921448\n",
            "epoch 473, loss with penalty 0.8782289028167725\n",
            "New min loss = tensor([0.8777], grad_fn=<AddBackward0>)\n",
            "epoch 474, loss with penalty 0.8776757121086121\n",
            "epoch 475, loss with penalty 0.8771233558654785\n",
            "New min loss = tensor([0.8766], grad_fn=<AddBackward0>)\n",
            "epoch 476, loss with penalty 0.8765720725059509\n",
            "epoch 477, loss with penalty 0.8760228157043457\n",
            "New min loss = tensor([0.8755], grad_fn=<AddBackward0>)\n",
            "epoch 478, loss with penalty 0.8754751682281494\n",
            "epoch 479, loss with penalty 0.8749282956123352\n",
            "New min loss = tensor([0.8744], grad_fn=<AddBackward0>)\n",
            "epoch 480, loss with penalty 0.8743826746940613\n",
            "epoch 481, loss with penalty 0.8738380074501038\n",
            "New min loss = tensor([0.8733], grad_fn=<AddBackward0>)\n",
            "epoch 482, loss with penalty 0.8732945322990417\n",
            "epoch 483, loss with penalty 0.8727535009384155\n",
            "New min loss = tensor([0.8722], grad_fn=<AddBackward0>)\n",
            "epoch 484, loss with penalty 0.8722134232521057\n",
            "epoch 485, loss with penalty 0.8716737627983093\n",
            "New min loss = tensor([0.8711], grad_fn=<AddBackward0>)\n",
            "epoch 486, loss with penalty 0.8711362481117249\n",
            "epoch 487, loss with penalty 0.8705994486808777\n",
            "New min loss = tensor([0.8701], grad_fn=<AddBackward0>)\n",
            "epoch 488, loss with penalty 0.8700643181800842\n",
            "epoch 489, loss with penalty 0.8695303797721863\n",
            "New min loss = tensor([0.8690], grad_fn=<AddBackward0>)\n",
            "epoch 490, loss with penalty 0.8689981698989868\n",
            "epoch 491, loss with penalty 0.868466854095459\n",
            "New min loss = tensor([0.8679], grad_fn=<AddBackward0>)\n",
            "epoch 492, loss with penalty 0.8679366707801819\n",
            "epoch 493, loss with penalty 0.8674068450927734\n",
            "New min loss = tensor([0.8669], grad_fn=<AddBackward0>)\n",
            "epoch 494, loss with penalty 0.8668792843818665\n",
            "epoch 495, loss with penalty 0.8663526773452759\n",
            "New min loss = tensor([0.8658], grad_fn=<AddBackward0>)\n",
            "epoch 496, loss with penalty 0.8658269643783569\n",
            "epoch 497, loss with penalty 0.8653029799461365\n",
            "New min loss = tensor([0.8648], grad_fn=<AddBackward0>)\n",
            "epoch 498, loss with penalty 0.864780843257904\n",
            "epoch 499, loss with penalty 0.8642584085464478\n",
            "New min loss = tensor([0.8637], grad_fn=<AddBackward0>)\n",
            "epoch 500, loss with penalty 0.8637377619743347\n",
            "epoch 501, loss with penalty 0.8632186651229858\n",
            "New min loss = tensor([0.8627], grad_fn=<AddBackward0>)\n",
            "epoch 502, loss with penalty 0.8627005219459534\n",
            "epoch 503, loss with penalty 0.8621825575828552\n",
            "New min loss = tensor([0.8617], grad_fn=<AddBackward0>)\n",
            "epoch 504, loss with penalty 0.8616674542427063\n",
            "epoch 505, loss with penalty 0.8611520528793335\n",
            "New min loss = tensor([0.8606], grad_fn=<AddBackward0>)\n",
            "epoch 506, loss with penalty 0.860637903213501\n",
            "epoch 507, loss with penalty 0.8601253628730774\n",
            "New min loss = tensor([0.8596], grad_fn=<AddBackward0>)\n",
            "epoch 508, loss with penalty 0.8596137166023254\n",
            "epoch 509, loss with penalty 0.859104335308075\n",
            "New min loss = tensor([0.8586], grad_fn=<AddBackward0>)\n",
            "epoch 510, loss with penalty 0.8585952520370483\n",
            "epoch 511, loss with penalty 0.8580870628356934\n",
            "New min loss = tensor([0.8576], grad_fn=<AddBackward0>)\n",
            "epoch 512, loss with penalty 0.8575800657272339\n",
            "epoch 513, loss with penalty 0.8570748567581177\n",
            "New min loss = tensor([0.8566], grad_fn=<AddBackward0>)\n",
            "epoch 514, loss with penalty 0.856569230556488\n",
            "epoch 515, loss with penalty 0.8560659289360046\n",
            "New min loss = tensor([0.8556], grad_fn=<AddBackward0>)\n",
            "epoch 516, loss with penalty 0.8555629849433899\n",
            "epoch 517, loss with penalty 0.8550608158111572\n",
            "New min loss = tensor([0.8546], grad_fn=<AddBackward0>)\n",
            "epoch 518, loss with penalty 0.8545604944229126\n",
            "epoch 519, loss with penalty 0.8540614247322083\n",
            "epoch 520, loss with penalty 0.8535632491111755\n",
            "New min loss = tensor([0.8531], grad_fn=<AddBackward0>)\n",
            "epoch 521, loss with penalty 0.8530659675598145\n",
            "epoch 522, loss with penalty 0.8525699377059937\n",
            "epoch 523, loss with penalty 0.8520750403404236\n",
            "New min loss = tensor([0.8516], grad_fn=<AddBackward0>)\n",
            "epoch 524, loss with penalty 0.8515806198120117\n",
            "epoch 525, loss with penalty 0.8510875105857849\n",
            "epoch 526, loss with penalty 0.8505959510803223\n",
            "New min loss = tensor([0.8501], grad_fn=<AddBackward0>)\n",
            "epoch 527, loss with penalty 0.8501048684120178\n",
            "epoch 528, loss with penalty 0.8496143817901611\n",
            "epoch 529, loss with penalty 0.8491247296333313\n",
            "New min loss = tensor([0.8486], grad_fn=<AddBackward0>)\n",
            "epoch 530, loss with penalty 0.8486383557319641\n",
            "epoch 531, loss with penalty 0.8481504321098328\n",
            "epoch 532, loss with penalty 0.8476651906967163\n",
            "New min loss = tensor([0.8472], grad_fn=<AddBackward0>)\n",
            "epoch 533, loss with penalty 0.8471791744232178\n",
            "epoch 534, loss with penalty 0.8466953039169312\n",
            "epoch 535, loss with penalty 0.8462128043174744\n",
            "New min loss = tensor([0.8457], grad_fn=<AddBackward0>)\n",
            "epoch 536, loss with penalty 0.8457304239273071\n",
            "epoch 537, loss with penalty 0.845249593257904\n",
            "epoch 538, loss with penalty 0.8447694182395935\n",
            "New min loss = tensor([0.8443], grad_fn=<AddBackward0>)\n",
            "epoch 539, loss with penalty 0.8442898392677307\n",
            "epoch 540, loss with penalty 0.8438123464584351\n",
            "epoch 541, loss with penalty 0.8433346152305603\n",
            "New min loss = tensor([0.8429], grad_fn=<AddBackward0>)\n",
            "epoch 542, loss with penalty 0.8428579568862915\n",
            "epoch 543, loss with penalty 0.8423828482627869\n",
            "epoch 544, loss with penalty 0.8419077396392822\n",
            "New min loss = tensor([0.8414], grad_fn=<AddBackward0>)\n",
            "epoch 545, loss with penalty 0.8414346575737\n",
            "epoch 546, loss with penalty 0.8409627079963684\n",
            "epoch 547, loss with penalty 0.8404910564422607\n",
            "New min loss = tensor([0.8400], grad_fn=<AddBackward0>)\n",
            "epoch 548, loss with penalty 0.8400192856788635\n",
            "epoch 549, loss with penalty 0.8395496606826782\n",
            "epoch 550, loss with penalty 0.839080810546875\n",
            "New min loss = tensor([0.8386], grad_fn=<AddBackward0>)\n",
            "epoch 551, loss with penalty 0.8386133313179016\n",
            "epoch 552, loss with penalty 0.8381463885307312\n",
            "epoch 553, loss with penalty 0.8376797437667847\n",
            "New min loss = tensor([0.8372], grad_fn=<AddBackward0>)\n",
            "epoch 554, loss with penalty 0.8372146487236023\n",
            "epoch 555, loss with penalty 0.8367501497268677\n",
            "epoch 556, loss with penalty 0.836286723613739\n",
            "New min loss = tensor([0.8358], grad_fn=<AddBackward0>)\n",
            "epoch 557, loss with penalty 0.8358238935470581\n",
            "epoch 558, loss with penalty 0.8353621959686279\n",
            "epoch 559, loss with penalty 0.8349007964134216\n",
            "New min loss = tensor([0.8344], grad_fn=<AddBackward0>)\n",
            "epoch 560, loss with penalty 0.8344415426254272\n",
            "epoch 561, loss with penalty 0.8339816331863403\n",
            "epoch 562, loss with penalty 0.8335233926773071\n",
            "New min loss = tensor([0.8331], grad_fn=<AddBackward0>)\n",
            "epoch 563, loss with penalty 0.8330656886100769\n",
            "epoch 564, loss with penalty 0.8326092958450317\n",
            "epoch 565, loss with penalty 0.8321540951728821\n",
            "New min loss = tensor([0.8317], grad_fn=<AddBackward0>)\n",
            "epoch 566, loss with penalty 0.8316987752914429\n",
            "epoch 567, loss with penalty 0.8312442302703857\n",
            "epoch 568, loss with penalty 0.830791711807251\n",
            "New min loss = tensor([0.8303], grad_fn=<AddBackward0>)\n",
            "epoch 569, loss with penalty 0.8303394317626953\n",
            "epoch 570, loss with penalty 0.8298877477645874\n",
            "epoch 571, loss with penalty 0.8294369578361511\n",
            "New min loss = tensor([0.8290], grad_fn=<AddBackward0>)\n",
            "epoch 572, loss with penalty 0.8289867043495178\n",
            "epoch 573, loss with penalty 0.8285375237464905\n",
            "epoch 574, loss with penalty 0.8280885219573975\n",
            "New min loss = tensor([0.8276], grad_fn=<AddBackward0>)\n",
            "epoch 575, loss with penalty 0.8276418447494507\n",
            "epoch 576, loss with penalty 0.8271945714950562\n",
            "epoch 577, loss with penalty 0.8267490863800049\n",
            "New min loss = tensor([0.8263], grad_fn=<AddBackward0>)\n",
            "epoch 578, loss with penalty 0.8263042569160461\n",
            "epoch 579, loss with penalty 0.8258602023124695\n",
            "epoch 580, loss with penalty 0.8254166841506958\n",
            "New min loss = tensor([0.8250], grad_fn=<AddBackward0>)\n",
            "epoch 581, loss with penalty 0.8249743580818176\n",
            "epoch 582, loss with penalty 0.8245328068733215\n",
            "epoch 583, loss with penalty 0.8240908980369568\n",
            "New min loss = tensor([0.8237], grad_fn=<AddBackward0>)\n",
            "epoch 584, loss with penalty 0.8236514329910278\n",
            "epoch 585, loss with penalty 0.823211133480072\n",
            "epoch 586, loss with penalty 0.8227726817131042\n",
            "New min loss = tensor([0.8223], grad_fn=<AddBackward0>)\n",
            "epoch 587, loss with penalty 0.8223347663879395\n",
            "epoch 588, loss with penalty 0.8218982815742493\n",
            "epoch 589, loss with penalty 0.8214616179466248\n",
            "New min loss = tensor([0.8210], grad_fn=<AddBackward0>)\n",
            "epoch 590, loss with penalty 0.8210257887840271\n",
            "epoch 591, loss with penalty 0.8205912113189697\n",
            "epoch 592, loss with penalty 0.8201574087142944\n",
            "New min loss = tensor([0.8197], grad_fn=<AddBackward0>)\n",
            "epoch 593, loss with penalty 0.8197230100631714\n",
            "epoch 594, loss with penalty 0.8192905783653259\n",
            "epoch 595, loss with penalty 0.8188595175743103\n",
            "New min loss = tensor([0.8184], grad_fn=<AddBackward0>)\n",
            "epoch 596, loss with penalty 0.8184285759925842\n",
            "epoch 597, loss with penalty 0.8179983496665955\n",
            "epoch 598, loss with penalty 0.8175678849220276\n",
            "New min loss = tensor([0.8171], grad_fn=<AddBackward0>)\n",
            "epoch 599, loss with penalty 0.8171389698982239\n",
            "epoch 600, loss with penalty 0.8167117238044739\n",
            "epoch 601, loss with penalty 0.8162840008735657\n",
            "New min loss = tensor([0.8159], grad_fn=<AddBackward0>)\n",
            "epoch 602, loss with penalty 0.815857470035553\n",
            "epoch 603, loss with penalty 0.8154314756393433\n",
            "epoch 604, loss with penalty 0.8150061964988708\n",
            "New min loss = tensor([0.8146], grad_fn=<AddBackward0>)\n",
            "epoch 605, loss with penalty 0.8145816326141357\n",
            "epoch 606, loss with penalty 0.8141583204269409\n",
            "epoch 607, loss with penalty 0.8137357234954834\n",
            "New min loss = tensor([0.8133], grad_fn=<AddBackward0>)\n",
            "epoch 608, loss with penalty 0.8133131265640259\n",
            "epoch 609, loss with penalty 0.8128913640975952\n",
            "epoch 610, loss with penalty 0.8124708533287048\n",
            "New min loss = tensor([0.8121], grad_fn=<AddBackward0>)\n",
            "epoch 611, loss with penalty 0.8120505213737488\n",
            "epoch 612, loss with penalty 0.8116307854652405\n",
            "epoch 613, loss with penalty 0.8112115859985352\n",
            "New min loss = tensor([0.8108], grad_fn=<AddBackward0>)\n",
            "epoch 614, loss with penalty 0.8107939958572388\n",
            "epoch 615, loss with penalty 0.8103768229484558\n",
            "epoch 616, loss with penalty 0.8099597096443176\n",
            "New min loss = tensor([0.8095], grad_fn=<AddBackward0>)\n",
            "epoch 617, loss with penalty 0.8095441460609436\n",
            "epoch 618, loss with penalty 0.8091290593147278\n",
            "epoch 619, loss with penalty 0.8087137341499329\n",
            "New min loss = tensor([0.8083], grad_fn=<AddBackward0>)\n",
            "epoch 620, loss with penalty 0.8083007335662842\n",
            "epoch 621, loss with penalty 0.8078869581222534\n",
            "epoch 622, loss with penalty 0.8074750900268555\n",
            "New min loss = tensor([0.8071], grad_fn=<AddBackward0>)\n",
            "epoch 623, loss with penalty 0.8070628046989441\n",
            "epoch 624, loss with penalty 0.806652307510376\n",
            "epoch 625, loss with penalty 0.8062418103218079\n",
            "New min loss = tensor([0.8058], grad_fn=<AddBackward0>)\n",
            "epoch 626, loss with penalty 0.805831789970398\n",
            "epoch 627, loss with penalty 0.8054232001304626\n",
            "epoch 628, loss with penalty 0.8050147294998169\n",
            "New min loss = tensor([0.8046], grad_fn=<AddBackward0>)\n",
            "epoch 629, loss with penalty 0.8046064376831055\n",
            "epoch 630, loss with penalty 0.8041992783546448\n",
            "epoch 631, loss with penalty 0.8037935495376587\n",
            "New min loss = tensor([0.8034], grad_fn=<AddBackward0>)\n",
            "epoch 632, loss with penalty 0.8033874034881592\n",
            "epoch 633, loss with penalty 0.8029822111129761\n",
            "epoch 634, loss with penalty 0.802578330039978\n",
            "New min loss = tensor([0.8022], grad_fn=<AddBackward0>)\n",
            "epoch 635, loss with penalty 0.8021742105484009\n",
            "epoch 636, loss with penalty 0.8017709255218506\n",
            "epoch 637, loss with penalty 0.8013687133789062\n",
            "New min loss = tensor([0.8010], grad_fn=<AddBackward0>)\n",
            "epoch 638, loss with penalty 0.8009670376777649\n",
            "epoch 639, loss with penalty 0.8005660772323608\n",
            "epoch 640, loss with penalty 0.800165057182312\n",
            "New min loss = tensor([0.7998], grad_fn=<AddBackward0>)\n",
            "epoch 641, loss with penalty 0.799765408039093\n",
            "epoch 642, loss with penalty 0.7993660569190979\n",
            "epoch 643, loss with penalty 0.7989683747291565\n",
            "New min loss = tensor([0.7986], grad_fn=<AddBackward0>)\n",
            "epoch 644, loss with penalty 0.7985700964927673\n",
            "epoch 645, loss with penalty 0.7981730699539185\n",
            "epoch 646, loss with penalty 0.797776460647583\n",
            "New min loss = tensor([0.7974], grad_fn=<AddBackward0>)\n",
            "epoch 647, loss with penalty 0.7973805069923401\n",
            "epoch 648, loss with penalty 0.7969854474067688\n",
            "epoch 649, loss with penalty 0.7965900301933289\n",
            "New min loss = tensor([0.7962], grad_fn=<AddBackward0>)\n",
            "epoch 650, loss with penalty 0.7961963415145874\n",
            "epoch 651, loss with penalty 0.7958030104637146\n",
            "epoch 652, loss with penalty 0.795409619808197\n",
            "New min loss = tensor([0.7950], grad_fn=<AddBackward0>)\n",
            "epoch 653, loss with penalty 0.7950177788734436\n",
            "epoch 654, loss with penalty 0.7946258783340454\n",
            "epoch 655, loss with penalty 0.7942347526550293\n",
            "New min loss = tensor([0.7938], grad_fn=<AddBackward0>)\n",
            "epoch 656, loss with penalty 0.7938446998596191\n",
            "epoch 657, loss with penalty 0.7934547662734985\n",
            "epoch 658, loss with penalty 0.7930658459663391\n",
            "New min loss = tensor([0.7927], grad_fn=<AddBackward0>)\n",
            "epoch 659, loss with penalty 0.7926774024963379\n",
            "epoch 660, loss with penalty 0.792289674282074\n",
            "epoch 661, loss with penalty 0.7919026017189026\n",
            "New min loss = tensor([0.7915], grad_fn=<AddBackward0>)\n",
            "epoch 662, loss with penalty 0.7915164232254028\n",
            "epoch 663, loss with penalty 0.7911298871040344\n",
            "epoch 664, loss with penalty 0.7907446026802063\n",
            "New min loss = tensor([0.7904], grad_fn=<AddBackward0>)\n",
            "epoch 665, loss with penalty 0.7903605103492737\n",
            "epoch 666, loss with penalty 0.7899758219718933\n",
            "epoch 667, loss with penalty 0.7895925641059875\n",
            "New min loss = tensor([0.7892], grad_fn=<AddBackward0>)\n",
            "epoch 668, loss with penalty 0.7892093062400818\n",
            "epoch 669, loss with penalty 0.7888280153274536\n",
            "epoch 670, loss with penalty 0.788445234298706\n",
            "New min loss = tensor([0.7881], grad_fn=<AddBackward0>)\n",
            "epoch 671, loss with penalty 0.7880643010139465\n",
            "epoch 672, loss with penalty 0.7876835465431213\n",
            "epoch 673, loss with penalty 0.7873047590255737\n",
            "New min loss = tensor([0.7869], grad_fn=<AddBackward0>)\n",
            "epoch 674, loss with penalty 0.7869239449501038\n",
            "epoch 675, loss with penalty 0.7865462303161621\n",
            "epoch 676, loss with penalty 0.7861679792404175\n",
            "New min loss = tensor([0.7858], grad_fn=<AddBackward0>)\n",
            "epoch 677, loss with penalty 0.7857901453971863\n",
            "epoch 678, loss with penalty 0.7854136824607849\n",
            "epoch 679, loss with penalty 0.7850369215011597\n",
            "New min loss = tensor([0.7847], grad_fn=<AddBackward0>)\n",
            "epoch 680, loss with penalty 0.7846619486808777\n",
            "epoch 681, loss with penalty 0.7842862010002136\n",
            "epoch 682, loss with penalty 0.7839123010635376\n",
            "New min loss = tensor([0.7835], grad_fn=<AddBackward0>)\n",
            "epoch 683, loss with penalty 0.7835376858711243\n",
            "epoch 684, loss with penalty 0.7831639647483826\n",
            "epoch 685, loss with penalty 0.7827908992767334\n",
            "New min loss = tensor([0.7824], grad_fn=<AddBackward0>)\n",
            "epoch 686, loss with penalty 0.7824187874794006\n",
            "epoch 687, loss with penalty 0.7820478081703186\n",
            "epoch 688, loss with penalty 0.781676709651947\n",
            "New min loss = tensor([0.7813], grad_fn=<AddBackward0>)\n",
            "epoch 689, loss with penalty 0.7813059091567993\n",
            "epoch 690, loss with penalty 0.7809357643127441\n",
            "epoch 691, loss with penalty 0.7805670499801636\n",
            "New min loss = tensor([0.7802], grad_fn=<AddBackward0>)\n",
            "epoch 692, loss with penalty 0.7801967859268188\n",
            "epoch 693, loss with penalty 0.7798290848731995\n",
            "epoch 694, loss with penalty 0.7794619798660278\n",
            "New min loss = tensor([0.7791], grad_fn=<AddBackward0>)\n",
            "epoch 695, loss with penalty 0.7790954113006592\n",
            "epoch 696, loss with penalty 0.7787285447120667\n",
            "epoch 697, loss with penalty 0.7783632874488831\n",
            "New min loss = tensor([0.7780], grad_fn=<AddBackward0>)\n",
            "epoch 698, loss with penalty 0.7779982686042786\n",
            "epoch 699, loss with penalty 0.7776330709457397\n",
            "epoch 700, loss with penalty 0.7772688269615173\n",
            "New min loss = tensor([0.7769], grad_fn=<AddBackward0>)\n",
            "epoch 701, loss with penalty 0.7769047021865845\n",
            "epoch 702, loss with penalty 0.7765421271324158\n",
            "epoch 703, loss with penalty 0.7761800289154053\n",
            "New min loss = tensor([0.7758], grad_fn=<AddBackward0>)\n",
            "epoch 704, loss with penalty 0.7758182883262634\n",
            "epoch 705, loss with penalty 0.7754557728767395\n",
            "epoch 706, loss with penalty 0.7750952243804932\n",
            "New min loss = tensor([0.7747], grad_fn=<AddBackward0>)\n",
            "epoch 707, loss with penalty 0.7747355699539185\n",
            "epoch 708, loss with penalty 0.7743756175041199\n",
            "epoch 709, loss with penalty 0.7740169167518616\n",
            "New min loss = tensor([0.7737], grad_fn=<AddBackward0>)\n",
            "epoch 710, loss with penalty 0.7736577987670898\n",
            "epoch 711, loss with penalty 0.773299515247345\n",
            "epoch 712, loss with penalty 0.7729429602622986\n",
            "New min loss = tensor([0.7726], grad_fn=<AddBackward0>)\n",
            "epoch 713, loss with penalty 0.7725858688354492\n",
            "epoch 714, loss with penalty 0.7722294926643372\n",
            "epoch 715, loss with penalty 0.7718735933303833\n",
            "New min loss = tensor([0.7715], grad_fn=<AddBackward0>)\n",
            "epoch 716, loss with penalty 0.7715181708335876\n",
            "epoch 717, loss with penalty 0.7711634039878845\n",
            "epoch 718, loss with penalty 0.7708092331886292\n",
            "New min loss = tensor([0.7705], grad_fn=<AddBackward0>)\n",
            "epoch 719, loss with penalty 0.7704566121101379\n",
            "epoch 720, loss with penalty 0.7701034545898438\n",
            "epoch 721, loss with penalty 0.7697507739067078\n",
            "New min loss = tensor([0.7694], grad_fn=<AddBackward0>)\n",
            "epoch 722, loss with penalty 0.7693996429443359\n",
            "epoch 723, loss with penalty 0.7690476775169373\n",
            "epoch 724, loss with penalty 0.7686966061592102\n",
            "New min loss = tensor([0.7683], grad_fn=<AddBackward0>)\n",
            "epoch 725, loss with penalty 0.76834636926651\n",
            "epoch 726, loss with penalty 0.7679966688156128\n",
            "epoch 727, loss with penalty 0.7676478624343872\n",
            "New min loss = tensor([0.7673], grad_fn=<AddBackward0>)\n",
            "epoch 728, loss with penalty 0.767298698425293\n",
            "epoch 729, loss with penalty 0.7669507265090942\n",
            "epoch 730, loss with penalty 0.7666031718254089\n",
            "New min loss = tensor([0.7663], grad_fn=<AddBackward0>)\n",
            "epoch 731, loss with penalty 0.7662562131881714\n",
            "epoch 732, loss with penalty 0.7659087777137756\n",
            "epoch 733, loss with penalty 0.7655639052391052\n",
            "New min loss = tensor([0.7652], grad_fn=<AddBackward0>)\n",
            "epoch 734, loss with penalty 0.7652175426483154\n",
            "epoch 735, loss with penalty 0.7648723125457764\n",
            "epoch 736, loss with penalty 0.7645295262336731\n",
            "New min loss = tensor([0.7642], grad_fn=<AddBackward0>)\n",
            "epoch 737, loss with penalty 0.7641849517822266\n",
            "epoch 738, loss with penalty 0.7638417482376099\n",
            "epoch 739, loss with penalty 0.7634984850883484\n",
            "New min loss = tensor([0.7632], grad_fn=<AddBackward0>)\n",
            "epoch 740, loss with penalty 0.763156533241272\n",
            "epoch 741, loss with penalty 0.7628145217895508\n",
            "epoch 742, loss with penalty 0.7624738812446594\n",
            "New min loss = tensor([0.7621], grad_fn=<AddBackward0>)\n",
            "epoch 743, loss with penalty 0.7621335983276367\n",
            "epoch 744, loss with penalty 0.7617936134338379\n",
            "epoch 745, loss with penalty 0.7614536881446838\n",
            "New min loss = tensor([0.7611], grad_fn=<AddBackward0>)\n",
            "epoch 746, loss with penalty 0.7611150145530701\n",
            "epoch 747, loss with penalty 0.7607760429382324\n",
            "epoch 748, loss with penalty 0.7604383230209351\n",
            "New min loss = tensor([0.7601], grad_fn=<AddBackward0>)\n",
            "epoch 749, loss with penalty 0.7601009607315063\n",
            "epoch 750, loss with penalty 0.7597636580467224\n",
            "epoch 751, loss with penalty 0.7594273686408997\n",
            "New min loss = tensor([0.7591], grad_fn=<AddBackward0>)\n",
            "epoch 752, loss with penalty 0.7590909004211426\n",
            "epoch 753, loss with penalty 0.7587554454803467\n",
            "epoch 754, loss with penalty 0.7584205269813538\n",
            "New min loss = tensor([0.7581], grad_fn=<AddBackward0>)\n",
            "epoch 755, loss with penalty 0.7580869197845459\n",
            "epoch 756, loss with penalty 0.7577530145645142\n",
            "epoch 757, loss with penalty 0.7574190497398376\n",
            "New min loss = tensor([0.7571], grad_fn=<AddBackward0>)\n",
            "epoch 758, loss with penalty 0.7570866346359253\n",
            "epoch 759, loss with penalty 0.7567545771598816\n",
            "epoch 760, loss with penalty 0.7564229369163513\n",
            "epoch 761, loss with penalty 0.7560916543006897\n",
            "New min loss = tensor([0.7558], grad_fn=<AddBackward0>)\n",
            "epoch 762, loss with penalty 0.7557607293128967\n",
            "epoch 763, loss with penalty 0.755430281162262\n",
            "epoch 764, loss with penalty 0.7551012635231018\n",
            "epoch 765, loss with penalty 0.7547718286514282\n",
            "New min loss = tensor([0.7544], grad_fn=<AddBackward0>)\n",
            "epoch 766, loss with penalty 0.7544435262680054\n",
            "epoch 767, loss with penalty 0.754115641117096\n",
            "epoch 768, loss with penalty 0.7537875175476074\n",
            "epoch 769, loss with penalty 0.7534600496292114\n",
            "New min loss = tensor([0.7531], grad_fn=<AddBackward0>)\n",
            "epoch 770, loss with penalty 0.7531337141990662\n",
            "epoch 771, loss with penalty 0.7528082728385925\n",
            "epoch 772, loss with penalty 0.7524820566177368\n",
            "epoch 773, loss with penalty 0.7521572709083557\n",
            "New min loss = tensor([0.7518], grad_fn=<AddBackward0>)\n",
            "epoch 774, loss with penalty 0.7518329620361328\n",
            "epoch 775, loss with penalty 0.751508891582489\n",
            "epoch 776, loss with penalty 0.7511850595474243\n",
            "epoch 777, loss with penalty 0.750861644744873\n",
            "New min loss = tensor([0.7505], grad_fn=<AddBackward0>)\n",
            "epoch 778, loss with penalty 0.7505393624305725\n",
            "epoch 779, loss with penalty 0.7502174973487854\n",
            "epoch 780, loss with penalty 0.7498956918716431\n",
            "epoch 781, loss with penalty 0.7495743036270142\n",
            "New min loss = tensor([0.7493], grad_fn=<AddBackward0>)\n",
            "epoch 782, loss with penalty 0.7492538094520569\n",
            "epoch 783, loss with penalty 0.748934268951416\n",
            "epoch 784, loss with penalty 0.7486139535903931\n",
            "epoch 785, loss with penalty 0.7482947111129761\n",
            "New min loss = tensor([0.7480], grad_fn=<AddBackward0>)\n",
            "epoch 786, loss with penalty 0.7479762434959412\n",
            "epoch 787, loss with penalty 0.7476584911346436\n",
            "epoch 788, loss with penalty 0.7473410964012146\n",
            "epoch 789, loss with penalty 0.7470241785049438\n",
            "New min loss = tensor([0.7467], grad_fn=<AddBackward0>)\n",
            "epoch 790, loss with penalty 0.7467074394226074\n",
            "epoch 791, loss with penalty 0.7463920712471008\n",
            "epoch 792, loss with penalty 0.7460761070251465\n",
            "epoch 793, loss with penalty 0.7457611560821533\n",
            "New min loss = tensor([0.7454], grad_fn=<AddBackward0>)\n",
            "epoch 794, loss with penalty 0.7454473376274109\n",
            "epoch 795, loss with penalty 0.7451323866844177\n",
            "epoch 796, loss with penalty 0.7448186874389648\n",
            "epoch 797, loss with penalty 0.7445056438446045\n",
            "New min loss = tensor([0.7442], grad_fn=<AddBackward0>)\n",
            "epoch 798, loss with penalty 0.7441930174827576\n",
            "epoch 799, loss with penalty 0.7438811659812927\n",
            "epoch 800, loss with penalty 0.7435694336891174\n",
            "epoch 801, loss with penalty 0.7432591915130615\n",
            "New min loss = tensor([0.7429], grad_fn=<AddBackward0>)\n",
            "epoch 802, loss with penalty 0.7429478764533997\n",
            "epoch 803, loss with penalty 0.7426378726959229\n",
            "epoch 804, loss with penalty 0.7423276901245117\n",
            "epoch 805, loss with penalty 0.7420196533203125\n",
            "New min loss = tensor([0.7417], grad_fn=<AddBackward0>)\n",
            "epoch 806, loss with penalty 0.7417106032371521\n",
            "epoch 807, loss with penalty 0.741402268409729\n",
            "epoch 808, loss with penalty 0.7410945892333984\n",
            "epoch 809, loss with penalty 0.7407873868942261\n",
            "New min loss = tensor([0.7405], grad_fn=<AddBackward0>)\n",
            "epoch 810, loss with penalty 0.7404809594154358\n",
            "epoch 811, loss with penalty 0.7401742935180664\n",
            "epoch 812, loss with penalty 0.7398685812950134\n",
            "epoch 813, loss with penalty 0.7395640015602112\n",
            "New min loss = tensor([0.7393], grad_fn=<AddBackward0>)\n",
            "epoch 814, loss with penalty 0.7392592430114746\n",
            "epoch 815, loss with penalty 0.7389539480209351\n",
            "epoch 816, loss with penalty 0.7386504411697388\n",
            "epoch 817, loss with penalty 0.7383471727371216\n",
            "New min loss = tensor([0.7380], grad_fn=<AddBackward0>)\n",
            "epoch 818, loss with penalty 0.738044023513794\n",
            "epoch 819, loss with penalty 0.7377415895462036\n",
            "epoch 820, loss with penalty 0.7374395728111267\n",
            "epoch 821, loss with penalty 0.737138032913208\n",
            "New min loss = tensor([0.7368], grad_fn=<AddBackward0>)\n",
            "epoch 822, loss with penalty 0.7368373274803162\n",
            "epoch 823, loss with penalty 0.7365368008613586\n",
            "epoch 824, loss with penalty 0.7362368106842041\n",
            "epoch 825, loss with penalty 0.7359367609024048\n",
            "New min loss = tensor([0.7356], grad_fn=<AddBackward0>)\n",
            "epoch 826, loss with penalty 0.7356380224227905\n",
            "epoch 827, loss with penalty 0.7353394031524658\n",
            "epoch 828, loss with penalty 0.7350414991378784\n",
            "epoch 829, loss with penalty 0.7347440123558044\n",
            "New min loss = tensor([0.7344], grad_fn=<AddBackward0>)\n",
            "epoch 830, loss with penalty 0.7344463467597961\n",
            "epoch 831, loss with penalty 0.7341498136520386\n",
            "epoch 832, loss with penalty 0.7338535189628601\n",
            "epoch 833, loss with penalty 0.7335575222969055\n",
            "New min loss = tensor([0.7333], grad_fn=<AddBackward0>)\n",
            "epoch 834, loss with penalty 0.733262300491333\n",
            "epoch 835, loss with penalty 0.7329678535461426\n",
            "epoch 836, loss with penalty 0.7326736450195312\n",
            "epoch 837, loss with penalty 0.7323795557022095\n",
            "New min loss = tensor([0.7321], grad_fn=<AddBackward0>)\n",
            "epoch 838, loss with penalty 0.7320863008499146\n",
            "epoch 839, loss with penalty 0.7317934632301331\n",
            "epoch 840, loss with penalty 0.7315007448196411\n",
            "epoch 841, loss with penalty 0.7312092185020447\n",
            "New min loss = tensor([0.7309], grad_fn=<AddBackward0>)\n",
            "epoch 842, loss with penalty 0.7309169769287109\n",
            "epoch 843, loss with penalty 0.7306264042854309\n",
            "epoch 844, loss with penalty 0.7303355932235718\n",
            "epoch 845, loss with penalty 0.7300454378128052\n",
            "New min loss = tensor([0.7298], grad_fn=<AddBackward0>)\n",
            "epoch 846, loss with penalty 0.7297556400299072\n",
            "epoch 847, loss with penalty 0.7294667959213257\n",
            "epoch 848, loss with penalty 0.7291778922080994\n",
            "epoch 849, loss with penalty 0.7288890480995178\n",
            "New min loss = tensor([0.7286], grad_fn=<AddBackward0>)\n",
            "epoch 850, loss with penalty 0.7286013960838318\n",
            "epoch 851, loss with penalty 0.7283130288124084\n",
            "epoch 852, loss with penalty 0.7280270457267761\n",
            "epoch 853, loss with penalty 0.7277407050132751\n",
            "New min loss = tensor([0.7275], grad_fn=<AddBackward0>)\n",
            "epoch 854, loss with penalty 0.7274543642997742\n",
            "epoch 855, loss with penalty 0.7271683812141418\n",
            "epoch 856, loss with penalty 0.7268837094306946\n",
            "epoch 857, loss with penalty 0.7265987396240234\n",
            "New min loss = tensor([0.7263], grad_fn=<AddBackward0>)\n",
            "epoch 858, loss with penalty 0.7263148427009583\n",
            "epoch 859, loss with penalty 0.7260314226150513\n",
            "epoch 860, loss with penalty 0.7257468700408936\n",
            "epoch 861, loss with penalty 0.7254646420478821\n",
            "New min loss = tensor([0.7252], grad_fn=<AddBackward0>)\n",
            "epoch 862, loss with penalty 0.7251825332641602\n",
            "epoch 863, loss with penalty 0.7249006032943726\n",
            "epoch 864, loss with penalty 0.7246190309524536\n",
            "epoch 865, loss with penalty 0.7243384718894958\n",
            "New min loss = tensor([0.7241], grad_fn=<AddBackward0>)\n",
            "epoch 866, loss with penalty 0.7240577340126038\n",
            "epoch 867, loss with penalty 0.7237774729728699\n",
            "epoch 868, loss with penalty 0.723497748374939\n",
            "epoch 869, loss with penalty 0.7232185006141663\n",
            "New min loss = tensor([0.7229], grad_fn=<AddBackward0>)\n",
            "epoch 870, loss with penalty 0.7229396104812622\n",
            "epoch 871, loss with penalty 0.7226611375808716\n",
            "epoch 872, loss with penalty 0.7223836183547974\n",
            "epoch 873, loss with penalty 0.7221056818962097\n",
            "New min loss = tensor([0.7218], grad_fn=<AddBackward0>)\n",
            "epoch 874, loss with penalty 0.7218287587165833\n",
            "epoch 875, loss with penalty 0.7215521931648254\n",
            "epoch 876, loss with penalty 0.7212761044502258\n",
            "epoch 877, loss with penalty 0.7210015058517456\n",
            "New min loss = tensor([0.7207], grad_fn=<AddBackward0>)\n",
            "epoch 878, loss with penalty 0.720724880695343\n",
            "epoch 879, loss with penalty 0.7204509973526001\n",
            "epoch 880, loss with penalty 0.7201762199401855\n",
            "epoch 881, loss with penalty 0.7199025750160217\n",
            "New min loss = tensor([0.7196], grad_fn=<AddBackward0>)\n",
            "epoch 882, loss with penalty 0.7196290493011475\n",
            "epoch 883, loss with penalty 0.7193556427955627\n",
            "epoch 884, loss with penalty 0.7190837264060974\n",
            "epoch 885, loss with penalty 0.7188115119934082\n",
            "New min loss = tensor([0.7185], grad_fn=<AddBackward0>)\n",
            "epoch 886, loss with penalty 0.7185401320457458\n",
            "epoch 887, loss with penalty 0.7182683944702148\n",
            "epoch 888, loss with penalty 0.7179970145225525\n",
            "epoch 889, loss with penalty 0.7177271246910095\n",
            "New min loss = tensor([0.7175], grad_fn=<AddBackward0>)\n",
            "epoch 890, loss with penalty 0.7174574732780457\n",
            "epoch 891, loss with penalty 0.7171879410743713\n",
            "epoch 892, loss with penalty 0.7169186472892761\n",
            "epoch 893, loss with penalty 0.7166499495506287\n",
            "New min loss = tensor([0.7164], grad_fn=<AddBackward0>)\n",
            "epoch 894, loss with penalty 0.7163821458816528\n",
            "epoch 895, loss with penalty 0.7161144018173218\n",
            "epoch 896, loss with penalty 0.715846836566925\n",
            "epoch 897, loss with penalty 0.7155797481536865\n",
            "New min loss = tensor([0.7153], grad_fn=<AddBackward0>)\n",
            "epoch 898, loss with penalty 0.7153134942054749\n",
            "epoch 899, loss with penalty 0.7150468826293945\n",
            "epoch 900, loss with penalty 0.7147815227508545\n",
            "epoch 901, loss with penalty 0.7145164608955383\n",
            "New min loss = tensor([0.7143], grad_fn=<AddBackward0>)\n",
            "epoch 902, loss with penalty 0.7142515182495117\n",
            "epoch 903, loss with penalty 0.7139868140220642\n",
            "epoch 904, loss with penalty 0.713723361492157\n",
            "epoch 905, loss with penalty 0.7134593725204468\n",
            "New min loss = tensor([0.7132], grad_fn=<AddBackward0>)\n",
            "epoch 906, loss with penalty 0.7131959795951843\n",
            "epoch 907, loss with penalty 0.7129341959953308\n",
            "epoch 908, loss with penalty 0.7126715779304504\n",
            "epoch 909, loss with penalty 0.7124102711677551\n",
            "New min loss = tensor([0.7121], grad_fn=<AddBackward0>)\n",
            "epoch 910, loss with penalty 0.7121486663818359\n",
            "epoch 911, loss with penalty 0.7118867635726929\n",
            "epoch 912, loss with penalty 0.7116265892982483\n",
            "epoch 913, loss with penalty 0.7113665342330933\n",
            "New min loss = tensor([0.7111], grad_fn=<AddBackward0>)\n",
            "epoch 914, loss with penalty 0.7111068964004517\n",
            "epoch 915, loss with penalty 0.7108473777770996\n",
            "epoch 916, loss with penalty 0.7105884552001953\n",
            "epoch 917, loss with penalty 0.7103303670883179\n",
            "New min loss = tensor([0.7101], grad_fn=<AddBackward0>)\n",
            "epoch 918, loss with penalty 0.7100713849067688\n",
            "epoch 919, loss with penalty 0.7098147869110107\n",
            "epoch 920, loss with penalty 0.7095571756362915\n",
            "epoch 921, loss with penalty 0.709300696849823\n",
            "New min loss = tensor([0.7090], grad_fn=<AddBackward0>)\n",
            "epoch 922, loss with penalty 0.7090445756912231\n",
            "epoch 923, loss with penalty 0.7087885141372681\n",
            "epoch 924, loss with penalty 0.7085327506065369\n",
            "epoch 925, loss with penalty 0.7082772850990295\n",
            "New min loss = tensor([0.7080], grad_fn=<AddBackward0>)\n",
            "epoch 926, loss with penalty 0.7080228924751282\n",
            "epoch 927, loss with penalty 0.7077686190605164\n",
            "epoch 928, loss with penalty 0.7075145840644836\n",
            "epoch 929, loss with penalty 0.7072609663009644\n",
            "New min loss = tensor([0.7070], grad_fn=<AddBackward0>)\n",
            "epoch 930, loss with penalty 0.7070082426071167\n",
            "epoch 931, loss with penalty 0.7067556381225586\n",
            "epoch 932, loss with penalty 0.7065034508705139\n",
            "epoch 933, loss with penalty 0.7062512636184692\n",
            "New min loss = tensor([0.7060], grad_fn=<AddBackward0>)\n",
            "epoch 934, loss with penalty 0.7060004472732544\n",
            "epoch 935, loss with penalty 0.7057494521141052\n",
            "epoch 936, loss with penalty 0.7054988741874695\n",
            "epoch 937, loss with penalty 0.7052484154701233\n",
            "New min loss = tensor([0.7050], grad_fn=<AddBackward0>)\n",
            "epoch 938, loss with penalty 0.7049989104270935\n",
            "epoch 939, loss with penalty 0.7047487497329712\n",
            "epoch 940, loss with penalty 0.7045001983642578\n",
            "epoch 941, loss with penalty 0.7042516469955444\n",
            "epoch 942, loss with penalty 0.7040034532546997\n",
            "New min loss = tensor([0.7038], grad_fn=<AddBackward0>)\n",
            "epoch 943, loss with penalty 0.703755795955658\n",
            "epoch 944, loss with penalty 0.7035084962844849\n",
            "epoch 945, loss with penalty 0.7032609581947327\n",
            "epoch 946, loss with penalty 0.7030142545700073\n",
            "epoch 947, loss with penalty 0.7027682662010193\n",
            "New min loss = tensor([0.7025], grad_fn=<AddBackward0>)\n",
            "epoch 948, loss with penalty 0.7025223970413208\n",
            "epoch 949, loss with penalty 0.7022777795791626\n",
            "epoch 950, loss with penalty 0.7020328640937805\n",
            "epoch 951, loss with penalty 0.7017881274223328\n",
            "epoch 952, loss with penalty 0.701543927192688\n",
            "New min loss = tensor([0.7013], grad_fn=<AddBackward0>)\n",
            "epoch 953, loss with penalty 0.7013002634048462\n",
            "epoch 954, loss with penalty 0.7010566592216492\n",
            "epoch 955, loss with penalty 0.700813889503479\n",
            "epoch 956, loss with penalty 0.700570821762085\n",
            "epoch 957, loss with penalty 0.7003290057182312\n",
            "New min loss = tensor([0.7001], grad_fn=<AddBackward0>)\n",
            "epoch 958, loss with penalty 0.7000875473022461\n",
            "epoch 959, loss with penalty 0.6998457908630371\n",
            "epoch 960, loss with penalty 0.6996043920516968\n",
            "epoch 961, loss with penalty 0.6993644833564758\n",
            "epoch 962, loss with penalty 0.6991235017776489\n",
            "New min loss = tensor([0.6989], grad_fn=<AddBackward0>)\n",
            "epoch 963, loss with penalty 0.698883593082428\n",
            "epoch 964, loss with penalty 0.6986445188522339\n",
            "epoch 965, loss with penalty 0.6984057426452637\n",
            "epoch 966, loss with penalty 0.6981666684150696\n",
            "epoch 967, loss with penalty 0.6979287266731262\n",
            "New min loss = tensor([0.6977], grad_fn=<AddBackward0>)\n",
            "epoch 968, loss with penalty 0.6976903676986694\n",
            "epoch 969, loss with penalty 0.6974527835845947\n",
            "epoch 970, loss with penalty 0.6972159743309021\n",
            "epoch 971, loss with penalty 0.6969794631004333\n",
            "epoch 972, loss with penalty 0.6967434883117676\n",
            "New min loss = tensor([0.6965], grad_fn=<AddBackward0>)\n",
            "epoch 973, loss with penalty 0.696506917476654\n",
            "epoch 974, loss with penalty 0.6962718963623047\n",
            "epoch 975, loss with penalty 0.6960365772247314\n",
            "epoch 976, loss with penalty 0.6958014369010925\n",
            "epoch 977, loss with penalty 0.6955669522285461\n",
            "New min loss = tensor([0.6953], grad_fn=<AddBackward0>)\n",
            "epoch 978, loss with penalty 0.6953328251838684\n",
            "epoch 979, loss with penalty 0.6950990557670593\n",
            "epoch 980, loss with penalty 0.6948658227920532\n",
            "epoch 981, loss with penalty 0.6946328282356262\n",
            "epoch 982, loss with penalty 0.6943997740745544\n",
            "New min loss = tensor([0.6942], grad_fn=<AddBackward0>)\n",
            "epoch 983, loss with penalty 0.6941678524017334\n",
            "epoch 984, loss with penalty 0.6939361095428467\n",
            "epoch 985, loss with penalty 0.6937042474746704\n",
            "epoch 986, loss with penalty 0.6934734582901001\n",
            "epoch 987, loss with penalty 0.6932423710823059\n",
            "New min loss = tensor([0.6930], grad_fn=<AddBackward0>)\n",
            "epoch 988, loss with penalty 0.6930122375488281\n",
            "epoch 989, loss with penalty 0.6927822232246399\n",
            "epoch 990, loss with penalty 0.6925530433654785\n",
            "epoch 991, loss with penalty 0.6923235058784485\n",
            "epoch 992, loss with penalty 0.6920942664146423\n",
            "New min loss = tensor([0.6919], grad_fn=<AddBackward0>)\n",
            "epoch 993, loss with penalty 0.6918657422065735\n",
            "epoch 994, loss with penalty 0.6916375160217285\n",
            "epoch 995, loss with penalty 0.6914099454879761\n",
            "epoch 996, loss with penalty 0.6911823153495789\n",
            "epoch 997, loss with penalty 0.6909558773040771\n",
            "New min loss = tensor([0.6907], grad_fn=<AddBackward0>)\n",
            "epoch 998, loss with penalty 0.6907286643981934\n",
            "epoch 999, loss with penalty 0.690501868724823\n",
            "epoch 1000, loss with penalty 0.6902763247489929\n",
            "epoch 1001, loss with penalty 0.690050482749939\n",
            "epoch 1002, loss with penalty 0.6898249387741089\n",
            "New min loss = tensor([0.6896], grad_fn=<AddBackward0>)\n",
            "epoch 1003, loss with penalty 0.6896008253097534\n",
            "epoch 1004, loss with penalty 0.6893760561943054\n",
            "epoch 1005, loss with penalty 0.6891517639160156\n",
            "epoch 1006, loss with penalty 0.6889274716377258\n",
            "epoch 1007, loss with penalty 0.688704788684845\n",
            "New min loss = tensor([0.6885], grad_fn=<AddBackward0>)\n",
            "epoch 1008, loss with penalty 0.6884813904762268\n",
            "epoch 1009, loss with penalty 0.688258707523346\n",
            "epoch 1010, loss with penalty 0.6880369186401367\n",
            "epoch 1011, loss with penalty 0.6878153681755066\n",
            "epoch 1012, loss with penalty 0.6875931620597839\n",
            "New min loss = tensor([0.6874], grad_fn=<AddBackward0>)\n",
            "epoch 1013, loss with penalty 0.6873716711997986\n",
            "epoch 1014, loss with penalty 0.6871516108512878\n",
            "epoch 1015, loss with penalty 0.6869307160377502\n",
            "epoch 1016, loss with penalty 0.6867102980613708\n",
            "epoch 1017, loss with penalty 0.686491072177887\n",
            "New min loss = tensor([0.6863], grad_fn=<AddBackward0>)\n",
            "epoch 1018, loss with penalty 0.6862713098526001\n",
            "epoch 1019, loss with penalty 0.6860518455505371\n",
            "epoch 1020, loss with penalty 0.6858327984809875\n",
            "epoch 1021, loss with penalty 0.6856151819229126\n",
            "epoch 1022, loss with penalty 0.685397207736969\n",
            "New min loss = tensor([0.6852], grad_fn=<AddBackward0>)\n",
            "epoch 1023, loss with penalty 0.6851795315742493\n",
            "epoch 1024, loss with penalty 0.6849618554115295\n",
            "epoch 1025, loss with penalty 0.6847442388534546\n",
            "epoch 1026, loss with penalty 0.6845282316207886\n",
            "epoch 1027, loss with penalty 0.68431156873703\n",
            "New min loss = tensor([0.6841], grad_fn=<AddBackward0>)\n",
            "epoch 1028, loss with penalty 0.6840956211090088\n",
            "epoch 1029, loss with penalty 0.6838802695274353\n",
            "epoch 1030, loss with penalty 0.6836652159690857\n",
            "epoch 1031, loss with penalty 0.6834497451782227\n",
            "epoch 1032, loss with penalty 0.6832351684570312\n",
            "New min loss = tensor([0.6830], grad_fn=<AddBackward0>)\n",
            "epoch 1033, loss with penalty 0.683021068572998\n",
            "epoch 1034, loss with penalty 0.6828072667121887\n",
            "epoch 1035, loss with penalty 0.6825940608978271\n",
            "epoch 1036, loss with penalty 0.6823801398277283\n",
            "epoch 1037, loss with penalty 0.6821673512458801\n",
            "New min loss = tensor([0.6820], grad_fn=<AddBackward0>)\n",
            "epoch 1038, loss with penalty 0.6819551587104797\n",
            "epoch 1039, loss with penalty 0.6817428469657898\n",
            "epoch 1040, loss with penalty 0.6815308928489685\n",
            "epoch 1041, loss with penalty 0.6813195943832397\n",
            "epoch 1042, loss with penalty 0.6811087727546692\n",
            "New min loss = tensor([0.6809], grad_fn=<AddBackward0>)\n",
            "epoch 1043, loss with penalty 0.6808979511260986\n",
            "epoch 1044, loss with penalty 0.6806876063346863\n",
            "epoch 1045, loss with penalty 0.6804772615432739\n",
            "epoch 1046, loss with penalty 0.6802672743797302\n",
            "epoch 1047, loss with penalty 0.6800577640533447\n",
            "New min loss = tensor([0.6798], grad_fn=<AddBackward0>)\n",
            "epoch 1048, loss with penalty 0.6798490285873413\n",
            "epoch 1049, loss with penalty 0.6796396970748901\n",
            "epoch 1050, loss with penalty 0.679431140422821\n",
            "epoch 1051, loss with penalty 0.6792235374450684\n",
            "epoch 1052, loss with penalty 0.6790159344673157\n",
            "New min loss = tensor([0.6788], grad_fn=<AddBackward0>)\n",
            "epoch 1053, loss with penalty 0.6788082718849182\n",
            "epoch 1054, loss with penalty 0.6786009073257446\n",
            "epoch 1055, loss with penalty 0.678394079208374\n",
            "epoch 1056, loss with penalty 0.6781877279281616\n",
            "epoch 1057, loss with penalty 0.677981972694397\n",
            "New min loss = tensor([0.6778], grad_fn=<AddBackward0>)\n",
            "epoch 1058, loss with penalty 0.6777759194374084\n",
            "epoch 1059, loss with penalty 0.6775702238082886\n",
            "epoch 1060, loss with penalty 0.6773653030395508\n",
            "epoch 1061, loss with penalty 0.6771606802940369\n",
            "epoch 1062, loss with penalty 0.6769561767578125\n",
            "New min loss = tensor([0.6768], grad_fn=<AddBackward0>)\n",
            "epoch 1063, loss with penalty 0.676752507686615\n",
            "epoch 1064, loss with penalty 0.6765481233596802\n",
            "epoch 1065, loss with penalty 0.6763440370559692\n",
            "epoch 1066, loss with penalty 0.6761413812637329\n",
            "epoch 1067, loss with penalty 0.6759386658668518\n",
            "New min loss = tensor([0.6757], grad_fn=<AddBackward0>)\n",
            "epoch 1068, loss with penalty 0.6757360100746155\n",
            "epoch 1069, loss with penalty 0.6755334734916687\n",
            "epoch 1070, loss with penalty 0.6753316521644592\n",
            "epoch 1071, loss with penalty 0.6751304268836975\n",
            "epoch 1072, loss with penalty 0.6749287843704224\n",
            "New min loss = tensor([0.6747], grad_fn=<AddBackward0>)\n",
            "epoch 1073, loss with penalty 0.674728274345398\n",
            "epoch 1074, loss with penalty 0.6745281219482422\n",
            "epoch 1075, loss with penalty 0.6743271350860596\n",
            "epoch 1076, loss with penalty 0.6741271615028381\n",
            "epoch 1077, loss with penalty 0.6739272475242615\n",
            "New min loss = tensor([0.6737], grad_fn=<AddBackward0>)\n",
            "epoch 1078, loss with penalty 0.6737282276153564\n",
            "epoch 1079, loss with penalty 0.6735292077064514\n",
            "epoch 1080, loss with penalty 0.6733306050300598\n",
            "epoch 1081, loss with penalty 0.6731324195861816\n",
            "epoch 1082, loss with penalty 0.6729342937469482\n",
            "epoch 1083, loss with penalty 0.6727362275123596\n",
            "New min loss = tensor([0.6725], grad_fn=<AddBackward0>)\n",
            "epoch 1084, loss with penalty 0.6725388169288635\n",
            "epoch 1085, loss with penalty 0.6723418831825256\n",
            "epoch 1086, loss with penalty 0.6721451282501221\n",
            "epoch 1087, loss with penalty 0.6719486713409424\n",
            "epoch 1088, loss with penalty 0.6717526316642761\n",
            "epoch 1089, loss with penalty 0.6715565323829651\n",
            "New min loss = tensor([0.6714], grad_fn=<AddBackward0>)\n",
            "epoch 1090, loss with penalty 0.6713613867759705\n",
            "epoch 1091, loss with penalty 0.6711661219596863\n",
            "epoch 1092, loss with penalty 0.6709718108177185\n",
            "epoch 1093, loss with penalty 0.6707769632339478\n",
            "epoch 1094, loss with penalty 0.6705828309059143\n",
            "epoch 1095, loss with penalty 0.6703886389732361\n",
            "New min loss = tensor([0.6702], grad_fn=<AddBackward0>)\n",
            "epoch 1096, loss with penalty 0.670194685459137\n",
            "epoch 1097, loss with penalty 0.6700019240379333\n",
            "epoch 1098, loss with penalty 0.669808566570282\n",
            "epoch 1099, loss with penalty 0.6696162223815918\n",
            "epoch 1100, loss with penalty 0.6694241762161255\n",
            "epoch 1101, loss with penalty 0.6692312955856323\n",
            "New min loss = tensor([0.6690], grad_fn=<AddBackward0>)\n",
            "epoch 1102, loss with penalty 0.6690396070480347\n",
            "epoch 1103, loss with penalty 0.6688480377197266\n",
            "epoch 1104, loss with penalty 0.668657124042511\n",
            "epoch 1105, loss with penalty 0.6684662699699402\n",
            "epoch 1106, loss with penalty 0.6682755351066589\n",
            "epoch 1107, loss with penalty 0.6680851578712463\n",
            "New min loss = tensor([0.6679], grad_fn=<AddBackward0>)\n",
            "epoch 1108, loss with penalty 0.6678955554962158\n",
            "epoch 1109, loss with penalty 0.6677055954933167\n",
            "epoch 1110, loss with penalty 0.6675167679786682\n",
            "epoch 1111, loss with penalty 0.667326807975769\n",
            "epoch 1112, loss with penalty 0.6671386361122131\n",
            "epoch 1113, loss with penalty 0.6669497489929199\n",
            "New min loss = tensor([0.6668], grad_fn=<AddBackward0>)\n",
            "epoch 1114, loss with penalty 0.6667618751525879\n",
            "epoch 1115, loss with penalty 0.6665737628936768\n",
            "epoch 1116, loss with penalty 0.6663862466812134\n",
            "epoch 1117, loss with penalty 0.6661986708641052\n",
            "epoch 1118, loss with penalty 0.6660114526748657\n",
            "epoch 1119, loss with penalty 0.6658247113227844\n",
            "New min loss = tensor([0.6656], grad_fn=<AddBackward0>)\n",
            "epoch 1120, loss with penalty 0.6656385660171509\n",
            "epoch 1121, loss with penalty 0.6654521226882935\n",
            "epoch 1122, loss with penalty 0.6652663946151733\n",
            "epoch 1123, loss with penalty 0.665080726146698\n",
            "epoch 1124, loss with penalty 0.6648949980735779\n",
            "epoch 1125, loss with penalty 0.6647107005119324\n",
            "New min loss = tensor([0.6645], grad_fn=<AddBackward0>)\n",
            "epoch 1126, loss with penalty 0.66452556848526\n",
            "epoch 1127, loss with penalty 0.6643413305282593\n",
            "epoch 1128, loss with penalty 0.6641573905944824\n",
            "epoch 1129, loss with penalty 0.663973331451416\n",
            "epoch 1130, loss with penalty 0.6637901067733765\n",
            "epoch 1131, loss with penalty 0.6636061072349548\n",
            "New min loss = tensor([0.6634], grad_fn=<AddBackward0>)\n",
            "epoch 1132, loss with penalty 0.6634231209754944\n",
            "epoch 1133, loss with penalty 0.6632407307624817\n",
            "epoch 1134, loss with penalty 0.6630584597587585\n",
            "epoch 1135, loss with penalty 0.6628761887550354\n",
            "epoch 1136, loss with penalty 0.6626943349838257\n",
            "epoch 1137, loss with penalty 0.662513256072998\n",
            "New min loss = tensor([0.6623], grad_fn=<AddBackward0>)\n",
            "epoch 1138, loss with penalty 0.6623316407203674\n",
            "epoch 1139, loss with penalty 0.6621502041816711\n",
            "epoch 1140, loss with penalty 0.6619696617126465\n",
            "epoch 1141, loss with penalty 0.661789059638977\n",
            "epoch 1142, loss with penalty 0.6616092324256897\n",
            "epoch 1143, loss with penalty 0.6614294648170471\n",
            "New min loss = tensor([0.6612], grad_fn=<AddBackward0>)\n",
            "epoch 1144, loss with penalty 0.6612496376037598\n",
            "epoch 1145, loss with penalty 0.6610701680183411\n",
            "epoch 1146, loss with penalty 0.6608908176422119\n",
            "epoch 1147, loss with penalty 0.6607121825218201\n",
            "epoch 1148, loss with penalty 0.6605340242385864\n",
            "epoch 1149, loss with penalty 0.6603553891181946\n",
            "New min loss = tensor([0.6602], grad_fn=<AddBackward0>)\n",
            "epoch 1150, loss with penalty 0.6601769328117371\n",
            "epoch 1151, loss with penalty 0.6599996089935303\n",
            "epoch 1152, loss with penalty 0.6598224639892578\n",
            "epoch 1153, loss with penalty 0.6596454381942749\n",
            "epoch 1154, loss with penalty 0.6594688892364502\n",
            "epoch 1155, loss with penalty 0.6592923402786255\n",
            "New min loss = tensor([0.6591], grad_fn=<AddBackward0>)\n",
            "epoch 1156, loss with penalty 0.6591157913208008\n",
            "epoch 1157, loss with penalty 0.6589395403862\n",
            "epoch 1158, loss with penalty 0.6587640047073364\n",
            "epoch 1159, loss with penalty 0.6585880517959595\n",
            "epoch 1160, loss with penalty 0.6584129929542542\n",
            "epoch 1161, loss with penalty 0.6582382321357727\n",
            "New min loss = tensor([0.6581], grad_fn=<AddBackward0>)\n",
            "epoch 1162, loss with penalty 0.658063530921936\n",
            "epoch 1163, loss with penalty 0.6578889489173889\n",
            "epoch 1164, loss with penalty 0.6577149629592896\n",
            "epoch 1165, loss with penalty 0.6575412750244141\n",
            "epoch 1166, loss with penalty 0.6573678255081177\n",
            "epoch 1167, loss with penalty 0.657194197177887\n",
            "New min loss = tensor([0.6570], grad_fn=<AddBackward0>)\n",
            "epoch 1168, loss with penalty 0.6570212244987488\n",
            "epoch 1169, loss with penalty 0.6568477153778076\n",
            "epoch 1170, loss with penalty 0.6566756367683411\n",
            "epoch 1171, loss with penalty 0.6565032601356506\n",
            "epoch 1172, loss with penalty 0.6563318371772766\n",
            "epoch 1173, loss with penalty 0.6561598181724548\n",
            "New min loss = tensor([0.6560], grad_fn=<AddBackward0>)\n",
            "epoch 1174, loss with penalty 0.6559882760047913\n",
            "epoch 1175, loss with penalty 0.655816912651062\n",
            "epoch 1176, loss with penalty 0.6556460857391357\n",
            "epoch 1177, loss with penalty 0.6554750204086304\n",
            "epoch 1178, loss with penalty 0.6553045511245728\n",
            "epoch 1179, loss with penalty 0.6551348567008972\n",
            "New min loss = tensor([0.6550], grad_fn=<AddBackward0>)\n",
            "epoch 1180, loss with penalty 0.6549644470214844\n",
            "epoch 1181, loss with penalty 0.6547945737838745\n",
            "epoch 1182, loss with penalty 0.6546251177787781\n",
            "epoch 1183, loss with penalty 0.6544561982154846\n",
            "epoch 1184, loss with penalty 0.6542879343032837\n",
            "epoch 1185, loss with penalty 0.6541190147399902\n",
            "New min loss = tensor([0.6540], grad_fn=<AddBackward0>)\n",
            "epoch 1186, loss with penalty 0.6539502143859863\n",
            "epoch 1187, loss with penalty 0.653782844543457\n",
            "epoch 1188, loss with penalty 0.6536147594451904\n",
            "epoch 1189, loss with penalty 0.6534473896026611\n",
            "epoch 1190, loss with penalty 0.6532800197601318\n",
            "epoch 1191, loss with penalty 0.6531129479408264\n",
            "New min loss = tensor([0.6529], grad_fn=<AddBackward0>)\n",
            "epoch 1192, loss with penalty 0.652946412563324\n",
            "epoch 1193, loss with penalty 0.6527795195579529\n",
            "epoch 1194, loss with penalty 0.6526131629943848\n",
            "epoch 1195, loss with penalty 0.6524469256401062\n",
            "epoch 1196, loss with penalty 0.6522812843322754\n",
            "epoch 1197, loss with penalty 0.6521163582801819\n",
            "epoch 1198, loss with penalty 0.6519506573677063\n",
            "New min loss = tensor([0.6518], grad_fn=<AddBackward0>)\n",
            "epoch 1199, loss with penalty 0.6517859697341919\n",
            "epoch 1200, loss with penalty 0.6516209244728088\n",
            "epoch 1201, loss with penalty 0.6514562964439392\n",
            "epoch 1202, loss with penalty 0.6512919664382935\n",
            "epoch 1203, loss with penalty 0.6511278748512268\n",
            "epoch 1204, loss with penalty 0.6509647965431213\n",
            "epoch 1205, loss with penalty 0.6508011817932129\n",
            "New min loss = tensor([0.6506], grad_fn=<AddBackward0>)\n",
            "epoch 1206, loss with penalty 0.6506378650665283\n",
            "epoch 1207, loss with penalty 0.6504742503166199\n",
            "epoch 1208, loss with penalty 0.6503121852874756\n",
            "epoch 1209, loss with penalty 0.6501495838165283\n",
            "epoch 1210, loss with penalty 0.6499871015548706\n",
            "epoch 1211, loss with penalty 0.6498251557350159\n",
            "epoch 1212, loss with penalty 0.6496625542640686\n",
            "New min loss = tensor([0.6495], grad_fn=<AddBackward0>)\n",
            "epoch 1213, loss with penalty 0.6495013236999512\n",
            "epoch 1214, loss with penalty 0.6493400931358337\n",
            "epoch 1215, loss with penalty 0.6491793394088745\n",
            "epoch 1216, loss with penalty 0.6490183472633362\n",
            "epoch 1217, loss with penalty 0.6488578915596008\n",
            "epoch 1218, loss with penalty 0.6486969590187073\n",
            "epoch 1219, loss with penalty 0.6485372185707092\n",
            "New min loss = tensor([0.6484], grad_fn=<AddBackward0>)\n",
            "epoch 1220, loss with penalty 0.6483771204948425\n",
            "epoch 1221, loss with penalty 0.6482175588607788\n",
            "epoch 1222, loss with penalty 0.6480581760406494\n",
            "epoch 1223, loss with penalty 0.6478989124298096\n",
            "epoch 1224, loss with penalty 0.6477394700050354\n",
            "epoch 1225, loss with penalty 0.6475809812545776\n",
            "epoch 1226, loss with penalty 0.6474226713180542\n",
            "New min loss = tensor([0.6473], grad_fn=<AddBackward0>)\n",
            "epoch 1227, loss with penalty 0.6472648978233337\n",
            "epoch 1228, loss with penalty 0.6471067667007446\n",
            "epoch 1229, loss with penalty 0.6469491124153137\n",
            "epoch 1230, loss with penalty 0.6467912197113037\n",
            "epoch 1231, loss with penalty 0.6466342806816101\n",
            "epoch 1232, loss with penalty 0.6464771032333374\n",
            "epoch 1233, loss with penalty 0.6463205814361572\n",
            "New min loss = tensor([0.6462], grad_fn=<AddBackward0>)\n",
            "epoch 1234, loss with penalty 0.6461634039878845\n",
            "epoch 1235, loss with penalty 0.6460078358650208\n",
            "epoch 1236, loss with penalty 0.6458510756492615\n",
            "epoch 1237, loss with penalty 0.6456952691078186\n",
            "epoch 1238, loss with penalty 0.6455399990081787\n",
            "epoch 1239, loss with penalty 0.6453844308853149\n",
            "epoch 1240, loss with penalty 0.6452296376228333\n",
            "New min loss = tensor([0.6451], grad_fn=<AddBackward0>)\n",
            "epoch 1241, loss with penalty 0.6450740098953247\n",
            "epoch 1242, loss with penalty 0.6449196338653564\n",
            "epoch 1243, loss with penalty 0.6447652578353882\n",
            "epoch 1244, loss with penalty 0.6446104645729065\n",
            "epoch 1245, loss with penalty 0.6444569826126099\n",
            "epoch 1246, loss with penalty 0.644303023815155\n",
            "epoch 1247, loss with penalty 0.6441501975059509\n",
            "New min loss = tensor([0.6440], grad_fn=<AddBackward0>)\n",
            "epoch 1248, loss with penalty 0.6439964771270752\n",
            "epoch 1249, loss with penalty 0.6438432931900024\n",
            "epoch 1250, loss with penalty 0.6436909437179565\n",
            "epoch 1251, loss with penalty 0.6435378193855286\n",
            "epoch 1252, loss with penalty 0.6433857083320618\n",
            "epoch 1253, loss with penalty 0.6432335376739502\n",
            "epoch 1254, loss with penalty 0.6430816650390625\n",
            "New min loss = tensor([0.6429], grad_fn=<AddBackward0>)\n",
            "epoch 1255, loss with penalty 0.6429304480552673\n",
            "epoch 1256, loss with penalty 0.642778754234314\n",
            "epoch 1257, loss with penalty 0.6426275968551636\n",
            "epoch 1258, loss with penalty 0.642476499080658\n",
            "epoch 1259, loss with penalty 0.6423254609107971\n",
            "epoch 1260, loss with penalty 0.6421752572059631\n",
            "epoch 1261, loss with penalty 0.6420247554779053\n",
            "New min loss = tensor([0.6419], grad_fn=<AddBackward0>)\n",
            "epoch 1262, loss with penalty 0.6418742537498474\n",
            "epoch 1263, loss with penalty 0.6417243480682373\n",
            "epoch 1264, loss with penalty 0.6415743827819824\n",
            "epoch 1265, loss with penalty 0.6414252519607544\n",
            "epoch 1266, loss with penalty 0.6412760019302368\n",
            "epoch 1267, loss with penalty 0.6411270499229431\n",
            "epoch 1268, loss with penalty 0.6409777402877808\n",
            "New min loss = tensor([0.6408], grad_fn=<AddBackward0>)\n",
            "epoch 1269, loss with penalty 0.6408292055130005\n",
            "epoch 1270, loss with penalty 0.6406807899475098\n",
            "epoch 1271, loss with penalty 0.640532910823822\n",
            "epoch 1272, loss with penalty 0.640384316444397\n",
            "epoch 1273, loss with penalty 0.6402367949485779\n",
            "epoch 1274, loss with penalty 0.6400892734527588\n",
            "epoch 1275, loss with penalty 0.6399422883987427\n",
            "New min loss = tensor([0.6398], grad_fn=<AddBackward0>)\n",
            "epoch 1276, loss with penalty 0.6397952437400818\n",
            "epoch 1277, loss with penalty 0.6396481990814209\n",
            "epoch 1278, loss with penalty 0.6395015716552734\n",
            "epoch 1279, loss with penalty 0.6393547654151917\n",
            "epoch 1280, loss with penalty 0.6392084956169128\n",
            "epoch 1281, loss with penalty 0.6390622854232788\n",
            "epoch 1282, loss with penalty 0.6389166116714478\n",
            "New min loss = tensor([0.6388], grad_fn=<AddBackward0>)\n",
            "epoch 1283, loss with penalty 0.6387712955474854\n",
            "epoch 1284, loss with penalty 0.6386258602142334\n",
            "epoch 1285, loss with penalty 0.6384811401367188\n",
            "epoch 1286, loss with penalty 0.6383357644081116\n",
            "epoch 1287, loss with penalty 0.6381910443305969\n",
            "epoch 1288, loss with penalty 0.6380465030670166\n",
            "epoch 1289, loss with penalty 0.6379018425941467\n",
            "New min loss = tensor([0.6378], grad_fn=<AddBackward0>)\n",
            "epoch 1290, loss with penalty 0.6377579569816589\n",
            "epoch 1291, loss with penalty 0.637613832950592\n",
            "epoch 1292, loss with penalty 0.6374701857566833\n",
            "epoch 1293, loss with penalty 0.6373264193534851\n",
            "epoch 1294, loss with penalty 0.637183427810669\n",
            "epoch 1295, loss with penalty 0.6370405554771423\n",
            "epoch 1296, loss with penalty 0.6368975043296814\n",
            "New min loss = tensor([0.6368], grad_fn=<AddBackward0>)\n",
            "epoch 1297, loss with penalty 0.6367544531822205\n",
            "epoch 1298, loss with penalty 0.6366131901741028\n",
            "epoch 1299, loss with penalty 0.636470377445221\n",
            "epoch 1300, loss with penalty 0.6363282203674316\n",
            "epoch 1301, loss with penalty 0.6361867189407349\n",
            "epoch 1302, loss with penalty 0.6360452771186829\n",
            "epoch 1303, loss with penalty 0.6359033584594727\n",
            "epoch 1304, loss with penalty 0.6357627511024475\n",
            "New min loss = tensor([0.6356], grad_fn=<AddBackward0>)\n",
            "epoch 1305, loss with penalty 0.6356217861175537\n",
            "epoch 1306, loss with penalty 0.6354812383651733\n",
            "epoch 1307, loss with penalty 0.6353405714035034\n",
            "epoch 1308, loss with penalty 0.6352002620697021\n",
            "epoch 1309, loss with penalty 0.6350600123405457\n",
            "epoch 1310, loss with penalty 0.6349196434020996\n",
            "epoch 1311, loss with penalty 0.6347802877426147\n",
            "epoch 1312, loss with penalty 0.6346409916877747\n",
            "New min loss = tensor([0.6345], grad_fn=<AddBackward0>)\n",
            "epoch 1313, loss with penalty 0.6345012187957764\n",
            "epoch 1314, loss with penalty 0.6343621611595154\n",
            "epoch 1315, loss with penalty 0.634223461151123\n",
            "epoch 1316, loss with penalty 0.6340841054916382\n",
            "epoch 1317, loss with penalty 0.6339452862739563\n",
            "epoch 1318, loss with penalty 0.6338074207305908\n",
            "epoch 1319, loss with penalty 0.6336691975593567\n",
            "epoch 1320, loss with penalty 0.6335310339927673\n",
            "New min loss = tensor([0.6334], grad_fn=<AddBackward0>)\n",
            "epoch 1321, loss with penalty 0.6333935856819153\n",
            "epoch 1322, loss with penalty 0.6332557201385498\n",
            "epoch 1323, loss with penalty 0.6331183910369873\n",
            "epoch 1324, loss with penalty 0.6329810619354248\n",
            "epoch 1325, loss with penalty 0.6328443884849548\n",
            "epoch 1326, loss with penalty 0.6327078938484192\n",
            "epoch 1327, loss with penalty 0.6325706839561462\n",
            "epoch 1328, loss with penalty 0.6324344277381897\n",
            "New min loss = tensor([0.6323], grad_fn=<AddBackward0>)\n",
            "epoch 1329, loss with penalty 0.6322986483573914\n",
            "epoch 1330, loss with penalty 0.6321626305580139\n",
            "epoch 1331, loss with penalty 0.6320269107818604\n",
            "epoch 1332, loss with penalty 0.6318910121917725\n",
            "epoch 1333, loss with penalty 0.6317556500434875\n",
            "epoch 1334, loss with penalty 0.6316202282905579\n",
            "epoch 1335, loss with penalty 0.6314855813980103\n",
            "epoch 1336, loss with penalty 0.6313509345054626\n",
            "New min loss = tensor([0.6312], grad_fn=<AddBackward0>)\n",
            "epoch 1337, loss with penalty 0.6312161684036255\n",
            "epoch 1338, loss with penalty 0.6310813426971436\n",
            "epoch 1339, loss with penalty 0.6309470534324646\n",
            "epoch 1340, loss with penalty 0.6308130621910095\n",
            "epoch 1341, loss with penalty 0.6306792497634888\n",
            "epoch 1342, loss with penalty 0.6305462718009949\n",
            "epoch 1343, loss with penalty 0.6304121613502502\n",
            "epoch 1344, loss with penalty 0.6302788853645325\n",
            "New min loss = tensor([0.6301], grad_fn=<AddBackward0>)\n",
            "epoch 1345, loss with penalty 0.6301462650299072\n",
            "epoch 1346, loss with penalty 0.630013644695282\n",
            "epoch 1347, loss with penalty 0.6298802495002747\n",
            "epoch 1348, loss with penalty 0.6297478675842285\n",
            "epoch 1349, loss with penalty 0.6296156048774719\n",
            "epoch 1350, loss with penalty 0.6294838190078735\n",
            "epoch 1351, loss with penalty 0.629351794719696\n",
            "epoch 1352, loss with penalty 0.6292198896408081\n",
            "New min loss = tensor([0.6291], grad_fn=<AddBackward0>)\n",
            "epoch 1353, loss with penalty 0.6290881633758545\n",
            "epoch 1354, loss with penalty 0.6289573311805725\n",
            "epoch 1355, loss with penalty 0.6288257241249084\n",
            "epoch 1356, loss with penalty 0.6286947727203369\n",
            "epoch 1357, loss with penalty 0.6285641193389893\n",
            "epoch 1358, loss with penalty 0.6284332871437073\n",
            "epoch 1359, loss with penalty 0.6283031702041626\n",
            "epoch 1360, loss with penalty 0.6281728148460388\n",
            "New min loss = tensor([0.6280], grad_fn=<AddBackward0>)\n",
            "epoch 1361, loss with penalty 0.6280428767204285\n",
            "epoch 1362, loss with penalty 0.6279125809669495\n",
            "epoch 1363, loss with penalty 0.6277831196784973\n",
            "epoch 1364, loss with penalty 0.6276536583900452\n",
            "epoch 1365, loss with penalty 0.6275243163108826\n",
            "epoch 1366, loss with penalty 0.6273950338363647\n",
            "epoch 1367, loss with penalty 0.6272662878036499\n",
            "epoch 1368, loss with penalty 0.6271376013755798\n",
            "New min loss = tensor([0.6270], grad_fn=<AddBackward0>)\n",
            "epoch 1369, loss with penalty 0.6270082592964172\n",
            "epoch 1370, loss with penalty 0.6268802285194397\n",
            "epoch 1371, loss with penalty 0.6267519593238831\n",
            "epoch 1372, loss with penalty 0.6266240477561951\n",
            "epoch 1373, loss with penalty 0.6264962553977966\n",
            "epoch 1374, loss with penalty 0.6263680458068848\n",
            "epoch 1375, loss with penalty 0.6262406706809998\n",
            "epoch 1376, loss with penalty 0.6261130571365356\n",
            "New min loss = tensor([0.6260], grad_fn=<AddBackward0>)\n",
            "epoch 1377, loss with penalty 0.6259861588478088\n",
            "epoch 1378, loss with penalty 0.6258590221405029\n",
            "epoch 1379, loss with penalty 0.6257319450378418\n",
            "epoch 1380, loss with penalty 0.6256057024002075\n",
            "epoch 1381, loss with penalty 0.6254790425300598\n",
            "epoch 1382, loss with penalty 0.6253529787063599\n",
            "epoch 1383, loss with penalty 0.6252262592315674\n",
            "epoch 1384, loss with penalty 0.6251007914543152\n",
            "New min loss = tensor([0.6250], grad_fn=<AddBackward0>)\n",
            "epoch 1385, loss with penalty 0.6249749660491943\n",
            "epoch 1386, loss with penalty 0.6248494386672974\n",
            "epoch 1387, loss with penalty 0.6247240304946899\n",
            "epoch 1388, loss with penalty 0.6245993971824646\n",
            "epoch 1389, loss with penalty 0.6244741082191467\n",
            "epoch 1390, loss with penalty 0.6243489384651184\n",
            "epoch 1391, loss with penalty 0.6242244243621826\n",
            "epoch 1392, loss with penalty 0.6241003274917603\n",
            "epoch 1393, loss with penalty 0.6239760518074036\n",
            "New min loss = tensor([0.6239], grad_fn=<AddBackward0>)\n",
            "epoch 1394, loss with penalty 0.6238515377044678\n",
            "epoch 1395, loss with penalty 0.6237272024154663\n",
            "epoch 1396, loss with penalty 0.6236037015914917\n",
            "epoch 1397, loss with penalty 0.6234799027442932\n",
            "epoch 1398, loss with penalty 0.6233564019203186\n",
            "epoch 1399, loss with penalty 0.6232331395149231\n",
            "epoch 1400, loss with penalty 0.6231104731559753\n",
            "epoch 1401, loss with penalty 0.6229871511459351\n",
            "epoch 1402, loss with penalty 0.6228644251823425\n",
            "New min loss = tensor([0.6227], grad_fn=<AddBackward0>)\n",
            "epoch 1403, loss with penalty 0.62274169921875\n",
            "epoch 1404, loss with penalty 0.6226191520690918\n",
            "epoch 1405, loss with penalty 0.62249755859375\n",
            "epoch 1406, loss with penalty 0.622374951839447\n",
            "epoch 1407, loss with penalty 0.6222531795501709\n",
            "epoch 1408, loss with penalty 0.6221312880516052\n",
            "epoch 1409, loss with penalty 0.6220093965530396\n",
            "epoch 1410, loss with penalty 0.6218884587287903\n",
            "epoch 1411, loss with penalty 0.6217672228813171\n",
            "New min loss = tensor([0.6216], grad_fn=<AddBackward0>)\n",
            "epoch 1412, loss with penalty 0.6216462254524231\n",
            "epoch 1413, loss with penalty 0.6215251088142395\n",
            "epoch 1414, loss with penalty 0.6214043498039246\n",
            "epoch 1415, loss with penalty 0.621284008026123\n",
            "epoch 1416, loss with penalty 0.6211637258529663\n",
            "epoch 1417, loss with penalty 0.62104332447052\n",
            "epoch 1418, loss with penalty 0.6209233403205872\n",
            "epoch 1419, loss with penalty 0.6208033561706543\n",
            "epoch 1420, loss with penalty 0.6206830143928528\n",
            "New min loss = tensor([0.6206], grad_fn=<AddBackward0>)\n",
            "epoch 1421, loss with penalty 0.6205635070800781\n",
            "epoch 1422, loss with penalty 0.6204441785812378\n",
            "epoch 1423, loss with penalty 0.6203254461288452\n",
            "epoch 1424, loss with penalty 0.6202059984207153\n",
            "epoch 1425, loss with penalty 0.6200876235961914\n",
            "epoch 1426, loss with penalty 0.6199691295623779\n",
            "epoch 1427, loss with penalty 0.6198502779006958\n",
            "epoch 1428, loss with penalty 0.6197323799133301\n",
            "epoch 1429, loss with penalty 0.6196138262748718\n",
            "New min loss = tensor([0.6195], grad_fn=<AddBackward0>)\n",
            "epoch 1430, loss with penalty 0.6194957494735718\n",
            "epoch 1431, loss with penalty 0.6193776726722717\n",
            "epoch 1432, loss with penalty 0.6192602515220642\n",
            "epoch 1433, loss with penalty 0.6191424131393433\n",
            "epoch 1434, loss with penalty 0.6190255880355835\n",
            "epoch 1435, loss with penalty 0.6189076900482178\n",
            "epoch 1436, loss with penalty 0.6187911033630371\n",
            "epoch 1437, loss with penalty 0.6186738014221191\n",
            "epoch 1438, loss with penalty 0.6185575127601624\n",
            "New min loss = tensor([0.6184], grad_fn=<AddBackward0>)\n",
            "epoch 1439, loss with penalty 0.6184406280517578\n",
            "epoch 1440, loss with penalty 0.6183245778083801\n",
            "epoch 1441, loss with penalty 0.6182076930999756\n",
            "epoch 1442, loss with penalty 0.6180917620658875\n",
            "epoch 1443, loss with penalty 0.6179760694503784\n",
            "epoch 1444, loss with penalty 0.6178598999977112\n",
            "epoch 1445, loss with penalty 0.6177441477775574\n",
            "epoch 1446, loss with penalty 0.6176288723945618\n",
            "epoch 1447, loss with penalty 0.6175134181976318\n",
            "New min loss = tensor([0.6174], grad_fn=<AddBackward0>)\n",
            "epoch 1448, loss with penalty 0.6173986792564392\n",
            "epoch 1449, loss with penalty 0.617283821105957\n",
            "epoch 1450, loss with penalty 0.6171684265136719\n",
            "epoch 1451, loss with penalty 0.6170539259910583\n",
            "epoch 1452, loss with penalty 0.6169394254684448\n",
            "epoch 1453, loss with penalty 0.6168248057365417\n",
            "epoch 1454, loss with penalty 0.6167107820510864\n",
            "epoch 1455, loss with penalty 0.6165964603424072\n",
            "epoch 1456, loss with penalty 0.6164832711219788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrbOIPSxK5LM"
      },
      "source": [
        "### The Finite Difference Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkg6Zfo7u8Fq"
      },
      "outputs": [],
      "source": [
        "def finite_diff_method(lower_bound, upper_bound, N_points, given_fn):\n",
        "    discrete_points = np.linspace(lower_bound, upper_bound, N_points)\n",
        "    h = discrete_points[1]-discrete_points[0]\n",
        "    c = np.sqrt(1/h)\n",
        "\n",
        "    H = np.zeros((N_points,N_points))\n",
        "    # Question: Why H has dimension (N-2) x (N-2)?\n",
        "    V = np.zeros((N_points,N_points))\n",
        "\n",
        "    for i in range (N_points):\n",
        "        for j in range (N_points):\n",
        "            if i == j:\n",
        "                H[i][j] = -2\n",
        "            elif abs(i-j) == 1:\n",
        "                H[i][j] = 1\n",
        "    for i in range (N_points):\n",
        "        for j in range (N_points):\n",
        "            if i == j:\n",
        "                V[i][j] = given_fn.forward(discrete_points[i])\n",
        "\n",
        "    A = -H/(h**2) + V\n",
        "    \n",
        "    eig_val, eig_vec = np.linalg.eig(A)\n",
        "    sorted_id_eig_val = np.argsort(eig_val)\n",
        "    # Get the indices that would sort eig_val\n",
        "    z = sorted_id_eig_val[0:1]\n",
        "    # eig_val[z] will return sorted first [T] value of eigenvalues.\n",
        "    energies = eig_val[z]/eig_val[z][0]\n",
        "\n",
        "    ground_state = c * eig_vec[:,z[0]]\n",
        "\n",
        "    return (eig_val[z], ground_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIGR6Gohu8Fr"
      },
      "outputs": [],
      "source": [
        "def plot_finite_diff_sol(eig_val, ground_state, discrete_points, nn_model=None):\n",
        "    plt.figure(figsize=(12,10))\n",
        "    for i in range(len(eig_val)):\n",
        "        y = []\n",
        "        y = np.append(y, (-1) * -ground_state)\n",
        "\n",
        "        plt.plot(discrete_points,y,lw=3, label=\"{} \".format(i))\n",
        "        plt.xlabel('x', size=14)\n",
        "        plt.ylabel('$\\psi$(x)',size=14)\n",
        "    if nn_model != None:\n",
        "        model_output = model_on_interval(nn_model, DISCRETE_POINTS)\n",
        "        plt.plot(discrete_points, model_output)\n",
        "    plt.legend()\n",
        "#     plt.title('normalized wavefunctions for a harmonic oscillator using finite difference method',size=14)\n",
        "    plt.show()\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx7UHtNBK5LO"
      },
      "outputs": [],
      "source": [
        "eig_val, ground_state = finite_diff_method(L_BOUND, U_BOUND, N_POINTS, pt_fn)\n",
        "plot_finite_diff_sol(eig_val, ground_state, DISCRETE_POINTS, nn_model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Data-running version.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}