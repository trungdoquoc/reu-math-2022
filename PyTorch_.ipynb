{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpofWgGancyb"
      },
      "source": [
        "The input of the NN is 1D. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e-NAhHF3ncye"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.functional import normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn.functional import normalize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "INPUT_SIZE = 1\n",
        "OUTPUT_SIZE = 1\n",
        "\n",
        "LAMBDA_PEN = 1000000\n",
        "LOWER_BOUND = -6\n",
        "UPPER_BOUND = 6\n",
        "N_POINTS = 1001\n",
        "\n",
        "\n",
        "def quad_fn(a, b, c, x):\n",
        "    return a*x**2 + b*x + c\n",
        "\n",
        "def x_square(x: torch.tensor) -> torch.Tensor:\n",
        "    return x**2\n",
        "\n",
        "# DEFINE GIVEN FUNCTION V(x): ax^2 + bx + c\n",
        "given_fn = x_square"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EstQ_ILqQeOt",
        "outputId": "02bae447-f7ca-42f9-8e47-bbb8d3121c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1001, 1) (1001, 1)\n"
          ]
        }
      ],
      "source": [
        "# CREATING DATASET:\n",
        "x_values = [i for i in np.linspace(LOWER_BOUND, UPPER_BOUND, N_POINTS)]\n",
        "y_values = [given_fn(i) for i in x_values]\n",
        "\n",
        "x_train = np.array(x_values, dtype=np.float32).reshape(-1, 1)\n",
        "y_train = np.array(y_values, dtype=np.float32).reshape(-1, 1)\n",
        "\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SOtFslmCncyg"
      },
      "outputs": [],
      "source": [
        "# CUSTOM LOSS FUNCTION:\n",
        "# def EpsilonLoss(nn.Module):\n",
        "\n",
        "def epsilon_Loss(v_x, model_u, lower_bound, upper_bound, n_points):\n",
        "    \"\"\"\n",
        "    GOAL: Epsilon function evaluated at u using discretized estimation\n",
        "    minimizing Epsilon(u) = \n",
        "    \n",
        "    ARGS: \n",
        "    n_points (int): number of discretized points on the interval [-L, L]\n",
        "    e.g.: -(L)|---|---|---|---|(L) interval has n_points = 5\n",
        "\n",
        "    v_x (torch.Tensor): function instance\n",
        "    model_u (torch.Tensor): model output\n",
        "    \"\"\"\n",
        "    sum = 0\n",
        "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
        "    h = discrete_points[1] - discrete_points[0]\n",
        "    for i in discrete_points:\n",
        "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
        "        u_xi = model_u(x_i)\n",
        "\n",
        "        u_prime = model_u.u_prime_2(x_i)\n",
        "        \n",
        "        v_xi = v_x(x_i)\n",
        "        t = torch.square(u_prime) + v_xi*(u_xi**2)\n",
        "        sum += t\n",
        "    return 0.5*h*sum\n",
        "\n",
        "def epsilon_Loss_penalty(v_x, model_u, lambda_pen,\n",
        "                         lower_bound, upper_bound, n_points):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    sum = 0\n",
        "    pen = 0\n",
        "\n",
        "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
        "    h = discrete_points[1] - discrete_points[0]\n",
        "    for i in discrete_points:\n",
        "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
        "        u_xi = model_u(x_i)\n",
        "\n",
        "        u_prime = model_u.u_prime_2(x_i)\n",
        "        \n",
        "        v_xi = v_x(x_i)\n",
        "        t = torch.square(u_prime) + v_xi*(u_xi**2)\n",
        "        sum += t\n",
        "    epsilon_fn = 0.5*h*sum\n",
        "    \n",
        "    temp = 0\n",
        "    for i in discrete_points:\n",
        "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
        "        temp += torch.square(model_u(x_i))\n",
        "    \n",
        "    pen = lambda_pen * torch.square((temp*h-1))\n",
        "    return epsilon_fn + pen \n",
        "\n",
        "# NORMALIZE MODEL u(x) OUTPUT:\n",
        "def normalize_u(model_u, lower_bound, upper_bound, n_points):\n",
        "    \"\"\"\n",
        "    Normalize model.output weight by: \n",
        "    model.output *= c\n",
        "    where,\n",
        "    scalar c = 1/denom\n",
        "    \"\"\"\n",
        "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
        "    h = discrete_points[1] - discrete_points[0]\n",
        "    s = 0\n",
        "    for i in discrete_points:\n",
        "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
        "        s += model_u(x_i)**2\n",
        "    denom = math.sqrt(h) * torch.sqrt(s)\n",
        "    return 1/denom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "06QWhlkpQ5Eh"
      },
      "outputs": [],
      "source": [
        "# CREATING MODEL CLASS\n",
        "class Nonlinear(nn.Module):\n",
        "    def __init__(self, n):\n",
        "        # One hidden layer with n nodes\n",
        "        super().__init__()\n",
        "        self.hidden = nn.Linear(1, n)\n",
        "        self.output = nn.Linear(n, 1)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.target_fn = given_fn\n",
        "\n",
        "    def forward(self, x, use_tanh_fn = False, activation_on_output = False):\n",
        "        if use_tanh_fn == True:\n",
        "            x = self.hidden(x)\n",
        "            x = self.tanh(x)\n",
        "            x = self.output(x)\n",
        "            \n",
        "        else:\n",
        "            x = self.hidden(x)\n",
        "            x = self.sigmoid(x)\n",
        "            x = self.output(x)\n",
        "            \n",
        "        if activation_on_output == False:\n",
        "            return x\n",
        "        else:\n",
        "            if use_tanh_fn == True:\n",
        "                return self.tanh(x)\n",
        "            else:       \n",
        "                return self.sigmoid(x)\n",
        "        # output is a linear combination of the hidden layers because \n",
        "        # we perform regression ???\n",
        "        return x\n",
        "\n",
        "    def u_prime_2(self, input):\n",
        "        i_clone = input.clone()\n",
        "        clone = copy.deepcopy(self)\n",
        "        res = clone(i_clone).backward()\n",
        "        return i_clone.clone()\n",
        "\n",
        "    def u_prime(self, input):\n",
        "        \"\"\"\n",
        "        NN with 1 hidden node layer is of the form:\n",
        "        u(x) = SUM_i_to_N(a_i * sigmoid(w.x + b))\n",
        "\n",
        "        where\n",
        "        a_i is the corresponding weight of self.output layerq\n",
        "        w is self.hidden.weight vector\n",
        "        b is self.hidden.bias vector\n",
        "        sigmoid(w.x + b) is the sigmoid-activated hidden vector\n",
        "\n",
        "        Formula of u'(x) (for 1 hidden layer NN):\n",
        "        u'(x) = SUM_i_to_N(w_i*a_i*sigmoid'(w_i*x+b))\n",
        "        Note: sigmoid'(w_i*x +b) = sigmoid(w_i*x+b)*(1-sigmoid(w_i*x+b))\n",
        "        \"\"\"\n",
        "        a_i = self.output.weight.data\n",
        "        w_i = torch.transpose(self.hidden.weight.data, 0, 1)\n",
        "        wi_ai = w_i * a_i\n",
        "\n",
        "        hid_layer = self.hidden(input)\n",
        "        hid_layer_T = torch.reshape(hid_layer, (list(hid_layer.shape)[0], 1))\n",
        "        m = hid_layer_T * (1-hid_layer_T)\n",
        "\n",
        "        return wi_ai @ m\n",
        "    \n",
        "# TRANING MODEL\n",
        "    def train_network_with_penalty(self, num_epochs, v_x, optimizer, lambda_pen,\n",
        "                                    lower_bound, upper_bound, n_points):\n",
        "        # For plotting loss value over epochs:\n",
        "        x_epochs = []\n",
        "        y_loss = []\n",
        "        \n",
        "        # stopping criterion:\n",
        "        stop_counter = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            optimizer.zero_grad()\n",
        "            loss = epsilon_Loss_penalty(v_x, self, lambda_pen,\n",
        "                                        lower_bound, upper_bound, n_points)\n",
        "            y_loss.append(loss)\n",
        "            x_epochs.append(epoch)\n",
        "            #check if need to stop training:\n",
        "            if epoch > 0 and stop_counter >= 5:\n",
        "                c = normalize_u(self, lower_bound, upper_bound, n_points)\n",
        "                self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
        "                self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
        "                print(\"c value = \" + str(c))\n",
        "                print(\"LOSS VALUE = \" \n",
        "                      + str(epsilon_Loss(v_x, self, lower_bound, upper_bound, n_points)))\n",
        "                break\n",
        "            elif epoch > 0 and stop_counter < 5:\n",
        "                if torch.abs(y_loss[epoch-1]-loss) <= 1e-5:\n",
        "                    stop_counter += 1\n",
        "                else:\n",
        "                    stop_counter = 0\n",
        "\n",
        "            print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
        "            loss.backward()\n",
        "            # Calculate the derivative(loss) w.r.t the model's parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        return (x_epochs, y_loss)\n",
        "\n",
        "    def train_network(self, num_epochs, v_x, optimizer,\n",
        "                                    lower_bound, upper_bound, n_points):\n",
        "        # For plotting loss value over epochs:\n",
        "        x_epochs = []\n",
        "        y_loss = []\n",
        "        \n",
        "        # stopping criterion:\n",
        "        stop_counter = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            if epoch > 0 and epoch % 50 == 0:\n",
        "                c = normalize_u(self, lower_bound, upper_bound, n_points)\n",
        "                print(\"Pre normalize: \")\n",
        "                print(self.output.weight.data)\n",
        "                print(self.output.bias.data)\n",
        "                print(\"After normalize: \")\n",
        "                self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
        "                self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
        "                print(self.output.weight.data)\n",
        "                print(self.output.bias.data)\n",
        "                print(\"c value = \" + str(c))\n",
        "            optimizer.zero_grad()\n",
        "            loss = epsilon_Loss(v_x, self,\n",
        "                                lower_bound, upper_bound, n_points)\n",
        "            y_loss.append(loss)\n",
        "            x_epochs.append(epoch)\n",
        "            #check if need to stop training:\n",
        "            if epoch > 0 and stop_counter >= 5:\n",
        "                print(\"LOSS VALUE = \" \n",
        "                      + str(epsilon_Loss(v_x, self, lower_bound, upper_bound, n_points)))\n",
        "                break\n",
        "            elif epoch > 0 and stop_counter < 5:\n",
        "                if torch.abs(y_loss[epoch-1]-loss) <= 1e-5:\n",
        "                    stop_counter += 1\n",
        "                else:\n",
        "                    stop_counter = 0\n",
        "\n",
        "            print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return (x_epochs, y_loss)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = epsilon_Loss(given_fn, model, \n",
        "                            LOWER_BOUND, UPPER_BOUND, N_POINTS)\n",
        "l"
      ],
      "metadata": {
        "id": "2saw960759Td",
        "outputId": "15fd19c7-cccc-403f-dd6f-1b223c54c84d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([78.7160], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgx4K0yugVxX",
        "outputId": "ae40cf07-991b-46ce-e62b-da3c83c7990a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([78.7160], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ],
      "source": [
        "loss = epsilon_Loss_penalty(given_fn, model, LAMBDA_PEN, \n",
        "                            LOWER_BOUND, UPPER_BOUND, N_POINTS)\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hbBxk8_fncyl"
      },
      "outputs": [],
      "source": [
        "# DEFINE HYPER-PARAMETERS\n",
        "batch_size = 50\n",
        "learningRate = 0.05\n",
        "num_epochs = 2000\n",
        "# num_epochs = int(num_iters/(len(x_train)/batch_size))\n",
        "\n",
        "#INIT PARAMETERS: \n",
        "# v_x = given_fn\n",
        "l_b = -10\n",
        "u_b = 10\n",
        "n_points = 5\n",
        "\n",
        "#INIT MODEL\n",
        "model = Nonlinear(20)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "# INIT OPTIMIZER CLASS\n",
        "# What is an optimizer: \n",
        "# SGD:\n",
        "# SGD_optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
        "# Adam:\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), \n",
        "                                    lr=learningRate, \n",
        "                                    betas=(0.9, 0.999), \n",
        "                                    eps=1e-08, \n",
        "                                    weight_decay=0, \n",
        "                                    amsgrad=False)\n",
        "\n",
        "# INIT LOSS FUNCTION: MSE\n",
        "# criterion = epsilon_Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZbaJ3MsB08r",
        "outputId": "44329c00-cd1a-4980-874c-f1019eb10188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 92.66681671142578\n",
            "epoch 1, loss 73.67607116699219\n",
            "epoch 2, loss 74.71785736083984\n",
            "epoch 3, loss 80.13819885253906\n",
            "epoch 4, loss 79.50361633300781\n",
            "epoch 5, loss 75.77351379394531\n",
            "epoch 6, loss 72.82444763183594\n",
            "epoch 7, loss 72.34623718261719\n",
            "epoch 8, loss 73.6412353515625\n",
            "epoch 9, loss 74.9586181640625\n",
            "epoch 10, loss 75.2249526977539\n",
            "epoch 11, loss 74.42913818359375\n",
            "epoch 12, loss 73.28616333007812\n",
            "epoch 13, loss 72.47100067138672\n",
            "epoch 14, loss 72.23152923583984\n",
            "epoch 15, loss 72.4844741821289\n",
            "epoch 16, loss 72.92973327636719\n",
            "epoch 17, loss 73.23461151123047\n",
            "epoch 18, loss 73.25938415527344\n",
            "epoch 19, loss 73.02680206298828\n",
            "epoch 20, loss 72.66358184814453\n",
            "epoch 21, loss 72.35860443115234\n",
            "epoch 22, loss 72.22747802734375\n",
            "epoch 23, loss 72.27960205078125\n",
            "epoch 24, loss 72.4524917602539\n",
            "epoch 25, loss 72.61605834960938\n",
            "epoch 26, loss 72.67101287841797\n",
            "epoch 27, loss 72.60542297363281\n",
            "epoch 28, loss 72.45940399169922\n",
            "epoch 29, loss 72.31392669677734\n",
            "epoch 30, loss 72.23249053955078\n",
            "epoch 31, loss 72.22611999511719\n",
            "epoch 32, loss 72.2793197631836\n",
            "epoch 33, loss 72.34758758544922\n",
            "epoch 34, loss 72.3881607055664\n",
            "epoch 35, loss 72.3862075805664\n",
            "epoch 36, loss 72.34300994873047\n",
            "epoch 37, loss 72.28421020507812\n",
            "epoch 38, loss 72.2376480102539\n",
            "epoch 39, loss 72.21803283691406\n",
            "epoch 40, loss 72.22979736328125\n",
            "epoch 41, loss 72.25711059570312\n",
            "epoch 42, loss 72.28267669677734\n",
            "epoch 43, loss 72.29257202148438\n",
            "epoch 44, loss 72.28134155273438\n",
            "epoch 45, loss 72.2581558227539\n",
            "epoch 46, loss 72.23360443115234\n",
            "epoch 47, loss 72.21932983398438\n",
            "epoch 48, loss 72.21925354003906\n",
            "epoch 49, loss 72.22916412353516\n",
            "Pre normalize: \n",
            "tensor([[-0.0218,  0.0503, -0.0024,  0.0612, -0.0274, -0.0005,  0.0051,  0.0105,\n",
            "          0.1750, -0.1309, -0.0002,  0.1315, -0.1643, -0.0647,  0.0146,  0.1011,\n",
            "          0.1006, -0.0073, -0.0017,  0.0551]])\n",
            "tensor([-0.1232])\n",
            "After normalize: \n",
            "tensor([[-0.3573,  0.8250, -0.0398,  1.0025, -0.4489, -0.0084,  0.0829,  0.1718,\n",
            "          2.8679, -2.1448, -0.0037,  2.1543, -2.6929, -1.0600,  0.2387,  1.6567,\n",
            "          1.6478, -0.1197, -0.0285,  0.9022]])\n",
            "tensor([-2.0191])\n",
            "c value = tensor([16.3879], grad_fn=<MulBackward0>)\n",
            "epoch 50, loss 79.1587905883789\n",
            "epoch 51, loss 73.43922424316406\n",
            "epoch 52, loss 77.86131286621094\n",
            "epoch 53, loss 76.35604095458984\n",
            "epoch 54, loss 72.8687973022461\n",
            "epoch 55, loss 74.75116729736328\n",
            "epoch 56, loss 76.23934936523438\n",
            "epoch 57, loss 73.27356719970703\n",
            "epoch 58, loss 72.77972412109375\n",
            "epoch 59, loss 74.39002990722656\n",
            "epoch 60, loss 74.3657455444336\n",
            "epoch 61, loss 72.66740417480469\n",
            "epoch 62, loss 72.58282470703125\n",
            "epoch 63, loss 73.87308502197266\n",
            "epoch 64, loss 73.45443725585938\n",
            "epoch 65, loss 72.48023223876953\n",
            "epoch 66, loss 72.51851654052734\n",
            "epoch 67, loss 73.23468017578125\n",
            "epoch 68, loss 73.10978698730469\n",
            "epoch 69, loss 72.33888244628906\n",
            "epoch 70, loss 72.45506286621094\n",
            "epoch 71, loss 72.87711334228516\n",
            "epoch 72, loss 72.80455780029297\n",
            "epoch 73, loss 72.36666870117188\n",
            "epoch 74, loss 72.31515502929688\n",
            "epoch 75, loss 72.6696548461914\n",
            "epoch 76, loss 72.60542297363281\n",
            "epoch 77, loss 72.33023071289062\n",
            "epoch 78, loss 72.28351593017578\n",
            "epoch 79, loss 72.48463439941406\n",
            "epoch 80, loss 72.501953125\n",
            "epoch 81, loss 72.28658294677734\n",
            "epoch 82, loss 72.2644271850586\n",
            "epoch 83, loss 72.37879180908203\n",
            "epoch 84, loss 72.40751647949219\n",
            "epoch 85, loss 72.27457427978516\n",
            "epoch 86, loss 72.2376480102539\n",
            "epoch 87, loss 72.33089447021484\n",
            "epoch 88, loss 72.33882141113281\n",
            "epoch 89, loss 72.26593780517578\n",
            "epoch 90, loss 72.2293930053711\n",
            "epoch 91, loss 72.29376983642578\n",
            "epoch 92, loss 72.30268096923828\n",
            "epoch 93, loss 72.24889373779297\n",
            "epoch 94, loss 72.23163604736328\n",
            "epoch 95, loss 72.26701354980469\n",
            "epoch 96, loss 72.27820587158203\n",
            "epoch 97, loss 72.23643493652344\n",
            "epoch 98, loss 72.22941589355469\n",
            "epoch 99, loss 72.25138854980469\n",
            "Pre normalize: \n",
            "tensor([[-0.3219,  0.7875, -0.0100,  0.9643, -0.4219, -0.0052,  0.0804,  0.1322,\n",
            "          2.8326, -2.0515, -0.0046,  2.1699, -2.6389, -1.0131,  0.2349,  1.6366,\n",
            "          1.6624, -0.0954, -0.0106,  0.8756]])\n",
            "tensor([-1.9989])\n",
            "After normalize: \n",
            "tensor([[ -3.5968,   8.8002,  -0.1116,  10.7763,  -4.7151,  -0.0582,   0.8988,\n",
            "           1.4773,  31.6533, -22.9247,  -0.0519,  24.2479, -29.4892, -11.3214,\n",
            "           2.6255,  18.2891,  18.5771,  -1.0660,  -0.1188,   9.7843]])\n",
            "tensor([-22.3371])\n",
            "c value = tensor([11.1748], grad_fn=<MulBackward0>)\n",
            "epoch 100, loss 77.32325744628906\n",
            "epoch 101, loss 869.1807250976562\n",
            "epoch 102, loss 307.4857177734375\n",
            "epoch 103, loss 694.7946166992188\n",
            "epoch 104, loss 304.17852783203125\n",
            "epoch 105, loss 421.4009094238281\n",
            "epoch 106, loss 350.38818359375\n",
            "epoch 107, loss 165.40679931640625\n",
            "epoch 108, loss 361.4961853027344\n",
            "epoch 109, loss 365.28277587890625\n",
            "epoch 110, loss 117.59968566894531\n",
            "epoch 111, loss 121.29309844970703\n",
            "epoch 112, loss 286.2100524902344\n",
            "epoch 113, loss 231.27157592773438\n",
            "epoch 114, loss 119.90242767333984\n",
            "epoch 115, loss 143.53668212890625\n",
            "epoch 116, loss 165.03799438476562\n",
            "epoch 117, loss 127.3382568359375\n",
            "epoch 118, loss 132.4881591796875\n",
            "epoch 119, loss 162.1827850341797\n",
            "epoch 120, loss 127.01406860351562\n",
            "epoch 121, loss 77.01838684082031\n",
            "epoch 122, loss 98.7210922241211\n",
            "epoch 123, loss 139.65548706054688\n",
            "epoch 124, loss 120.57662200927734\n",
            "epoch 125, loss 82.76052856445312\n",
            "epoch 126, loss 85.47132873535156\n",
            "epoch 127, loss 102.24810028076172\n",
            "epoch 128, loss 97.24291229248047\n",
            "epoch 129, loss 88.79579162597656\n",
            "epoch 130, loss 93.01577758789062\n",
            "epoch 131, loss 91.39432525634766\n",
            "epoch 132, loss 78.6880111694336\n",
            "epoch 133, loss 77.2413101196289\n",
            "epoch 134, loss 89.38159942626953\n",
            "epoch 135, loss 91.02056884765625\n",
            "epoch 136, loss 78.57097625732422\n",
            "epoch 137, loss 73.05854797363281\n",
            "epoch 138, loss 79.52408599853516\n",
            "epoch 139, loss 82.86177062988281\n",
            "epoch 140, loss 78.70834350585938\n",
            "epoch 141, loss 76.44998168945312\n",
            "epoch 142, loss 77.71012115478516\n",
            "epoch 143, loss 76.42709350585938\n",
            "epoch 144, loss 74.1080551147461\n",
            "epoch 145, loss 75.8536148071289\n",
            "epoch 146, loss 78.18531036376953\n",
            "epoch 147, loss 75.84125518798828\n",
            "epoch 148, loss 72.53519439697266\n",
            "epoch 149, loss 73.456787109375\n",
            "Pre normalize: \n",
            "tensor([[-3.3703e+00,  8.7426e+00, -9.2199e-02,  1.0703e+01, -4.7164e+00,\n",
            "         -4.0234e-02,  8.3410e-01,  1.2667e+00,  3.1625e+01, -2.2870e+01,\n",
            "         -5.0812e-02,  2.4307e+01, -2.9276e+01, -1.1079e+01,  2.6603e+00,\n",
            "          1.8258e+01,  1.8634e+01, -9.5223e-01, -3.2334e-03,  9.7423e+00]])\n",
            "tensor([-22.2432])\n",
            "After normalize: \n",
            "tensor([[-4.4205e+00,  1.1467e+01, -1.2093e-01,  1.4038e+01, -6.1861e+00,\n",
            "         -5.2771e-02,  1.0940e+00,  1.6615e+00,  4.1480e+01, -2.9997e+01,\n",
            "         -6.6645e-02,  3.1881e+01, -3.8399e+01, -1.4532e+01,  3.4892e+00,\n",
            "          2.3947e+01,  2.4440e+01, -1.2489e+00, -4.2410e-03,  1.2778e+01]])\n",
            "tensor([-29.1744])\n",
            "c value = tensor([1.3116], grad_fn=<MulBackward0>)\n",
            "epoch 150, loss 78.3766098022461\n",
            "epoch 151, loss 74.69227600097656\n",
            "epoch 152, loss 74.3564224243164\n",
            "epoch 153, loss 75.37458038330078\n",
            "epoch 154, loss 73.40096282958984\n",
            "epoch 155, loss 75.03314971923828\n",
            "epoch 156, loss 75.03511810302734\n",
            "epoch 157, loss 72.43351745605469\n",
            "epoch 158, loss 73.90884399414062\n",
            "epoch 159, loss 74.47269439697266\n",
            "epoch 160, loss 72.98967742919922\n",
            "epoch 161, loss 73.55256652832031\n",
            "epoch 162, loss 73.2338638305664\n",
            "epoch 163, loss 72.82040405273438\n",
            "epoch 164, loss 73.75289154052734\n",
            "epoch 165, loss 72.95711517333984\n",
            "epoch 166, loss 72.45104217529297\n",
            "epoch 167, loss 73.26897430419922\n",
            "epoch 168, loss 72.88610076904297\n",
            "epoch 169, loss 72.6717300415039\n",
            "epoch 170, loss 72.88408660888672\n",
            "epoch 171, loss 72.49211120605469\n",
            "epoch 172, loss 72.74592590332031\n",
            "epoch 173, loss 72.85813903808594\n",
            "epoch 174, loss 72.37103271484375\n",
            "epoch 175, loss 72.57716369628906\n",
            "epoch 176, loss 72.67694091796875\n",
            "epoch 177, loss 72.4559326171875\n",
            "epoch 178, loss 72.5653076171875\n",
            "epoch 179, loss 72.44872283935547\n",
            "epoch 180, loss 72.41333770751953\n",
            "epoch 181, loss 72.5828628540039\n",
            "epoch 182, loss 72.40139770507812\n",
            "epoch 183, loss 72.36978912353516\n",
            "epoch 184, loss 72.48162078857422\n",
            "epoch 185, loss 72.39008331298828\n",
            "epoch 186, loss 72.41234588623047\n",
            "epoch 187, loss 72.3980712890625\n",
            "epoch 188, loss 72.33794403076172\n",
            "epoch 189, loss 72.42176818847656\n",
            "epoch 190, loss 72.37834930419922\n",
            "epoch 191, loss 72.32715606689453\n",
            "epoch 192, loss 72.38053131103516\n",
            "epoch 193, loss 72.35154724121094\n",
            "epoch 194, loss 72.35173034667969\n",
            "epoch 195, loss 72.35584259033203\n",
            "epoch 196, loss 72.3199234008789\n",
            "epoch 197, loss 72.35359954833984\n",
            "epoch 198, loss 72.34687042236328\n",
            "epoch 199, loss 72.31745910644531\n",
            "Pre normalize: \n",
            "tensor([[-4.4130e+00,  1.1469e+01, -1.1699e-01,  1.4039e+01, -6.1827e+00,\n",
            "         -4.9558e-02,  1.0961e+00,  1.6566e+00,  4.1483e+01, -2.9991e+01,\n",
            "         -6.2795e-02,  3.1882e+01, -3.8391e+01, -1.4522e+01,  3.4912e+00,\n",
            "          2.3950e+01,  2.4441e+01, -1.2425e+00, -1.3518e-03,  1.2780e+01]])\n",
            "tensor([-29.1710])\n",
            "After normalize: \n",
            "tensor([[-2.0945e+01,  5.4437e+01, -5.5527e-01,  6.6634e+01, -2.9345e+01,\n",
            "         -2.3522e-01,  5.2022e+00,  7.8627e+00,  1.9689e+02, -1.4234e+02,\n",
            "         -2.9804e-01,  1.5132e+02, -1.8221e+02, -6.8926e+01,  1.6570e+01,\n",
            "          1.1367e+02,  1.1600e+02, -5.8973e+00, -6.4159e-03,  6.0659e+01]])\n",
            "tensor([-138.4533])\n",
            "c value = tensor([4.7463], grad_fn=<MulBackward0>)\n",
            "epoch 200, loss 74.97361755371094\n",
            "epoch 201, loss 365.41595458984375\n",
            "epoch 202, loss 37433.38671875\n",
            "epoch 203, loss 3200.3583984375\n",
            "epoch 204, loss 36692.359375\n",
            "epoch 205, loss 11446.4873046875\n",
            "epoch 206, loss 3003.894287109375\n",
            "epoch 207, loss 20209.05078125\n",
            "epoch 208, loss 13774.1982421875\n",
            "epoch 209, loss 1585.2847900390625\n",
            "epoch 210, loss 5360.755859375\n",
            "epoch 211, loss 11902.01953125\n",
            "epoch 212, loss 7968.41162109375\n",
            "epoch 213, loss 1432.0079345703125\n",
            "epoch 214, loss 2045.1973876953125\n",
            "epoch 215, loss 6101.39599609375\n",
            "epoch 216, loss 5877.70263671875\n",
            "epoch 217, loss 2395.442138671875\n",
            "epoch 218, loss 800.4566040039062\n",
            "epoch 219, loss 2147.30078125\n",
            "epoch 220, loss 3633.891357421875\n",
            "epoch 221, loss 3097.553955078125\n",
            "epoch 222, loss 1338.7550048828125\n",
            "epoch 223, loss 508.86370849609375\n",
            "epoch 224, loss 1226.3052978515625\n",
            "epoch 225, loss 2074.05517578125\n",
            "epoch 226, loss 1809.804443359375\n",
            "epoch 227, loss 870.6257934570312\n",
            "epoch 228, loss 380.0768737792969\n",
            "epoch 229, loss 670.2261962890625\n",
            "epoch 230, loss 1159.8408203125\n",
            "epoch 231, loss 1196.95458984375\n",
            "epoch 232, loss 741.2745971679688\n",
            "epoch 233, loss 299.8001708984375\n",
            "epoch 234, loss 298.0882873535156\n",
            "epoch 235, loss 609.7553100585938\n",
            "epoch 236, loss 775.3644409179688\n",
            "epoch 237, loss 587.72998046875\n",
            "epoch 238, loss 280.7906188964844\n",
            "epoch 239, loss 174.00088500976562\n",
            "epoch 240, loss 308.8186340332031\n",
            "epoch 241, loss 468.904541015625\n",
            "epoch 242, loss 455.83636474609375\n",
            "epoch 243, loss 286.97039794921875\n",
            "epoch 244, loss 142.0303955078125\n",
            "epoch 245, loss 151.06065368652344\n",
            "epoch 246, loss 260.9676818847656\n",
            "epoch 247, loss 318.4025573730469\n",
            "epoch 248, loss 254.1416015625\n",
            "epoch 249, loss 145.14137268066406\n",
            "Pre normalize: \n",
            "tensor([[-2.0262e+01,  5.4492e+01, -3.8174e-01,  6.6675e+01, -2.9245e+01,\n",
            "         -4.0883e-02,  5.2409e+00,  7.4131e+00,  1.9696e+02, -1.4218e+02,\n",
            "         -1.6052e-01,  1.5146e+02, -1.8103e+02, -6.8013e+01,  1.6860e+01,\n",
            "          1.1375e+02,  1.1614e+02, -5.6115e+00, -3.7785e-02,  6.0722e+01]])\n",
            "tensor([-138.0197])\n",
            "After normalize: \n",
            "tensor([[-3.4260e+00,  9.2139e+00, -6.4547e-02,  1.1274e+01, -4.9450e+00,\n",
            "         -6.9127e-03,  8.8616e-01,  1.2535e+00,  3.3304e+01, -2.4041e+01,\n",
            "         -2.7142e-02,  2.5610e+01, -3.0609e+01, -1.1500e+01,  2.8508e+00,\n",
            "          1.9233e+01,  1.9639e+01, -9.4883e-01, -6.3889e-03,  1.0267e+01]])\n",
            "tensor([-23.3373])\n",
            "c value = tensor([0.1691], grad_fn=<MulBackward0>)\n",
            "epoch 250, loss 73.11116790771484\n",
            "epoch 251, loss 75.10692596435547\n",
            "epoch 252, loss 79.49646759033203\n",
            "epoch 253, loss 85.0823974609375\n",
            "epoch 254, loss 90.83756256103516\n",
            "epoch 255, loss 95.96514892578125\n",
            "epoch 256, loss 99.92564392089844\n",
            "epoch 257, loss 102.43123626708984\n",
            "epoch 258, loss 103.41643524169922\n",
            "epoch 259, loss 102.99356079101562\n",
            "epoch 260, loss 101.39834594726562\n",
            "epoch 261, loss 98.93512725830078\n",
            "epoch 262, loss 95.92862701416016\n",
            "epoch 263, loss 92.68470001220703\n",
            "epoch 264, loss 89.46170806884766\n",
            "epoch 265, loss 86.45521545410156\n",
            "epoch 266, loss 83.79401397705078\n",
            "epoch 267, loss 81.54408264160156\n",
            "epoch 268, loss 79.71868896484375\n",
            "epoch 269, loss 78.29273223876953\n",
            "epoch 270, loss 77.21666717529297\n",
            "epoch 271, loss 76.42987823486328\n",
            "epoch 272, loss 75.87101745605469\n",
            "epoch 273, loss 75.4850845336914\n",
            "epoch 274, loss 75.22740173339844\n",
            "epoch 275, loss 75.06387329101562\n",
            "epoch 276, loss 74.97020721435547\n",
            "epoch 277, loss 74.92903137207031\n",
            "epoch 278, loss 74.92742156982422\n",
            "epoch 279, loss 74.95350646972656\n",
            "epoch 280, loss 74.99536895751953\n",
            "epoch 281, loss 75.04052734375\n",
            "epoch 282, loss 75.0753173828125\n",
            "epoch 283, loss 75.08658599853516\n",
            "epoch 284, loss 75.06285095214844\n",
            "epoch 285, loss 74.99546813964844\n",
            "epoch 286, loss 74.88027954101562\n",
            "epoch 287, loss 74.71791076660156\n",
            "epoch 288, loss 74.51397705078125\n",
            "epoch 289, loss 74.27810668945312\n",
            "epoch 290, loss 74.02302551269531\n",
            "epoch 291, loss 73.76309204101562\n",
            "epoch 292, loss 73.51263427734375\n",
            "epoch 293, loss 73.2841796875\n",
            "epoch 294, loss 73.08761596679688\n",
            "epoch 295, loss 72.92945861816406\n",
            "epoch 296, loss 72.81169128417969\n",
            "epoch 297, loss 72.7334213256836\n",
            "epoch 298, loss 72.68990325927734\n",
            "epoch 299, loss 72.67475128173828\n",
            "Pre normalize: \n",
            "tensor([[-3.3922e+00,  9.2923e+00,  1.4550e-02,  1.1354e+01, -4.8681e+00,\n",
            "          5.5872e-02,  9.6664e-01,  1.2583e+00,  3.3379e+01, -2.3959e+01,\n",
            "          4.7536e-02,  2.5538e+01, -3.0535e+01, -1.1589e+01,  2.9008e+00,\n",
            "          1.9308e+01,  1.9565e+01, -8.5991e-01,  2.4079e-02,  1.0177e+01]])\n",
            "tensor([-23.2882])\n",
            "After normalize: \n",
            "tensor([[-3.3465e+00,  9.1672e+00,  1.4354e-02,  1.1201e+01, -4.8026e+00,\n",
            "          5.5120e-02,  9.5362e-01,  1.2414e+00,  3.2929e+01, -2.3637e+01,\n",
            "          4.6896e-02,  2.5194e+01, -3.0124e+01, -1.1433e+01,  2.8617e+00,\n",
            "          1.9048e+01,  1.9301e+01, -8.4833e-01,  2.3755e-02,  1.0040e+01]])\n",
            "tensor([-22.9746])\n",
            "c value = tensor([0.9865], grad_fn=<MulBackward0>)\n",
            "epoch 300, loss 72.66726684570312\n",
            "epoch 301, loss 72.68404388427734\n",
            "epoch 302, loss 72.70516967773438\n",
            "epoch 303, loss 72.72430419921875\n",
            "epoch 304, loss 72.73677825927734\n",
            "epoch 305, loss 72.73988342285156\n",
            "epoch 306, loss 72.732421875\n",
            "epoch 307, loss 72.71508026123047\n",
            "epoch 308, loss 72.68946838378906\n",
            "epoch 309, loss 72.65789031982422\n",
            "epoch 310, loss 72.62300109863281\n",
            "epoch 311, loss 72.58726501464844\n",
            "epoch 312, loss 72.55291748046875\n",
            "epoch 313, loss 72.521484375\n",
            "epoch 314, loss 72.49422454833984\n",
            "epoch 315, loss 72.47132873535156\n",
            "epoch 316, loss 72.45305633544922\n",
            "epoch 317, loss 72.43877410888672\n",
            "epoch 318, loss 72.4278564453125\n",
            "epoch 319, loss 72.4198989868164\n",
            "epoch 320, loss 72.41384887695312\n",
            "epoch 321, loss 72.40921020507812\n",
            "epoch 322, loss 72.40564727783203\n",
            "epoch 323, loss 72.40264129638672\n",
            "epoch 324, loss 72.4000473022461\n",
            "epoch 325, loss 72.39769744873047\n",
            "epoch 326, loss 72.39546203613281\n",
            "epoch 327, loss 72.39342498779297\n",
            "epoch 328, loss 72.39138793945312\n",
            "epoch 329, loss 72.38932037353516\n",
            "epoch 330, loss 72.3873519897461\n",
            "epoch 331, loss 72.38528442382812\n",
            "epoch 332, loss 72.38309478759766\n",
            "epoch 333, loss 72.38079071044922\n",
            "epoch 334, loss 72.37821960449219\n",
            "epoch 335, loss 72.37561798095703\n",
            "epoch 336, loss 72.37296295166016\n",
            "epoch 337, loss 72.37030029296875\n",
            "epoch 338, loss 72.3677749633789\n",
            "epoch 339, loss 72.36528778076172\n",
            "epoch 340, loss 72.36306762695312\n",
            "epoch 341, loss 72.36132049560547\n",
            "epoch 342, loss 72.35974884033203\n",
            "epoch 343, loss 72.35855102539062\n",
            "epoch 344, loss 72.35773468017578\n",
            "epoch 345, loss 72.35718536376953\n",
            "epoch 346, loss 72.35680389404297\n",
            "epoch 347, loss 72.35657501220703\n",
            "epoch 348, loss 72.35637664794922\n",
            "epoch 349, loss 72.35621643066406\n",
            "Pre normalize: \n",
            "tensor([[-3.3483e+00,  9.1677e+00,  1.4035e-02,  1.1202e+01, -4.8016e+00,\n",
            "          5.3233e-02,  9.5377e-01,  1.2388e+00,  3.2930e+01, -2.3637e+01,\n",
            "          4.6745e-02,  2.5188e+01, -3.0133e+01, -1.1442e+01,  2.8537e+00,\n",
            "          1.9049e+01,  1.9294e+01, -8.5067e-01,  2.1552e-02,  1.0031e+01]])\n",
            "tensor([-22.9824])\n",
            "After normalize: \n",
            "tensor([[-9.4078e+00,  2.5758e+01,  3.9434e-02,  3.1473e+01, -1.3491e+01,\n",
            "          1.4957e-01,  2.6798e+00,  3.4807e+00,  9.2524e+01, -6.6413e+01,\n",
            "          1.3134e-01,  7.0769e+01, -8.4664e+01, -3.2147e+01,  8.0179e+00,\n",
            "          5.3521e+01,  5.4211e+01, -2.3901e+00,  6.0554e-02,  2.8185e+01]])\n",
            "tensor([-64.5732])\n",
            "c value = tensor([2.8097], grad_fn=<MulBackward0>)\n",
            "epoch 350, loss 73.32032012939453\n",
            "epoch 351, loss 73.31405639648438\n",
            "epoch 352, loss 73.30497741699219\n",
            "epoch 353, loss 73.29421997070312\n",
            "epoch 354, loss 73.28314208984375\n",
            "epoch 355, loss 73.27263641357422\n",
            "epoch 356, loss 73.26314544677734\n",
            "epoch 357, loss 73.25466918945312\n",
            "epoch 358, loss 73.24662017822266\n",
            "epoch 359, loss 73.23860168457031\n",
            "epoch 360, loss 73.2303695678711\n",
            "epoch 361, loss 73.2215805053711\n",
            "epoch 362, loss 73.21246337890625\n",
            "epoch 363, loss 73.20333862304688\n",
            "epoch 364, loss 73.19444274902344\n",
            "epoch 365, loss 73.18618774414062\n",
            "epoch 366, loss 73.17868041992188\n",
            "epoch 367, loss 73.17171478271484\n",
            "epoch 368, loss 73.16512298583984\n",
            "epoch 369, loss 73.15890502929688\n",
            "epoch 370, loss 73.15257263183594\n",
            "epoch 371, loss 73.14622497558594\n",
            "epoch 372, loss 73.13967895507812\n",
            "epoch 373, loss 73.13324737548828\n",
            "epoch 374, loss 73.12677001953125\n",
            "epoch 375, loss 73.12059783935547\n",
            "epoch 376, loss 73.1146240234375\n",
            "epoch 377, loss 73.10885620117188\n",
            "epoch 378, loss 73.10334777832031\n",
            "epoch 379, loss 73.09778594970703\n",
            "epoch 380, loss 73.09243774414062\n",
            "epoch 381, loss 73.08692932128906\n",
            "epoch 382, loss 73.08148193359375\n",
            "epoch 383, loss 73.07593536376953\n",
            "epoch 384, loss 73.07061004638672\n",
            "epoch 385, loss 73.06532287597656\n",
            "epoch 386, loss 73.06023406982422\n",
            "epoch 387, loss 73.05523681640625\n",
            "epoch 388, loss 73.05035400390625\n",
            "epoch 389, loss 73.04561614990234\n",
            "epoch 390, loss 73.0408935546875\n",
            "epoch 391, loss 73.03621673583984\n",
            "epoch 392, loss 73.03165435791016\n",
            "epoch 393, loss 73.02727508544922\n",
            "epoch 394, loss 73.02300262451172\n",
            "epoch 395, loss 73.01882934570312\n",
            "epoch 396, loss 73.0146484375\n",
            "epoch 397, loss 73.01074981689453\n",
            "epoch 398, loss 73.00694274902344\n",
            "epoch 399, loss 73.0031967163086\n",
            "Pre normalize: \n",
            "tensor([[-9.4065e+00,  2.5761e+01,  4.1707e-02,  3.1476e+01, -1.3488e+01,\n",
            "          1.5119e-01,  2.6829e+00,  3.4823e+00,  9.2527e+01, -6.6410e+01,\n",
            "          1.3355e-01,  7.0769e+01, -8.4657e+01, -3.2142e+01,  8.0240e+00,\n",
            "          5.3524e+01,  5.4211e+01, -2.3862e+00,  6.1688e-02,  2.8188e+01]])\n",
            "tensor([-64.5673])\n",
            "After normalize: \n",
            "tensor([[-1.3967e+01,  3.8251e+01,  6.1929e-02,  4.6736e+01, -2.0027e+01,\n",
            "          2.2449e-01,  3.9836e+00,  5.1706e+00,  1.3739e+02, -9.8607e+01,\n",
            "          1.9830e-01,  1.0508e+02, -1.2570e+02, -4.7725e+01,  1.1914e+01,\n",
            "          7.9474e+01,  8.0495e+01, -3.5432e+00,  9.1596e-02,  4.1855e+01]])\n",
            "tensor([-95.8718])\n",
            "c value = tensor([1.4848], grad_fn=<MulBackward0>)\n",
            "epoch 400, loss 73.94341278076172\n",
            "epoch 401, loss 73.9348373413086\n",
            "epoch 402, loss 73.92587280273438\n",
            "epoch 403, loss 73.91668701171875\n",
            "epoch 404, loss 73.90727996826172\n",
            "epoch 405, loss 73.89764404296875\n",
            "epoch 406, loss 73.88792419433594\n",
            "epoch 407, loss 73.87803649902344\n",
            "epoch 408, loss 73.86817169189453\n",
            "epoch 409, loss 73.85838317871094\n",
            "epoch 410, loss 73.8486328125\n",
            "epoch 411, loss 73.83915710449219\n",
            "epoch 412, loss 73.82987213134766\n",
            "epoch 413, loss 73.82071685791016\n",
            "epoch 414, loss 73.81171417236328\n",
            "epoch 415, loss 73.80299377441406\n",
            "epoch 416, loss 73.7945556640625\n",
            "epoch 417, loss 73.78636932373047\n",
            "epoch 418, loss 73.77847290039062\n",
            "epoch 419, loss 73.77086639404297\n",
            "epoch 420, loss 73.76350402832031\n",
            "epoch 421, loss 73.75643920898438\n",
            "epoch 422, loss 73.74961853027344\n",
            "epoch 423, loss 73.74301147460938\n",
            "epoch 424, loss 73.73676300048828\n",
            "epoch 425, loss 73.73072814941406\n",
            "epoch 426, loss 73.72493743896484\n",
            "epoch 427, loss 73.71939849853516\n",
            "epoch 428, loss 73.71407318115234\n",
            "epoch 429, loss 73.70897674560547\n",
            "epoch 430, loss 73.7040786743164\n",
            "epoch 431, loss 73.69938659667969\n",
            "epoch 432, loss 73.69489288330078\n",
            "epoch 433, loss 73.69059753417969\n",
            "epoch 434, loss 73.68643188476562\n",
            "epoch 435, loss 73.68244171142578\n",
            "epoch 436, loss 73.67857360839844\n",
            "epoch 437, loss 73.67491149902344\n",
            "epoch 438, loss 73.67134857177734\n",
            "epoch 439, loss 73.66789245605469\n",
            "epoch 440, loss 73.6645736694336\n",
            "epoch 441, loss 73.6613998413086\n",
            "epoch 442, loss 73.65831756591797\n",
            "epoch 443, loss 73.65528869628906\n",
            "epoch 444, loss 73.6524887084961\n",
            "epoch 445, loss 73.649658203125\n",
            "epoch 446, loss 73.64692687988281\n",
            "epoch 447, loss 73.64425659179688\n",
            "epoch 448, loss 73.64165496826172\n",
            "epoch 449, loss 73.63912963867188\n",
            "Pre normalize: \n",
            "tensor([[-1.3966e+01,  3.8255e+01,  6.5342e-02,  4.6740e+01, -2.0024e+01,\n",
            "          2.2701e-01,  3.9874e+00,  5.1725e+00,  1.3739e+02, -9.8603e+01,\n",
            "          2.0158e-01,  1.0508e+02, -1.2569e+02, -4.7722e+01,  1.1921e+01,\n",
            "          7.9477e+01,  8.0495e+01, -3.5384e+00,  9.3074e-02,  4.1856e+01]])\n",
            "tensor([-95.8650])\n",
            "After normalize: \n",
            "tensor([[-1.3995e+01,  3.8335e+01,  6.5479e-02,  4.6838e+01, -2.0066e+01,\n",
            "          2.2749e-01,  3.9958e+00,  5.1834e+00,  1.3768e+02, -9.8811e+01,\n",
            "          2.0200e-01,  1.0530e+02, -1.2596e+02, -4.7822e+01,  1.1946e+01,\n",
            "          7.9644e+01,  8.0664e+01, -3.5458e+00,  9.3269e-02,  4.1944e+01]])\n",
            "tensor([-96.0666])\n",
            "c value = tensor([1.0021], grad_fn=<MulBackward0>)\n",
            "epoch 450, loss 73.64265441894531\n",
            "epoch 451, loss 73.6402359008789\n",
            "epoch 452, loss 73.63784790039062\n",
            "epoch 453, loss 73.63557434082031\n",
            "epoch 454, loss 73.63331604003906\n",
            "epoch 455, loss 73.63101959228516\n",
            "epoch 456, loss 73.62889862060547\n",
            "epoch 457, loss 73.62670135498047\n",
            "epoch 458, loss 73.62458801269531\n",
            "epoch 459, loss 73.62244415283203\n",
            "epoch 460, loss 73.62036895751953\n",
            "epoch 461, loss 73.61837768554688\n",
            "epoch 462, loss 73.61629486083984\n",
            "epoch 463, loss 73.61429595947266\n",
            "epoch 464, loss 73.6123046875\n",
            "epoch 465, loss 73.61041259765625\n",
            "epoch 466, loss 73.60838317871094\n",
            "epoch 467, loss 73.60650634765625\n",
            "epoch 468, loss 73.6044692993164\n",
            "epoch 469, loss 73.60259246826172\n",
            "epoch 470, loss 73.60069274902344\n",
            "epoch 471, loss 73.59880828857422\n",
            "epoch 472, loss 73.596923828125\n",
            "epoch 473, loss 73.59512329101562\n",
            "epoch 474, loss 73.59323120117188\n",
            "epoch 475, loss 73.59139251708984\n",
            "epoch 476, loss 73.58952331542969\n",
            "epoch 477, loss 73.58763122558594\n",
            "epoch 478, loss 73.58586883544922\n",
            "epoch 479, loss 73.58405303955078\n",
            "epoch 480, loss 73.58222198486328\n",
            "epoch 481, loss 73.58045196533203\n",
            "epoch 482, loss 73.57861328125\n",
            "epoch 483, loss 73.57673645019531\n",
            "epoch 484, loss 73.57500457763672\n",
            "epoch 485, loss 73.57324981689453\n",
            "epoch 486, loss 73.5713882446289\n",
            "epoch 487, loss 73.56961822509766\n",
            "epoch 488, loss 73.56786346435547\n",
            "epoch 489, loss 73.56600952148438\n",
            "epoch 490, loss 73.56427001953125\n",
            "epoch 491, loss 73.5625\n",
            "epoch 492, loss 73.56069946289062\n",
            "epoch 493, loss 73.55888366699219\n",
            "epoch 494, loss 73.55714416503906\n",
            "epoch 495, loss 73.55540466308594\n",
            "epoch 496, loss 73.5536117553711\n",
            "epoch 497, loss 73.55182647705078\n",
            "epoch 498, loss 73.55004119873047\n",
            "epoch 499, loss 73.54827880859375\n",
            "Pre normalize: \n",
            "tensor([[-1.3994e+01,  3.8337e+01,  6.7601e-02,  4.6840e+01, -2.0065e+01,\n",
            "          2.2868e-01,  3.9980e+00,  5.1839e+00,  1.3768e+02, -9.8808e+01,\n",
            "          2.0404e-01,  1.0530e+02, -1.2596e+02, -4.7823e+01,  1.1948e+01,\n",
            "          7.9646e+01,  8.0663e+01, -3.5435e+00,  9.3528e-02,  4.1942e+01]])\n",
            "tensor([-96.0647])\n",
            "After normalize: \n",
            "tensor([[-1.3885e+01,  3.8039e+01,  6.7074e-02,  4.6475e+01, -1.9908e+01,\n",
            "          2.2690e-01,  3.9668e+00,  5.1435e+00,  1.3661e+02, -9.8038e+01,\n",
            "          2.0245e-01,  1.0448e+02, -1.2497e+02, -4.7450e+01,  1.1855e+01,\n",
            "          7.9025e+01,  8.0035e+01, -3.5159e+00,  9.2799e-02,  4.1615e+01]])\n",
            "tensor([-95.3161])\n",
            "c value = tensor([0.9922], grad_fn=<MulBackward0>)\n",
            "epoch 500, loss 73.52590942382812\n",
            "epoch 501, loss 73.52413940429688\n",
            "epoch 502, loss 73.52238464355469\n",
            "epoch 503, loss 73.52066802978516\n",
            "epoch 504, loss 73.51900482177734\n",
            "epoch 505, loss 73.51724243164062\n",
            "epoch 506, loss 73.51557922363281\n",
            "epoch 507, loss 73.51382446289062\n",
            "epoch 508, loss 73.5120620727539\n",
            "epoch 509, loss 73.51044464111328\n",
            "epoch 510, loss 73.50870513916016\n",
            "epoch 511, loss 73.50701141357422\n",
            "epoch 512, loss 73.50534057617188\n",
            "epoch 513, loss 73.50360870361328\n",
            "epoch 514, loss 73.501953125\n",
            "epoch 515, loss 73.500244140625\n",
            "epoch 516, loss 73.49856567382812\n",
            "epoch 517, loss 73.496826171875\n",
            "epoch 518, loss 73.49515533447266\n",
            "epoch 519, loss 73.49341583251953\n",
            "epoch 520, loss 73.49178314208984\n",
            "epoch 521, loss 73.49007415771484\n",
            "epoch 522, loss 73.48839569091797\n",
            "epoch 523, loss 73.48667907714844\n",
            "epoch 524, loss 73.48500061035156\n",
            "epoch 525, loss 73.48329162597656\n",
            "epoch 526, loss 73.481689453125\n",
            "epoch 527, loss 73.48001098632812\n",
            "epoch 528, loss 73.4782943725586\n",
            "epoch 529, loss 73.47657012939453\n",
            "epoch 530, loss 73.47488403320312\n",
            "epoch 531, loss 73.4732894897461\n",
            "epoch 532, loss 73.47151184082031\n",
            "epoch 533, loss 73.4698715209961\n",
            "epoch 534, loss 73.46820831298828\n",
            "epoch 535, loss 73.4665298461914\n",
            "epoch 536, loss 73.46488189697266\n",
            "epoch 537, loss 73.46316528320312\n",
            "epoch 538, loss 73.46153259277344\n",
            "epoch 539, loss 73.4598388671875\n",
            "epoch 540, loss 73.4581527709961\n",
            "epoch 541, loss 73.45647430419922\n",
            "epoch 542, loss 73.45480346679688\n",
            "epoch 543, loss 73.45304107666016\n",
            "epoch 544, loss 73.45144653320312\n",
            "epoch 545, loss 73.44975280761719\n",
            "epoch 546, loss 73.44806671142578\n",
            "epoch 547, loss 73.44648742675781\n",
            "epoch 548, loss 73.44481658935547\n",
            "epoch 549, loss 73.44309997558594\n",
            "Pre normalize: \n",
            "tensor([[-1.3885e+01,  3.8040e+01,  6.8887e-02,  4.6477e+01, -1.9907e+01,\n",
            "          2.2788e-01,  3.9685e+00,  5.1437e+00,  1.3661e+02, -9.8037e+01,\n",
            "          2.0419e-01,  1.0448e+02, -1.2497e+02, -4.7452e+01,  1.1856e+01,\n",
            "          7.9027e+01,  8.0034e+01, -3.5140e+00,  9.2848e-02,  4.1613e+01]])\n",
            "tensor([-95.3152])\n",
            "After normalize: \n",
            "tensor([[-1.4260e+01,  3.9067e+01,  7.0747e-02,  4.7732e+01, -2.0444e+01,\n",
            "          2.3403e-01,  4.0756e+00,  5.2825e+00,  1.4030e+02, -1.0068e+02,\n",
            "          2.0970e-01,  1.0730e+02, -1.2835e+02, -4.8733e+01,  1.2176e+01,\n",
            "          8.1160e+01,  8.2194e+01, -3.6088e+00,  9.5354e-02,  4.2736e+01]])\n",
            "tensor([-97.8879])\n",
            "c value = tensor([1.0270], grad_fn=<MulBackward0>)\n",
            "epoch 550, loss 73.50851440429688\n",
            "epoch 551, loss 73.50666809082031\n",
            "epoch 552, loss 73.50492095947266\n",
            "epoch 553, loss 73.50314331054688\n",
            "epoch 554, loss 73.50135040283203\n",
            "epoch 555, loss 73.49959564208984\n",
            "epoch 556, loss 73.49781036376953\n",
            "epoch 557, loss 73.49597930908203\n",
            "epoch 558, loss 73.49413299560547\n",
            "epoch 559, loss 73.49243927001953\n",
            "epoch 560, loss 73.49057006835938\n",
            "epoch 561, loss 73.48877716064453\n",
            "epoch 562, loss 73.48690032958984\n",
            "epoch 563, loss 73.48517608642578\n",
            "epoch 564, loss 73.48330688476562\n",
            "epoch 565, loss 73.4814453125\n",
            "epoch 566, loss 73.47966003417969\n",
            "epoch 567, loss 73.4778060913086\n",
            "epoch 568, loss 73.47604370117188\n",
            "epoch 569, loss 73.4742660522461\n",
            "epoch 570, loss 73.47239685058594\n",
            "epoch 571, loss 73.47054290771484\n",
            "epoch 572, loss 73.46878051757812\n",
            "epoch 573, loss 73.46695709228516\n",
            "epoch 574, loss 73.46513366699219\n",
            "epoch 575, loss 73.46329498291016\n",
            "epoch 576, loss 73.46147155761719\n",
            "epoch 577, loss 73.45963287353516\n",
            "epoch 578, loss 73.45780944824219\n",
            "epoch 579, loss 73.45602416992188\n",
            "epoch 580, loss 73.45421600341797\n",
            "epoch 581, loss 73.45238494873047\n",
            "epoch 582, loss 73.45059204101562\n",
            "epoch 583, loss 73.44872283935547\n",
            "epoch 584, loss 73.4469985961914\n",
            "epoch 585, loss 73.44510650634766\n",
            "epoch 586, loss 73.44328308105469\n",
            "epoch 587, loss 73.44149017333984\n",
            "epoch 588, loss 73.43966674804688\n",
            "epoch 589, loss 73.4378662109375\n",
            "epoch 590, loss 73.43605041503906\n",
            "epoch 591, loss 73.4342269897461\n",
            "epoch 592, loss 73.43241119384766\n",
            "epoch 593, loss 73.43063354492188\n",
            "epoch 594, loss 73.42879486083984\n",
            "epoch 595, loss 73.42700958251953\n",
            "epoch 596, loss 73.42522430419922\n",
            "epoch 597, loss 73.4233627319336\n",
            "epoch 598, loss 73.42157745361328\n",
            "epoch 599, loss 73.4198226928711\n",
            "Pre normalize: \n",
            "tensor([[-1.4260e+01,  3.9068e+01,  7.2623e-02,  4.7733e+01, -2.0443e+01,\n",
            "          2.3509e-01,  4.0774e+00,  5.2828e+00,  1.4030e+02, -1.0068e+02,\n",
            "          2.1149e-01,  1.0730e+02, -1.2835e+02, -4.8735e+01,  1.2177e+01,\n",
            "          8.1161e+01,  8.2193e+01, -3.6069e+00,  9.5408e-02,  4.2733e+01]])\n",
            "tensor([-97.8871])\n",
            "After normalize: \n",
            "tensor([[-1.4792e+01,  4.0526e+01,  7.5332e-02,  4.9514e+01, -2.1206e+01,\n",
            "          2.4386e-01,  4.2295e+00,  5.4798e+00,  1.4553e+02, -1.0444e+02,\n",
            "          2.1938e-01,  1.1130e+02, -1.3313e+02, -5.0553e+01,  1.2631e+01,\n",
            "          8.4188e+01,  8.5258e+01, -3.7415e+00,  9.8966e-02,  4.4327e+01]])\n",
            "tensor([-101.5383])\n",
            "c value = tensor([1.0373], grad_fn=<MulBackward0>)\n",
            "epoch 600, loss 73.5093765258789\n",
            "epoch 601, loss 73.50736999511719\n",
            "epoch 602, loss 73.50542449951172\n",
            "epoch 603, loss 73.50336456298828\n",
            "epoch 604, loss 73.50148010253906\n",
            "epoch 605, loss 73.49942779541016\n",
            "epoch 606, loss 73.49747467041016\n",
            "epoch 607, loss 73.49542236328125\n",
            "epoch 608, loss 73.49343872070312\n",
            "epoch 609, loss 73.49142456054688\n",
            "epoch 610, loss 73.48936462402344\n",
            "epoch 611, loss 73.48735809326172\n",
            "epoch 612, loss 73.4853515625\n",
            "epoch 613, loss 73.4833755493164\n",
            "epoch 614, loss 73.48138427734375\n",
            "epoch 615, loss 73.47932434082031\n",
            "epoch 616, loss 73.47723388671875\n",
            "epoch 617, loss 73.47518157958984\n",
            "epoch 618, loss 73.47311401367188\n",
            "epoch 619, loss 73.47117614746094\n",
            "epoch 620, loss 73.46910095214844\n",
            "epoch 621, loss 73.46707916259766\n",
            "epoch 622, loss 73.46495819091797\n",
            "epoch 623, loss 73.4629135131836\n",
            "epoch 624, loss 73.46092224121094\n",
            "epoch 625, loss 73.45886993408203\n",
            "epoch 626, loss 73.45684814453125\n",
            "epoch 627, loss 73.454833984375\n",
            "epoch 628, loss 73.45283508300781\n",
            "epoch 629, loss 73.45077514648438\n",
            "epoch 630, loss 73.44866943359375\n",
            "epoch 631, loss 73.44666290283203\n",
            "epoch 632, loss 73.44465637207031\n",
            "epoch 633, loss 73.44255828857422\n",
            "epoch 634, loss 73.44062805175781\n",
            "epoch 635, loss 73.43854522705078\n",
            "epoch 636, loss 73.43647766113281\n",
            "epoch 637, loss 73.43451690673828\n",
            "epoch 638, loss 73.43244934082031\n",
            "epoch 639, loss 73.43047332763672\n",
            "epoch 640, loss 73.42842102050781\n",
            "epoch 641, loss 73.42642974853516\n",
            "epoch 642, loss 73.42440032958984\n",
            "epoch 643, loss 73.4223861694336\n",
            "epoch 644, loss 73.42035675048828\n",
            "epoch 645, loss 73.4183349609375\n",
            "epoch 646, loss 73.41636657714844\n",
            "epoch 647, loss 73.4143295288086\n",
            "epoch 648, loss 73.41230010986328\n",
            "epoch 649, loss 73.41030883789062\n",
            "Pre normalize: \n",
            "tensor([[-1.4792e+01,  4.0527e+01,  7.7316e-02,  4.9516e+01, -2.1204e+01,\n",
            "          2.4501e-01,  4.2313e+00,  5.4800e+00,  1.4553e+02, -1.0443e+02,\n",
            "          2.2128e-01,  1.1130e+02, -1.3313e+02, -5.0555e+01,  1.2632e+01,\n",
            "          8.4189e+01,  8.5257e+01, -3.7395e+00,  9.9013e-02,  4.4325e+01]])\n",
            "tensor([-101.5373])\n",
            "After normalize: \n",
            "tensor([[-1.5436e+01,  4.2294e+01,  8.0688e-02,  5.1675e+01, -2.2129e+01,\n",
            "          2.5570e-01,  4.4158e+00,  5.7190e+00,  1.5188e+02, -1.0899e+02,\n",
            "          2.3093e-01,  1.1615e+02, -1.3894e+02, -5.2760e+01,  1.3183e+01,\n",
            "          8.7860e+01,  8.8975e+01, -3.9025e+00,  1.0333e-01,  4.6257e+01]])\n",
            "tensor([-105.9646])\n",
            "c value = tensor([1.0436], grad_fn=<MulBackward0>)\n",
            "epoch 650, loss 73.5145263671875\n",
            "epoch 651, loss 73.51226043701172\n",
            "epoch 652, loss 73.51009368896484\n",
            "epoch 653, loss 73.50790405273438\n",
            "epoch 654, loss 73.50559997558594\n",
            "epoch 655, loss 73.50334930419922\n",
            "epoch 656, loss 73.5010986328125\n",
            "epoch 657, loss 73.4988021850586\n",
            "epoch 658, loss 73.49656677246094\n",
            "epoch 659, loss 73.4942855834961\n",
            "epoch 660, loss 73.49190521240234\n",
            "epoch 661, loss 73.48961639404297\n",
            "epoch 662, loss 73.48741149902344\n",
            "epoch 663, loss 73.485107421875\n",
            "epoch 664, loss 73.4827880859375\n",
            "epoch 665, loss 73.4804916381836\n",
            "epoch 666, loss 73.47815704345703\n",
            "epoch 667, loss 73.4758529663086\n",
            "epoch 668, loss 73.47362518310547\n",
            "epoch 669, loss 73.47126770019531\n",
            "epoch 670, loss 73.46890258789062\n",
            "epoch 671, loss 73.4666519165039\n",
            "epoch 672, loss 73.46430969238281\n",
            "epoch 673, loss 73.46202850341797\n",
            "epoch 674, loss 73.4596939086914\n",
            "epoch 675, loss 73.45736694335938\n",
            "epoch 676, loss 73.45516204833984\n",
            "epoch 677, loss 73.45285034179688\n",
            "epoch 678, loss 73.4504623413086\n",
            "epoch 679, loss 73.44819641113281\n",
            "epoch 680, loss 73.44591522216797\n",
            "epoch 681, loss 73.44361877441406\n",
            "epoch 682, loss 73.44132232666016\n",
            "epoch 683, loss 73.43904876708984\n",
            "epoch 684, loss 73.436767578125\n",
            "epoch 685, loss 73.43437957763672\n",
            "epoch 686, loss 73.43212890625\n",
            "epoch 687, loss 73.42987823486328\n",
            "epoch 688, loss 73.42759704589844\n",
            "epoch 689, loss 73.42527770996094\n",
            "epoch 690, loss 73.42303466796875\n",
            "epoch 691, loss 73.42078399658203\n",
            "epoch 692, loss 73.41842651367188\n",
            "epoch 693, loss 73.41621398925781\n",
            "epoch 694, loss 73.41389465332031\n",
            "epoch 695, loss 73.41159057617188\n",
            "epoch 696, loss 73.40935516357422\n",
            "epoch 697, loss 73.4071273803711\n",
            "epoch 698, loss 73.4049072265625\n",
            "epoch 699, loss 73.40254974365234\n",
            "Pre normalize: \n",
            "tensor([[-1.5436e+01,  4.2296e+01,  8.2792e-02,  5.1677e+01, -2.2128e+01,\n",
            "          2.5695e-01,  4.4178e+00,  5.7192e+00,  1.5188e+02, -1.0899e+02,\n",
            "          2.3294e-01,  1.1615e+02, -1.3894e+02, -5.2762e+01,  1.3184e+01,\n",
            "          8.7861e+01,  8.8973e+01, -3.9004e+00,  1.0336e-01,  4.6254e+01]])\n",
            "tensor([-105.9636])\n",
            "After normalize: \n",
            "tensor([[-1.6206e+01,  4.4405e+01,  8.6921e-02,  5.4253e+01, -2.3231e+01,\n",
            "          2.6976e-01,  4.6381e+00,  6.0044e+00,  1.5945e+02, -1.1442e+02,\n",
            "          2.4455e-01,  1.2194e+02, -1.4586e+02, -5.5393e+01,  1.3842e+01,\n",
            "          9.2243e+01,  9.3410e+01, -4.0949e+00,  1.0852e-01,  4.8561e+01]])\n",
            "tensor([-111.2475])\n",
            "c value = tensor([1.0499], grad_fn=<MulBackward0>)\n",
            "epoch 700, loss 73.52136993408203\n",
            "epoch 701, loss 73.51885986328125\n",
            "epoch 702, loss 73.51632690429688\n",
            "epoch 703, loss 73.5137710571289\n",
            "epoch 704, loss 73.51123046875\n",
            "epoch 705, loss 73.50872039794922\n",
            "epoch 706, loss 73.506103515625\n",
            "epoch 707, loss 73.50345611572266\n",
            "epoch 708, loss 73.50081634521484\n",
            "epoch 709, loss 73.49827575683594\n",
            "epoch 710, loss 73.49562072753906\n",
            "epoch 711, loss 73.49296569824219\n",
            "epoch 712, loss 73.49034881591797\n",
            "epoch 713, loss 73.48773956298828\n",
            "epoch 714, loss 73.48509216308594\n",
            "epoch 715, loss 73.48245239257812\n",
            "epoch 716, loss 73.47981262207031\n",
            "epoch 717, loss 73.47710418701172\n",
            "epoch 718, loss 73.47450256347656\n",
            "epoch 719, loss 73.47184753417969\n",
            "epoch 720, loss 73.4692611694336\n",
            "epoch 721, loss 73.46663665771484\n",
            "epoch 722, loss 73.4639663696289\n",
            "epoch 723, loss 73.46131896972656\n",
            "epoch 724, loss 73.45865631103516\n",
            "epoch 725, loss 73.4560317993164\n",
            "epoch 726, loss 73.45338439941406\n",
            "epoch 727, loss 73.45077514648438\n",
            "epoch 728, loss 73.44813537597656\n",
            "epoch 729, loss 73.44549560546875\n",
            "epoch 730, loss 73.44288635253906\n",
            "epoch 731, loss 73.44023895263672\n",
            "epoch 732, loss 73.43763732910156\n",
            "epoch 733, loss 73.43498992919922\n",
            "epoch 734, loss 73.43241882324219\n",
            "epoch 735, loss 73.42987060546875\n",
            "epoch 736, loss 73.4271469116211\n",
            "epoch 737, loss 73.42463684082031\n",
            "epoch 738, loss 73.42196655273438\n",
            "epoch 739, loss 73.41931915283203\n",
            "epoch 740, loss 73.41677856445312\n",
            "epoch 741, loss 73.4141616821289\n",
            "epoch 742, loss 73.4115982055664\n",
            "epoch 743, loss 73.40906524658203\n",
            "epoch 744, loss 73.40645599365234\n",
            "epoch 745, loss 73.40383911132812\n",
            "epoch 746, loss 73.4012451171875\n",
            "epoch 747, loss 73.39872741699219\n",
            "epoch 748, loss 73.39607238769531\n",
            "epoch 749, loss 73.39350891113281\n",
            "Pre normalize: \n",
            "tensor([[-1.6206e+01,  4.4407e+01,  8.9150e-02,  5.4256e+01, -2.3229e+01,\n",
            "          2.7113e-01,  4.6402e+00,  6.0047e+00,  1.5946e+02, -1.1442e+02,\n",
            "          2.4669e-01,  1.2194e+02, -1.4586e+02, -5.5396e+01,  1.3843e+01,\n",
            "          9.2244e+01,  9.3409e+01, -4.0928e+00,  1.0854e-01,  4.8557e+01]])\n",
            "tensor([-111.2465])\n",
            "After normalize: \n",
            "tensor([[-1.7134e+01,  4.6950e+01,  9.4255e-02,  5.7363e+01, -2.4560e+01,\n",
            "          2.8666e-01,  4.9060e+00,  6.3485e+00,  1.6859e+02, -1.2097e+02,\n",
            "          2.6082e-01,  1.2892e+02, -1.5422e+02, -5.8569e+01,  1.4635e+01,\n",
            "          9.7527e+01,  9.8758e+01, -4.3272e+00,  1.1476e-01,  5.1338e+01]])\n",
            "tensor([-117.6176])\n",
            "c value = tensor([1.0573], grad_fn=<MulBackward0>)\n",
            "epoch 750, loss 73.5293960571289\n",
            "epoch 751, loss 73.5264892578125\n",
            "epoch 752, loss 73.52359008789062\n",
            "epoch 753, loss 73.52059936523438\n",
            "epoch 754, loss 73.5176773071289\n",
            "epoch 755, loss 73.51468658447266\n",
            "epoch 756, loss 73.51175689697266\n",
            "epoch 757, loss 73.50871276855469\n",
            "epoch 758, loss 73.50567626953125\n",
            "epoch 759, loss 73.50263214111328\n",
            "epoch 760, loss 73.49961853027344\n",
            "epoch 761, loss 73.49658966064453\n",
            "epoch 762, loss 73.49352264404297\n",
            "epoch 763, loss 73.49049377441406\n",
            "epoch 764, loss 73.48736572265625\n",
            "epoch 765, loss 73.48433685302734\n",
            "epoch 766, loss 73.48139190673828\n",
            "epoch 767, loss 73.47822570800781\n",
            "epoch 768, loss 73.47518920898438\n",
            "epoch 769, loss 73.47218322753906\n",
            "epoch 770, loss 73.46910095214844\n",
            "epoch 771, loss 73.46604919433594\n",
            "epoch 772, loss 73.462890625\n",
            "epoch 773, loss 73.45984649658203\n",
            "epoch 774, loss 73.4568862915039\n",
            "epoch 775, loss 73.45381164550781\n",
            "epoch 776, loss 73.45076751708984\n",
            "epoch 777, loss 73.44768524169922\n",
            "epoch 778, loss 73.44458770751953\n",
            "epoch 779, loss 73.4416275024414\n",
            "epoch 780, loss 73.4385757446289\n",
            "epoch 781, loss 73.43553924560547\n",
            "epoch 782, loss 73.43252563476562\n",
            "epoch 783, loss 73.42952728271484\n",
            "epoch 784, loss 73.4264907836914\n",
            "epoch 785, loss 73.4234390258789\n",
            "epoch 786, loss 73.42044067382812\n",
            "epoch 787, loss 73.4175033569336\n",
            "epoch 788, loss 73.41449737548828\n",
            "epoch 789, loss 73.41148376464844\n",
            "epoch 790, loss 73.40853881835938\n",
            "epoch 791, loss 73.40555572509766\n",
            "epoch 792, loss 73.40254974365234\n",
            "epoch 793, loss 73.39959716796875\n",
            "epoch 794, loss 73.3966293334961\n",
            "epoch 795, loss 73.39366149902344\n",
            "epoch 796, loss 73.39069366455078\n",
            "epoch 797, loss 73.38770294189453\n",
            "epoch 798, loss 73.38478088378906\n",
            "epoch 799, loss 73.38187408447266\n",
            "Pre normalize: \n",
            "tensor([[-1.7134e+01,  4.6952e+01,  9.6617e-02,  5.7365e+01, -2.4558e+01,\n",
            "          2.8815e-01,  4.9082e+00,  6.3488e+00,  1.6859e+02, -1.2097e+02,\n",
            "          2.6308e-01,  1.2892e+02, -1.5422e+02, -5.8572e+01,  1.4636e+01,\n",
            "          9.7529e+01,  9.8757e+01, -4.3249e+00,  1.1476e-01,  5.1335e+01]])\n",
            "tensor([-117.6166])\n",
            "After normalize: \n",
            "tensor([[-1.8279e+01,  5.0091e+01,  1.0308e-01,  6.1200e+01, -2.6200e+01,\n",
            "          3.0741e-01,  5.2364e+00,  6.7733e+00,  1.7986e+02, -1.2906e+02,\n",
            "          2.8067e-01,  1.3754e+02, -1.6453e+02, -6.2488e+01,  1.5615e+01,\n",
            "          1.0405e+02,  1.0536e+02, -4.6141e+00,  1.2243e-01,  5.4767e+01]])\n",
            "tensor([-125.4805])\n",
            "c value = tensor([1.0669], grad_fn=<MulBackward0>)\n",
            "epoch 800, loss 73.53954315185547\n",
            "epoch 801, loss 73.53617095947266\n",
            "epoch 802, loss 73.53276062011719\n",
            "epoch 803, loss 73.52938079833984\n",
            "epoch 804, loss 73.52584075927734\n",
            "epoch 805, loss 73.5223617553711\n",
            "epoch 806, loss 73.51884460449219\n",
            "epoch 807, loss 73.51533508300781\n",
            "epoch 808, loss 73.5117416381836\n",
            "epoch 809, loss 73.50822448730469\n",
            "epoch 810, loss 73.50460815429688\n",
            "epoch 811, loss 73.50103759765625\n",
            "epoch 812, loss 73.49748229980469\n",
            "epoch 813, loss 73.49385070800781\n",
            "epoch 814, loss 73.49028015136719\n",
            "epoch 815, loss 73.48667907714844\n",
            "epoch 816, loss 73.48309326171875\n",
            "epoch 817, loss 73.47948455810547\n",
            "epoch 818, loss 73.47589874267578\n",
            "epoch 819, loss 73.47223663330078\n",
            "epoch 820, loss 73.46875\n",
            "epoch 821, loss 73.46508026123047\n",
            "epoch 822, loss 73.46148681640625\n",
            "epoch 823, loss 73.4579086303711\n",
            "epoch 824, loss 73.45429992675781\n",
            "epoch 825, loss 73.45068359375\n",
            "epoch 826, loss 73.44711303710938\n",
            "epoch 827, loss 73.44354248046875\n",
            "epoch 828, loss 73.43994903564453\n",
            "epoch 829, loss 73.43643951416016\n",
            "epoch 830, loss 73.43289184570312\n",
            "epoch 831, loss 73.42929077148438\n",
            "epoch 832, loss 73.42581176757812\n",
            "epoch 833, loss 73.42222595214844\n",
            "epoch 834, loss 73.41870880126953\n",
            "epoch 835, loss 73.41517639160156\n",
            "epoch 836, loss 73.41165924072266\n",
            "epoch 837, loss 73.40813446044922\n",
            "epoch 838, loss 73.40463256835938\n",
            "epoch 839, loss 73.40110778808594\n",
            "epoch 840, loss 73.39772033691406\n",
            "epoch 841, loss 73.39424133300781\n",
            "epoch 842, loss 73.39077758789062\n",
            "epoch 843, loss 73.38724517822266\n",
            "epoch 844, loss 73.3838119506836\n",
            "epoch 845, loss 73.38040161132812\n",
            "epoch 846, loss 73.37687683105469\n",
            "epoch 847, loss 73.37342071533203\n",
            "epoch 848, loss 73.37002563476562\n",
            "epoch 849, loss 73.36653900146484\n",
            "Pre normalize: \n",
            "tensor([[-1.8279e+01,  5.0093e+01,  1.0557e-01,  6.1203e+01, -2.6198e+01,\n",
            "          3.0904e-01,  5.2387e+00,  6.7735e+00,  1.7986e+02, -1.2905e+02,\n",
            "          2.8306e-01,  1.3754e+02, -1.6453e+02, -6.2492e+01,  1.5616e+01,\n",
            "          1.0405e+02,  1.0536e+02, -4.6118e+00,  1.2241e-01,  5.4763e+01]])\n",
            "tensor([-125.4796])\n",
            "After normalize: \n",
            "tensor([[-1.9733e+01,  5.4078e+01,  1.1397e-01,  6.6071e+01, -2.8282e+01,\n",
            "          3.3362e-01,  5.6555e+00,  7.3123e+00,  1.9417e+02, -1.3932e+02,\n",
            "          3.0558e-01,  1.4848e+02, -1.7761e+02, -6.7463e+01,  1.6858e+01,\n",
            "          1.1233e+02,  1.1374e+02, -4.9787e+00,  1.3215e-01,  5.9119e+01]])\n",
            "tensor([-135.4612])\n",
            "c value = tensor([1.0795], grad_fn=<MulBackward0>)\n",
            "epoch 850, loss 73.5528564453125\n",
            "epoch 851, loss 73.548828125\n",
            "epoch 852, loss 73.54476165771484\n",
            "epoch 853, loss 73.54064178466797\n",
            "epoch 854, loss 73.53646087646484\n",
            "epoch 855, loss 73.53230285644531\n",
            "epoch 856, loss 73.528076171875\n",
            "epoch 857, loss 73.5238037109375\n",
            "epoch 858, loss 73.51951599121094\n",
            "epoch 859, loss 73.51531219482422\n",
            "epoch 860, loss 73.51104736328125\n",
            "epoch 861, loss 73.50662231445312\n",
            "epoch 862, loss 73.5023422241211\n",
            "epoch 863, loss 73.49797821044922\n",
            "epoch 864, loss 73.49374389648438\n",
            "epoch 865, loss 73.48934936523438\n",
            "epoch 866, loss 73.48501586914062\n",
            "epoch 867, loss 73.48063659667969\n",
            "epoch 868, loss 73.476318359375\n",
            "epoch 869, loss 73.47195434570312\n",
            "epoch 870, loss 73.46764373779297\n",
            "epoch 871, loss 73.46337127685547\n",
            "epoch 872, loss 73.45899200439453\n",
            "epoch 873, loss 73.45475769042969\n",
            "epoch 874, loss 73.4503402709961\n",
            "epoch 875, loss 73.44606018066406\n",
            "epoch 876, loss 73.44181060791016\n",
            "epoch 877, loss 73.4375228881836\n",
            "epoch 878, loss 73.43323516845703\n",
            "epoch 879, loss 73.42898559570312\n",
            "epoch 880, loss 73.42469024658203\n",
            "epoch 881, loss 73.42048645019531\n",
            "epoch 882, loss 73.41620635986328\n",
            "epoch 883, loss 73.41197204589844\n",
            "epoch 884, loss 73.40779113769531\n",
            "epoch 885, loss 73.403564453125\n",
            "epoch 886, loss 73.39935302734375\n",
            "epoch 887, loss 73.39519500732422\n",
            "epoch 888, loss 73.39096069335938\n",
            "epoch 889, loss 73.38677215576172\n",
            "epoch 890, loss 73.38265991210938\n",
            "epoch 891, loss 73.37852478027344\n",
            "epoch 892, loss 73.37431335449219\n",
            "epoch 893, loss 73.3701400756836\n",
            "epoch 894, loss 73.3661117553711\n",
            "epoch 895, loss 73.36199951171875\n",
            "epoch 896, loss 73.35787200927734\n",
            "epoch 897, loss 73.3538818359375\n",
            "epoch 898, loss 73.34967041015625\n",
            "epoch 899, loss 73.34569549560547\n",
            "Pre normalize: \n",
            "tensor([[-1.9733e+01,  5.4081e+01,  1.1660e-01,  6.6074e+01, -2.8281e+01,\n",
            "          3.3539e-01,  5.6580e+00,  7.3125e+00,  1.9417e+02, -1.3932e+02,\n",
            "          3.0810e-01,  1.4848e+02, -1.7761e+02, -6.7467e+01,  1.6859e+01,\n",
            "          1.1233e+02,  1.1374e+02, -4.9764e+00,  1.3211e-01,  5.9115e+01]])\n",
            "tensor([-135.4605])\n",
            "After normalize: \n",
            "tensor([[-2.1656e+01,  5.9352e+01,  1.2796e-01,  7.2514e+01, -3.1037e+01,\n",
            "          3.6808e-01,  6.2094e+00,  8.0252e+00,  2.1310e+02, -1.5290e+02,\n",
            "          3.3813e-01,  1.6295e+02, -1.9493e+02, -7.4043e+01,  1.8502e+01,\n",
            "          1.2328e+02,  1.2482e+02, -5.4614e+00,  1.4499e-01,  6.4877e+01]])\n",
            "tensor([-148.6633])\n",
            "c value = tensor([1.0975], grad_fn=<MulBackward0>)\n",
            "epoch 900, loss 73.57176208496094\n",
            "epoch 901, loss 73.56681823730469\n",
            "epoch 902, loss 73.56175231933594\n",
            "epoch 903, loss 73.55667114257812\n",
            "epoch 904, loss 73.55146789550781\n",
            "epoch 905, loss 73.54631042480469\n",
            "epoch 906, loss 73.54106903076172\n",
            "epoch 907, loss 73.53578186035156\n",
            "epoch 908, loss 73.53048706054688\n",
            "epoch 909, loss 73.52517700195312\n",
            "epoch 910, loss 73.51982879638672\n",
            "epoch 911, loss 73.51443481445312\n",
            "epoch 912, loss 73.50912475585938\n",
            "epoch 913, loss 73.50373840332031\n",
            "epoch 914, loss 73.49832916259766\n",
            "epoch 915, loss 73.49295043945312\n",
            "epoch 916, loss 73.4875717163086\n",
            "epoch 917, loss 73.48221588134766\n",
            "epoch 918, loss 73.47679901123047\n",
            "epoch 919, loss 73.47135162353516\n",
            "epoch 920, loss 73.46598815917969\n",
            "epoch 921, loss 73.46058654785156\n",
            "epoch 922, loss 73.45525360107422\n",
            "epoch 923, loss 73.44990539550781\n",
            "epoch 924, loss 73.44453430175781\n",
            "epoch 925, loss 73.43917083740234\n",
            "epoch 926, loss 73.43392181396484\n",
            "epoch 927, loss 73.42857360839844\n",
            "epoch 928, loss 73.42327117919922\n",
            "epoch 929, loss 73.41800689697266\n",
            "epoch 930, loss 73.41277313232422\n",
            "epoch 931, loss 73.40750122070312\n",
            "epoch 932, loss 73.4023208618164\n",
            "epoch 933, loss 73.39702606201172\n",
            "epoch 934, loss 73.3919677734375\n",
            "epoch 935, loss 73.38665771484375\n",
            "epoch 936, loss 73.38145446777344\n",
            "epoch 937, loss 73.3763656616211\n",
            "epoch 938, loss 73.37125396728516\n",
            "epoch 939, loss 73.36612701416016\n",
            "epoch 940, loss 73.36100769042969\n",
            "epoch 941, loss 73.35584259033203\n",
            "epoch 942, loss 73.35088348388672\n",
            "epoch 943, loss 73.34574890136719\n",
            "epoch 944, loss 73.34078216552734\n",
            "epoch 945, loss 73.33580017089844\n",
            "epoch 946, loss 73.33077239990234\n",
            "epoch 947, loss 73.32574462890625\n",
            "epoch 948, loss 73.32078552246094\n",
            "epoch 949, loss 73.31584930419922\n",
            "Pre normalize: \n",
            "tensor([[-2.1656e+01,  5.9354e+01,  1.3073e-01,  7.2516e+01, -3.1035e+01,\n",
            "          3.7002e-01,  6.2121e+00,  8.0254e+00,  2.1310e+02, -1.5289e+02,\n",
            "          3.4079e-01,  1.6295e+02, -1.9492e+02, -7.4047e+01,  1.8503e+01,\n",
            "          1.2328e+02,  1.2482e+02, -5.4591e+00,  1.4491e-01,  6.4873e+01]])\n",
            "tensor([-148.6626])\n",
            "After normalize: \n",
            "tensor([[-2.4355e+01,  6.6749e+01,  1.4702e-01,  8.1552e+01, -3.4902e+01,\n",
            "          4.1612e-01,  6.9861e+00,  9.0254e+00,  2.3965e+02, -1.7194e+02,\n",
            "          3.8325e-01,  1.8325e+02, -2.1921e+02, -8.3273e+01,  2.0809e+01,\n",
            "          1.3864e+02,  1.4037e+02, -6.1393e+00,  1.6296e-01,  7.2956e+01]])\n",
            "tensor([-167.1856])\n",
            "c value = tensor([1.1246], grad_fn=<MulBackward0>)\n",
            "epoch 950, loss 73.60081481933594\n",
            "epoch 951, loss 73.59442138671875\n",
            "epoch 952, loss 73.58800506591797\n",
            "epoch 953, loss 73.58147430419922\n",
            "epoch 954, loss 73.5747299194336\n",
            "epoch 955, loss 73.5680160522461\n",
            "epoch 956, loss 73.56124877929688\n",
            "epoch 957, loss 73.55428314208984\n",
            "epoch 958, loss 73.54747009277344\n",
            "epoch 959, loss 73.5404052734375\n",
            "epoch 960, loss 73.53353881835938\n",
            "epoch 961, loss 73.52647399902344\n",
            "epoch 962, loss 73.51946258544922\n",
            "epoch 963, loss 73.5124282836914\n",
            "epoch 964, loss 73.50531005859375\n",
            "epoch 965, loss 73.49829864501953\n",
            "epoch 966, loss 73.49131774902344\n",
            "epoch 967, loss 73.48422241210938\n",
            "epoch 968, loss 73.4771957397461\n",
            "epoch 969, loss 73.47013854980469\n",
            "epoch 970, loss 73.46318817138672\n",
            "epoch 971, loss 73.4561996459961\n"
          ]
        }
      ],
      "source": [
        "# graph_val_pen = model.train_network_with_penalty(num_epochs, given_fn, adam_optimizer, LAMBDA_PEN, LOWER_BOUND, UPPER_BOUND, N_POINTS)\n",
        "graph_val = model.train_network(num_epochs, given_fn, adam_optimizer, LOWER_BOUND, UPPER_BOUND, N_POINTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkbD3fXRLGBI",
        "outputId": "7dfa28bb-5379-4069-a4be-cf3dc88e551d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[345.6585]], grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epsilon_Loss(v_x, model, LOWER_BOUND, UPPER_BOUND, N_POINTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBYQKdIYHYOY"
      },
      "outputs": [],
      "source": [
        "loss_val = [i.detach().numpy().item() for i in graph_val[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKZP39qEHj_Z",
        "outputId": "065e7dfb-d136-4412-c8e1-70abb86d2b2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[270, 271, 272, 273, 274]"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "graph_val[0][270:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "jd2I0gRqHO8Z",
        "outputId": "15978184-99da-4f52-d206-d0deff5669aa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGMCAYAAAD0nYndAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde5ScV3nn++/z1rXv3Wq1rq2bLcmWr/JFvmEMMRAgJxkMATsJJDiYGCeTWWRYrEkyw+EMOcmsnJm1PJCZQwj3MeAkkwQSzOESbGzABoyxLdmWJUvWvXVXt/re1XXb54+qt9VqVXW/71vVltT1+6zVy13vu99du9p/1KO99/Nsc84hIiIiciHyzvcARERERKpRoCIiIiIXLAUqIiIicsFSoCIiIiIXLAUqIiIicsFSoCIiIiIXLAUqIiIicsGKn+8BBJFKpVxPT8/5HoaIiIiEcPjw4axzLlVLHxdFoNLT00NfX9/5HoaIiIiEYGYna+1DSz8iIiJywVKgIiIiIhesi2LpR0REJIpisYjOtJs/Zobnze+chwIVERFZcIrFIgcOHCCTyZzvoSx46XSaNWvWzFvAokBFREQWnBMnTuB5Hhs2bMDMzvdwFiznHIcPH+bEiRMsW7ZsXt5DgYqIiCwozjkGBwdZu3Yt8bi+5ubb0qVL2b9/P0uXLp2XoFCbaUVEZEFxzuGcI5FInO+hNIREIjH1N58PClRERGRB0ebZ80OBioiIyEVo8+bNbN68mSuuuIJYLDb1+p577nlN3v/d7343X/7yl+ds9+Uvf5mdO3fO/4BC0uKdiIjIPNq6dSsA+/fvZ/PmzVOvp8vn8+d9P82Xv/xlOjs7ufzyy8/rOGbSjIqIiMh5sHbtWv74j/+Ym266ife///088cQTbN68eer+Sy+9xNq1a6def+973+P222/nhhtu4KabbuLxxx+v2O/OnTu57bbbuPLKK7nrrrsYHh6euvfwww9z8803c91113HttdfyyCOPAPD5z3+eX/ziF/z7f//v2bx5M9/+9rd58cUXuf3227n++uu54oor+PM///P5+UPMQTMqIiKy4D300/0Vr/+ba1fQ2ZxkcDzLN7cdqdjmd25dC8D+U2P8aPfJivei6u/v5+mnn8bMeOKJJ6q227t3L//5P/9nvve979He3s6rr77K61//evbv308qdfaZf7/927/NAw88wH333ceLL77IjTfeyG/91m8B8Na3vpXf/M3fxMzYv38/t9xyCwcOHOCDH/wgX/3qV/mjP/oj7rrrLgBGRkZ47LHHSKVSTExMcNttt/HmN7+ZW265pabPHJYCFRERkfPk3nvvDZTS+93vfpdXX32VO+64Y+qa53kcPHiQDRs2TF0bHh5m69at3HvvvQBcffXV3H777VP39+3bx3vf+176+vqIx+MMDAywb9++iss9ExMT/MEf/AFbt27F8zwOHTrE1q1bFaiIiIjU21wzH53NyTnbrF3cwtrFLfUbFNDa2jr1ezwep1AoTL2eXlXXOcdb3vIWHn744dDvMT0Q+o3f+A3+8i//kne/+90ALFq0qGr13v/4H/8jixcv5vnnnycej/Oud73rvFT6bdg9KjuODvPPzx9mOJM730MRERHhkksu4cCBA5w8WVpe+spXvjJ1761vfSuPPvooL7zwwtS1n//85+f00d7eznXXXcdDDz0EwPbt23nyySen7p8+fZp169YB8NWvfpXTp0+f9ezQ0NBZbXt7e4nH47zyyit8//vfr9MnDadhA5XB8Rz7To2RzRfP91BERERYsWIF/+E//AduuukmbrnlFhYtWjR1b/369Tz88MN86EMf4tprr2XTpk188pOfrNjPQw89xGc/+1muuuoqPvaxj521XPSpT32Kd7/73Vx33XU8//zzrF69eure/fffz3/5L/9lajPtxz72Mb70pS9xzTXX8Cd/8ifceeed8/fhZ2EXQ2Gc3t5e19fXV9c+n97bz0/29PPeW1azpC1d175FROT8KRQK7Nq1i40bNxKLxc73cBa82f7eZnbYOddbS/8NO6PieaU1u4sgThMREWlYjRuolPcWFRWpiIiIXLAaNlDxd0EXFaeIiIhcsBo2UFnZ2cTrNyymLa0MbRGRhcT/h+jFsAdzIfD/zkHqwUTRsN/SS9vTLG3XJloRkYXG8zwSiQT9/f10d3fP2xeolIKU/v5+EokEnjc/cx8NG6iIiMjCtXr1ag4ePMjAwMD5HsqCl0gkzkpzrreGDVRePTHCD3ed4i2blrK6u/l8D0dEROoomUyyfv16isWiloDmkZnN20yKr2EDlVzBMTyRI1tQwTcRkYVqvr9EZf417P9BT5utRERELngNHKiU/qv0ZBERkQtXwwYqZ+qo1BapHBvK8N+/v4tDA+P1GJaIiIhM08CBSum/NQcqw6Ujr3ceG6l1SCIiIjJDwwYqy9rTvGPzClYvqi3jZ93iFgCS8Yb9U4qIiMybhs36aUnFuaSnteZ+WlNxzGB4IleHUYmIiMh0DTsN4Jwjmy9SqHE37fYjQzgHB7VHRUREpO4aNlA5MpTh/338VV7oG6ypn5FMHoBsXvVYRERE6q1hAxX/5Ida05Nz5YJx776ht7aORERE5BwNG6jUq+BbvlB6vqctVfOYRERE5GyBAxUz+xUze87MtprZS2b2/vL1JWb2XTPbXb5+R5XnrzazH5nZznK7L5pZU70+SFj1KviWL5ZmVIYmcoxn8zWOSkRERKYLFKhYqTraV4F7nXObgV8F/sbM2oC/BH7mnNsA/C7wsJklKnSTAf7QOXc5cC3QAvxxHT5DJPUq+JYrz6g8/PRBdh8frXlcIiIickaYpR8HdJZ/bwf6gUngbuAzAM65Z4AjwBvOedi53c65F8q/F4BngLVRB14rr04F365f08Wtl3YDMJxRirKIiEg9Baqj4pxzZnYP8HUzGwO6gHcBbUDCOXdsWvP9wOrZ+jOzFuCDwJ9GGXQ9dDYn+cDt60jVWKhtZWcTS9tS/HRPP8MTWvoRERGpp6BLP3HgY8C7nHNrgDcBXyFCwTgzSwJ/D/yrc+4bVdp8xMz6/J/R0fovqcQ8o6MpQToRq6kf5xzxmEdrKq4ZFRERkToLGmhsBlY4534EpSUeM+sDrgHyZrZs2qzKWuBgpU7Ke1f+HjgKfLjamznnHgQe9F/39vbW/YzjQtFxcmSSpmSMjqZKW2qCeeinB4jHjPamOEOqTisiIlJXQdc9DgHLzWwTgJmtBy4FXgH+AXigfH0LsBL44cwOyrMyfwcMAPe7WvOCazSRK/C3Pz/IcwdP19RPrlDEM6M9ncA5aq50KyIiImcE3aNy3MzuB/63mRUpBTh/6Jw7aGZ/DHzFzHYDWeB9zrkcgJn9GXDEOfcZ4B5K+1peAJ4vZ9085Zz7t3X/VAH4m2lrrqNSdMQ9461XLsPzOxUREZG6CLzHxDn3t8DfVrh+HPjlKs98fNrvXwO+FmGM88Iv+FassfJ9Ll8kEfMUpIiIiMyDhq1Ma3VIT3bOkS86EjGPiWyBF/uGODaUqdMIRUREpGEDlakS+jX04Rd7i8eMiVyBR3ccZ89JFX0TERGpl9DpxQuFZ0YiZlMBSxRxz/idW9cQj3k0J0tpzsPK/BEREambhg1UYp7xh3duqKkPzzO6W88cRticjKmWioiISB017NJPPeQKRU6NTpLJFQBob0qoOq2IiEgdNXSg8vKRYQ72j0d+vn80y1d+eoAXDw8B0J5OMDqZJ1+oMZVIREREgAZe+gF4bMdxVnc3s7q7OdLzuXJAEi+nJi/vTJMvFskVHPHaKvOLiIgIDR6oeJ7VlJ6cL1ehTcRKE1PXr+7i+tVddRmbiIiINPjSj1ltBd/8JR4/UBEREZH6auhvWM9qm1GZXkcFYCJb4Hvbj7H9yFBdxiciItLoGjxQgVqO+vEPIEx4pT+j55U26B4amKjH8ERERBpeQ+9RWdKWpiUV/U9wdW8HV61sn3qdisdIJ1RLRUREpF4aOlC567qVNfdhMyrbtjfFVZ1WRESkThp66adWJ4Yz7D4+QjZ/ZkduayrOeLZwHkclIiKycDR0oPLsgQF+sX8g8vPbjw7zrReOkp1W4C3ueRSKjmKxluMORUREBBp86Wf7kWGcgxvXLor0fN7P+vHOLP9sWNpKd2uyplOZRUREpKShAxUzo1BDIZVKdVQ2Lm2reVwiIiJS0tBLP7WmJ+eKDs+MmGdzNxYREZHQGjxQqbGEfqE4VezN99zB0/yvn+xnSJk/IiIiNWvwQKW2GZWWVJye1tRZ1zK5AgNjWZ2gLCIiUgcNvUdl3eJWxrP5yM+/9cpl51yLl6vUFpT1IyIiUrOGDlRuWhct22c2/n6VvAIVERGRmjX00k+tntk/wEuHzz6A0E9V1oyKiIhI7Ro6UPnpnn7+ZevhyM//Yv9pdh4bOeuaZlRERETqp6EDlRMjGQ70j0d+Pl8okpiR9bOqq5lfvWY5PW2pKk+JiIhIUA29R6WW9ORi0ZEvuqnNs76O5gQdzYl6DE9ERKThNfSMimeGc+AiBCu5ol+V9txib87prB8REZF6aPBApfTfKDGFf87P9PL5AIcGxvnko7t5ccYmWxEREQmvoQMVs1KkEmX5xzPj8mVtLGk/ey+Kp820IiIiddPQe1QuW9ZGT1sSz8Kf1dOUjPH2q5efc91PT66lNL+IiIiUNHSgsm5xC+sWt9S1z6n05IICFRERkVo19NJPLU6MZPjmtiMc6B8767oKvomIiNRP4EDFzH7FzJ4zs61m9pKZvb98fYmZfdfMdpev3zFLH79qZjvLbb9uZu31+BBR/XRPP5//8d5I5/2MTRbYc2KUkczZz54p+KZDCUVERGoVKFCx0q7TrwL3Ouc2A78K/I2ZtQF/CfzMObcB+F3gYTM7p5CImbUCXwDuKrc9Avyf9fkY0WQLRUYy+UizH7mCn5589p+wJRnnd25dMy/nCImIiDSaMEs/Dugs/94O9AOTwN3AZwCcc89QCkDeUOH5twPPO+d2ll9/GvjNCGOum1rSk88EKmdvxPU8o7s1RXOyobf/iIiI1EWgb1PnnDOze4Cvm9kY0AW8C2gDEs65Y9Oa7wdWV+hmNXBgRrvlZhZ3zoVfe6kDP9snSsG3anVUAE6NTpLwPFWoFRERqVHQpZ848DHgXc65NcCbgK8wT1lDZvYRM+vzf0ZHR+fjbfDnQiIVfCvvQYlXqEz7tZ8d5Ie7T9YwMhEREYHgSz+bgRXOuR/B1BJPH3ANkDezZdPargUOVujjILBmRrujlWZTnHMPOud6/Z/W1taAwwynloJvyzuauPXSbtrS586axGNGQZtpRUREahY0UDlEaZlmE4CZrQcuBV4B/gF4oHx9C7AS+GGFPr4LXG9ml5df/wHwd9GHXrvLl7XxzutW0pYOPzG0orOJWy7ppjV17rMxz1RHRUREpA6C7lE5bmb3A//bzIqUApw/dM4dNLM/Br5iZruBLPA+51wOwMz+DDjinPuMc27EzD4I/HN5Kekl4P3z8aGC6mpJ0tWSrHu/cc9UR0VERKQOAk8lOOf+FvjbCtePA79c5ZmPz3j9TeCbIcc4bwpFR75YJOF5U2f0BPXj3SfZe3KM37xpNcn42RNTMc901o+IiEgdNHRl2m19g3z68T0cHc6EfnYkk2dgLDtV4G26uGcoTBEREaldQxf78NOTixELvsU8qxiovO+WNVMbdUVERCS6hp5R8WOMKAcd5wuuYmoyoCBFRESkTho8UImenuzvbamk7/Q4rxwbqWlsIiIi0uBLPzZVQj98oJKdZUblF/tP03d6nMuWtdUyPBERkYbX0IHKVAn9CM++cWNP1QDHz/pxzmkZSEREpAYNHahc2tPKfa9fR1MiFvrZVYuaq96Le4ZzpdL8VSZdREREJICGDlSSce+cGihBZfNF4p5VrL/iZwLli0ViXvggSEREREoaejNtJlfg6NAE49nwhzd/5od7eOSFIxXv+XtXVJ1WRESkNg0dqBwenODvfn6I/afGQz1XLDoKRUciVvnP15ZO0NOWipT2LCIiImc09NJP1PTkXPlk5HiVsvtb1i5iy9pFtQ1OREREGntGJWrBN/9k5GozKiIiIlIfDf1NG3VGxQ9UqtVROTw4wVOvnmI4k6ttgCIiIg2uoQOVqAXf/KWfajMqx4cz/HzfAMMTClRERERq0dB7VGKekYx7oYuydTUn+cDr1lVNbfb3rijrR0REpDYNHags72ji3/7S+tDPxTyjozkx631QoCIiIlKrhl76iWoiW+D4cIZMrlDxfrx8WKECFRERkdo0dKCSyRV46fAQJ4YzoZ47dHqch58+yMGByvVXzlSmVaAiIiJSi4YOVMYm83z/5eO8enI01HO5wux1VNqb4ly+rI22dEOvrImIiNSsob9Jp05PDjnxkZujjsqStjRvv3p5TWMTERGRBp9RiV5HZfb0ZBEREamPhv6mtfKnD7uVJDdHwbehiRzf3HaEnceGaxmeiIhIw2vsQKX837AzKlDan5LwKv/5CkXHnhOjDIxmaxidiIiINPQelbjnsawjTXvITa+3XtrNrZd2V72vrB8REZH6aOhApSkZ4zdvWl33flWZVkREpD4aeuknqgP9Y7x8ZBhXZclIMyoiIiL10dCBSqHo+NGuk6E3vW49NMijO45XPSPozIxKseYxioiINLKGDlScczx74DT7T42Fei5XcFUzfqA0o3Lbpd1sWNpW6xBFREQaWkPvUTlTRyXcc/lCkeQsNVTMjJsvqb7ZVkRERIJp6BkVf+UmbHpyruiqls8XERGR+mnwQMXwzCLNqMTnqEr7988c5FsvHKlhdCIiItLQSz8AnlE1e6eaRS1J0onYrG1GMvnQAZCIiIicLVCgYmbdwGPTLjUDlwBLgEuBTwKtgAM+4pz7QZV+3g98FCiU2/4n59y3I4++Dq5Y0U5XSzLUM+/YvHLONnHPlJ4sIiJSo0CBinOuH9jsvzazjwJvAE4D3wDudc49amYbgUfN7DLn3MT0PsxsEfA/gI3OuWNmdjvwdUrBznnzpk1L56XfWMyjUFB6soiISC2i7lG5D/gC0A30OOceBXDO7QIGgbdXeS8D/JzdTqAv4vufN845nnjlBDuOzl57RTMqIiIitQsdqJjZbUAX8C3n3CngqJndXb63BbgMWDvzuXLbB4DnzOwA8EXg3sgjr5PvvHiUH+46Gbh90cHzBwfnrL0S80wl9EVERGoUZTPtfcBDzrl8+fU7gP/HzP4U2A48CeRnPmRmHcCHgZucczvM7NeAb5jZJudcdkbbjwAf8V93dHREGGYwhwcnaAtxKGG+XG3WmyM9+bZLuxWoiIiI1ChUoGJmrcDdwBb/mnNuG/C2aW12UApYZnoLMOic21F+7hEz+yKwBtg9vaFz7kHgQf91b2/vvH3jW8j0ZL8qfqxK+Xxfb1dzDaMSERERCL/0cw+wzTm3079gZsun/f57wBhQKetnL7DZzJaV295KKVA6FHbQ9eRZuIJvhXLb2Cwl9AGKRUc2Xwyd+iwiIiJnhF36uQ/43Ixr95vZeyltlN0BvNOVv53N7AFghXPu486558zsL4AfmFmO0vLQ3c65TG0foTZhC775yzlzzaj868vH2HF0hD+8cz2JOYIaERERqSxUoOKcu63CtU8An6jS/jMzXn8K+FSY95xvYQu+JWLGVSs7WNaRnrVdzCtNVhWKjjlqw4mIiEgVDV+Z9vo1XYRZnWlOxnnLFXPXXvHPAlKKsoiISHQNH6hcuWJ+Mopi5UBFmT8iIiLRNfShhFGcGp3k68/18eqJkVnbxRWoiIiI1KzhA5VHth3ha08fCNx+IlvgQP84o5OFWdvFppZ+VEZfREQkqoZf+pnIFRjNnFOfrio/lXmurJ9rV3Vy+bJ2WkMUkxMREZGzNfy3aNT0ZG+Ouah0IkZa6T4iIiI1afiln7AF36ZmVOYooZ/JFTg+nCGTm32JSERERKpToGIWqo5KPmDBt32nxnj46YP0nZ6oaXwiIiKNrOEDFTNCLf0saUtzx8bFLG5NzdpOWT8iIiK1a/g9Krdc0s3mVZ2B2y9qSbKoZdGc7ZT1IyIiUruGD1SWts9eCj+q+LQS+iIiIhJNwy/95AtFMrlC4H0qL/QN8vkf7+XE8OxnKfqnK6uEvoiISHQNH6g8uuMEf/3EnsABRSZXZCSTZ67Wcc+IexbqHCERERE5W8Mv/fjJO0FTlKfqqMyR9bO0Pc2/e9OGmsYmIiLS6Bp+RsUPOILOfPgBTXyOOioiIiJSOwUq5XgjaKBypjLt7IFKrlDk5SPDHBlUHRUREZGoFKiUZ1QCL/0ErEybKxT53vZj7Dg6XNsARUREGpj2qITco3JtbyfrultIx2eP8WIq+CYiIlKzhg9UbrmkmxvWdNGSDPanKBV8S87ZTnVUREREatfwgUrYU46z+SIORzLmYbNk/nhWmq1RHRUREZHoGn6Pykgmx5HBCXKFYKXuv//ycT79+J4525mV6qhoRkVERCS6hg9UXjo8zN8/c4ihiVyg9vlikZhns86m+Ja2p+loStQ6RBERkYbV8Es/XsjNtEXn5sz48b3nxlVRhyUiIiJoRmWqHkrwOipzV6UVERGR+lCgEnZGpeiIBfyrPXvgNM/sH4g4MhEREWn4QMWmCr4Fa19wLvCMys5jw7zYNxR1aCIiIg2v4feoxD0jOUfxtunu2rxyqjrtXGKmrB8REZFaNHygck1vJ9f0dgZu35QMXnMl5lngoEZERETO1fBLP2EdG8pwcmQyUNt4TDMqIiIitWj4QOX0WJaXDg8FrqPyrReO8NiO44HaxjyPfEGBioiISFQNH6gcHcrw/ZePc2o02CxJoeimUprnsnpRM1esaMdp+UdERCSSht+jUj47MHAwUXCOWMCsn82rgu99ERERkXMFmlExs24z2zrtZ5eZ5c1skZltMbOnzGxb+d6ds/TTZWZfKz+/3cz+sn4fJRovZHpyseiIx1TwTURE5LUQaEbFOdcPbPZfm9lHgTcAp4FvAPc65x41s43Ao2Z2mXNuokJXXwSecs69t9zPslo/QK3CFnzLF4PXUXnp8BC7jo/wtquW0Zxs+MkrERGR0KLuUbkP+ALQDfQ45x4FcM7tAgaBt898wMzWAzcCD/rXnHPHIr5/3UwVfAtweHKx6EjFY4HrrgyO5zjQP042H+xkZhERETlb6EDFzG4DuoBvOedOAUfN7O7yvS3AZcDaCo9eAfQBf21mz5rZv5rZdZFHXifpRIzlHWnSibn/FJ5n/P4bL+WtVwabCPIPL8wrRVlERCSSKDMq9wEPOefy5dfvAD5gZs8DHwaeBPIVnosDNwF/55y7AfjvwLfMLDGzoZl9xMz6/J/R0dEIwwxmZWcTv3HTai7paa173/5eFtVSERERiSbUxgkzawXuBrb415xz24C3TWuzA9he4fGDwGHn3OPl575jZklgDfDq9IbOuQeZtkTU29t7QXzT5wpFdhwdZnFrihWdTXO214yKiIhIbcLOqNwDbHPO7fQvmNnyab//HjAG/KDCs88Cw2Z2TbntTYABh8IOup6GxnP8cNdJDg2Mz9k2kyvw2I4T7D4RbIYnXg5UCir6JiIiEknYVJT7gM/NuHa/mb2XUtCxA3inKxclMbMHgBXOuY8755yZvR/4nJk1AZPArzvnglVamydj2TzPHThNayrOqkXNs7b1l3CC1lFZ1pHmjo2L6Wg+Z3VLREREAggVqDjnbqtw7RPAJ6q0/8yM188CN4d5z/nmpxoHKfjmBypewHmoJW1plrSlI49NRESk0TV8CX2/jkqQxRn/JOSgMyoiIiJSm4YPVPALvgXY8OrXWokFPOvn0MA4n//xXnYcHY46OhERkYbW8IFKmBL6Mc9Y2dlEWzr4npORTJ5JFXwTERGJpOHruqcTMa5a2UFPW2rOtj1tKe7esipw3/7Mi+qoiIiIRNPwgUprKs5brlg6L33HFaiIiIjUpOGXfsI4MZLh8VdOcGI4E6j9mYJvWvoRERGJouEDlYlsga8/18e2Q4Nzth0cz7H14CCDE7lAfcfLecyaUREREYmm4Zd+is5xoH+crubknG2n6qgETE9uTsW467qVdDap4JuIiEgUDR+onMn6CV7wLWh6ciLmsW5xS/TBiYiINLiGD1T8yZEgqzNhS+g758gWihhGMt7wq2wiIiKhNfy3Z6gZFReuhL5z8OnH9/D9l49HHp+IiEgjU6Dil9APEKgsbklx9coOWlPBJqI8z/DMlPUjIiISUcMv/cQ8446NPXS3zL2ZdnV3M6u7Zz9heaZ4zJT1IyIiElHDBypmxg1ruuat/5hn5BWoiIiIRNLwSz9hvNA3yD8928foZD7wM3HPAh14KCIiIudSoAJ86al9fPelY3O2GxjLcnBgPNDGW59mVERERKJr+KUfgPFsgYnc3LMkfoASND0Z4D03rgrVXkRERM5QoEKplkqQxJxCuU3Qgm9A4AwhEREROZeWfijVUgmyOBO2Mi1A/+gkR4cmIo5MRESksSlQAYxgBd+iLP088cpJ/vn5I1GHJiIi0tC0LkF5RiVAoHLF8naWdaTxQsyoxDyjoIJvIiIikShQAd521TISsbknl9YubmEt4Q4Z9LN+nHOYNtWKiIiEokAFWLUoXLXZMOKe4Vzp0MOY4hQREZFQtEcFyOaLZHKFOdv9y9bDfOVnB0L17W+8rfW8n5+8eopn9g/U1IeIiMjFRjMqwN89cxCA37l17aztJrIFJgMENNMl4x7pRCxQ+vNsnt5XClJuWN0Vao+MiIjIxUyBCqXzfgqFuSOJgnN4IfeZvPGyJbzxsiVRhwZwVgn+k6OTLG1P19SfiIjIxUJLP4BnpT0kcykWXagaKvXiecZd160EoO/0+Gv+/iIiIueLAhVK6clB6qjkiy70ssuJkQwv9A0ykQ23ZDRTb1cTcc8YHM/V1I+IiMjFREs/lGZUgpwzWCg6UvFwgcqB/nGe3H2KJW1pmpKxSOMbncxzbCjDb928mu7WVKQ+RERELkaaUaG0RyXIjMqvXL2cOy8Pt9/EXyoqhDhxeaajgxM8su0IJ0cnI/chIiJyMdKMCvCr1xjTzXAAACAASURBVCwPNKOyorMpdN9xP1ApRA9UxsvLRnHPY+uhQVpTMdYvaYvcn4iIyMVCMypAczJOS4BTjsezebL5cHnG9aijMlFOiW5JxfjxrpO80DcUuS8REZGLSaBAxcy6zWzrtJ9dZpY3s0VmtsXMnjKzbeV7dwbo7xNm5sxsc+0foXYDY1mODM59wvHnf7yP77x0NFTfca/0Jy4ESSuqwg9U2tIJlnWkOTI4UVN/IiIiF4tAgYpzrt85t9n/AT4LfAc4DXwD+L+cc9cCdwNfNrOqayRmdhOwBQhX4nUe/Xj3Sf7p2b5Z2zjnKERIT25OxljZ2UQqHm0jLTCVMdSUiLFqUTO5guPYcCZyfyIiIheLqEs/9wFfALqBHufcowDOuV3AIPD2Sg+ZWTPwP4EPRXzfeVHaTDt7G/9+LGTBt1WLmrl7yypWd0c/TygV9+huTRLzjN6uUgzYN6B6KiIisvCF3kxrZrcBXcC3nHN5MztqZnc75/63mW0BLgPWVnn8vwJ/7Zw7dCGdJFwq+Db7Ccf+Usv5KPj2pk1Lp35f1p4m7hmHTk9w82s+EhERkddWlBmV+4CHnHP58ut3AB8ws+eBDwNPAvmZD5nZW4A1zrkvzfUGZvYRM+vzf0ZHRyMMMzi/LP5smT9RA5XhTI7HXznBgf6xyOObLh7zuGFtFxuXttalPxERkQtZqBkVM2ultA9li3/NObcNeNu0NjuA7RUevxO43sz2l1/3At82sw855x6Z3tA59yDwoP+6t7d3XneO+rFH0Tk8qsyolKOYsJVpM9kCWw8O0pKMs6a7JfTYnHP8ZE8/S9vTrF9SCk5uu3Rx6H5EREQuRmFnVO4BtjnndvoXzGz5tN9/DxgDfjDzQefcnzrnVjrn1jrn1gJ9wK/MDFLOh0TMI5XwZt2n0pKM8aE3XMKtl3SH6rvW9OTJfJGf7xtg78n5nVUSERG5EIXdo3If8LkZ1+43s/cCBuwA3ulcafrBzB4AVjjnPl7zSOfRmzYtPWsfSCVmRnMyfH28WtOTM+XU5Onl9wfHs3zj+cNcvbKDG9cuitSviIjIxSDUN69z7rYK1z4BfKJK+8/M0tfaMO99vmXzRY4PZ+hoTtCeTgR+LhbzZ1SiBSp+VdrmaYFKrHw44ejkOVuBREREFhRVpgWODE7wQt8guUL15ZnB8Sz/+GwfrxwbCdV3rSX0/WJv6cSZQMWvyTIZskquiIjIxUaBCrDr+AiP7TgxtcxSiT8j4oVMq457xtUrOyKdEwRnir1NX3ZKxAzPbNbxioiILAQ6lJAzwcdsqzNR05PjMY83XzH7/pfZtKXjbFzaRkfTmeUmMyOV8DSjIiIiC54CFabXUakeqRTL98JWpq3Vmu6WimnNqbgCFRERWfgUqDC9jkr1Nv6Mihdhseyb247Q2ZTgjo09EUZX2Zs3LT0vVXJFREReSwpUYKpsfnGWGRUzoykZIxkLH6kcGZxgMuJ+kqdePcVIJs/brlp21vVVi6KfHSQiInKxUKBCaR/Iys6mqQydStYtbuGBN1waqf+4Z5HrqBwcGGesQhpysejIFoqk4l7V84lEREQudgpUgKtWdnDVyo556z/mWU11VKYXe/M9tvMELx0e4vffeOlZqcsiIiILidKTAzo5Msm2Q4OMZHKhn61lRiWTK9BUIRBJJ0r/6yZz2lArIiILlwIVoO/0OI+/coKh8epByOHBCX6w8wSDs7SpJuZ5kWZUcoUi2XzxrKq0vjNF31RLRUREFi4t/QCnRrNsPTjIhiWtdDRXLo8ftY4KwA1ruiIdSlipKq0vFS/PqChFWUREFjAFKpROUwSYJelnKiNotg231Vy2rC3CqCAZ87hjYw9L2lLn3Ev5Sz+aURERkQVMgQrTK9NWj1TyBb+OymuXYZNOxLhhTVfFe/7ST0Z7VEREZAHTHhXAAhR8q6Uy7fdfPs7f/HDPrJVvw+rtauKDr1/H5RFna0RERC4GClQINqPSnk6wsquJRDz8nyxfKDKeLcwaCFWy7dAgn//xXk4MZ865l4h5tKUTxCMUoBMREblYaOkH6GpJcPXKDtrS1f8cV/d2cHVvtFor/gbcfLFIzAte82R0Ms9IJl8xGCkWHcdHMiRjHt2t5+5hERERWQgUqADLO5pY3tE0b/3HY6VAJWwtlYlsaaNspToqBef4u58fYtPy9nPK64uIiCwUWjcIaPuRIR7feYJ8Ifzm1Vj5JMOwtVTGcwU8s6nibtPFPSPmmbJ+RERkQVOgAhwdmuCfnu3jQP9Y1TYH+8fZemgw0rk6/gbcQiFcoJLJFkgnKp/lY2ak4p7qqIiIyIKmpR9KKb4HB8ZnrXdSKG+0jZKdfNXKdtZ0N9M6yx6YSiZyhYpVaX0KVEREZKFToMKZ4GO27OFC0RHzLNKMSmdzks7mZOjn3nrlsqkAqZJUIlbxZGUREZGFQoEKwdKTi85FKp8PpfTkbKFIMuaFSide1pGe9X5LKs5kTntURERk4VKgwpmCb7PtICkUzwQ0Yb18dJjHdpzg16/vZXV3c6BnCkXHRPnk5GoB0r+5dkWk8YiIiFwstJmWYDMql/S0cNXK9kj9T6+jEtTp8Syf+9Fent7XH+k9RUREFgLNqADtTQnecFkPvZ3Va6lcv7rymTtBxMvpyWHqqMxWQ8V3fDjD4cEJrljeXvGEZRERkYudAhWgNRWvKRCZi78tJUwdlYny3pPmZPX/RQf6x3nq1VP0djYpUBERkQVJSz8BfefFo/zr9mORno3N04xKqnzukFKURURkoVKgApweK+0H+cX+gaptjg5lODk6Gan/uBe+hL4/o5JOVv9flEr4gYoyf0REZGHS0g+lbJ/RyTzZWWYmis5NVZgNa3lHmg+94RKSIVKT/VmSVHy2GZXSvUxOMyoiIrIwKVDhTMG32SY8CkWHF7GOSjxk/RSA113azU1rF00t71RyZulHMyoiIrIwKVCBqWqzs6Un54vRZ1RyhSLHhjK0peOBK9SWgpvZ27Qk4/R2NdGS0v9GERFZmLRHhekzKrNUpi1Gr0w7PlngH5/t48XDQ4GfOTo0Qd/p8VnbdDQneM+Nq7h8WbT6LiIiIhe6QIGKmXWb2dZpP7vMLG9mi8xsi5k9ZWbbyvfurNLHCjP7npm9YmYvmNk/mVlPfT9ONH7Bt9nO+nnjZUu4prcjUv+xmF/wLfhm2h/vPsW3Xjga6f1EREQWikBrBs65fmCz/9rMPgq8ATgNfAO41zn3qJltBB41s8uccxMzuikA/7dz7slyH/8N+G/AvTV/ihqlEzHedf1K2tKJqm2ujhikwLSsn0LwQCVXPhtoLk+8coK2dJwb1iyKPD4REZELVdSln/uALwDdQI9z7lEA59wuYBB4+8wHnHPH/SCl7GlgbcT3r6uYZ6zpbmFRS/gTjoPwZ2xmOwl5pmy+SGKWjbS+V46NsOfEWOSxiYiIXMhCBypmdhvQBXzLOXcKOGpmd5fvbQEuY44AxMxiwB8C/xL2/eeDc45MrlA1PTmTK/DXT+zhh7tORuo/Sh2VXKFIKsCMSiruKetHREQWrCgzKvcBDznn8uXX7wA+YGbPAx8GngTy1R62UorNpyktG32qSpuPmFmf/zM6OhphmMEVio6/fmIPP9h5vOr9TK5AIcShgtN5ntGSqn4KciWlGZW526cTMVWmFRGRBStUXquZtQJ3A1v8a865bcDbprXZAWyfpZu/AlYBdznnKn7DOuceBB70X/f29gafiojgTHpy5fv+ko1fCj+K+++4NHBb5xxNyTgts5zz40slPPrHFKiIiMjCFLYAxz3ANufcTv+CmS13zh0t//57wBjwg0oPm9lfAespBSnZaEOuv7nSk4vlCCZqHZWwzIz7bl8XqG0qHiObL1KsoSCdiIjIhSpsoHIf8LkZ1+43s/cCBuwA3ulc6RvfzB4AVjjnPm5mrwP+HbATeLo8i7HPOffOWj5APZgZZtXTk/29JTVMqPDykWFinnHZsrbonVSwbnELbek4RefwUKAiIiILS6hAxTl3W4VrnwA+UaX9Z6b9/hRcuN+knlnVGZWppZ8aZlR+trefZNwLFKiMZ/O8fGSY3q5mlnWkZ227abmKvYmIyMKlyrRl3iwzKp1NSd5zYy+X1xAUxGMWOOtneCLPj3efmrMyrYiIyEKnQ2LK7n3duqk04pmScY/eruaa+o95FviU41yh1C4RID15z8lRnt47wBsv62FFZ1NNYxQREbnQKFApa53lYL98ochkvkgqHv4UZF/cs6lNuXPx042DBCq5QpHjwxnGJqtmhIuIiFy0tPRTdmwow4mRTMV7h05P8Nkf7WXnsZHI/cc8L/BZP/6MSjJAZdpU+Yhl1VIREZGFSIFK2SPbjvD4zhMV7/l7S6KengywtD3Fis7ZN8b6/Aq5Qc76SZWDGVWnFRGRhUhLP2Vm1Qu+FV3tgcrrNwQ/KLolFWft4mZaUrE526YT5RmVgPtfRERELiYKVMrinlVdmpmqo/IaFXxbv6SV9UtaA7U9M6OiQEVERBYeLf2UxWMeuSpf9vVY+tl5bJhHXz4+tf+kXtKJGG+5YqnqqYiIyIKkQKUsGfPIVzl00A9UqqUvB3FkcIIXDw+RL8y9ofaFvkH+dfsx8gGCmphnXLWyY87CcCIiIhcjLf2UpRIe8cnKcduVK9rZuLQtUBZONf6yUSkYmn3vyaGBCXYdH+HNm5YG7t85N3W4ooiIyEKhQKXsHZtXVr0Xj3nE597XOqt4+aCgINVps4UCiZgFPmTw4acPUigW+e1b19YyRBERkQuOln4CGBzPcmhgfCptOAp/f0uQWiq5vAs1e+OZNtOKiMjCpECl7MjgBFsPDVbc7Pry0WH+8dk+RjK5yP3HY6VAJciMymShGKgqrS+V8BSoiIjIgqRApWzX8REe33mCTO7cwmn+HtuaCr61pdm8unOq7slscvmQgUo8RjZfDHzooYiIyMVCe1TK/MCgUlaOnw0UdM9IJau7m1ndHexgw6t7O0JlGPm1VLL5Ik3JGjfTiIiIXEAUqJT5gUGuQoqyX5m2lvTkMLasXRSq/ZnzfgoKVEREZEFRoFKWKM9K5CrNqBRqr0x7aGCcn+7p59ZLu1m1KNjMSlDXrurgsmVts54ALSIicjHSHpWyhOcv/Zw7o5KIeTQlY4EOCaxmMl/g8OAEo5P5WdtlcgW+8rMDPLN/IHDfbekEPW0p4jWMT0RE5EKkf4KXtabjrOxqqriJ9ZcuX8IvXb6kpv5jAeuoZAtFTo1MMh5i1iVXKDKSydOcjAXarFtNsegYGM+yuDUVuQ8REZF60j/By9YtbuHuG1exorNpXvr397fMGaiU04wTseDLTIdPT/C/frKfV0+MRh8gsLVvkK/89AC7j4/U1I+IiEi9KFAJ4OUjw+w4OlxTH0ELvvl1XFIhCr6lEv4JyuemVoex7+QYAM8fGqypHxERkXpRoFI2NJHj8Z0nONA/ds69Z/YP8IsQe0YqCT+jEq6OCsBkrraibxuXtgFwbCjDRLa2oEdERKQeFKiUTeYKbD00yPHhyXPu5UJWiq2kLZ3gTZuWsHaOWir+jEqYEvr+7Eut1Wmv7u3gbVcto1B07DxW2wySiIhIPWgzbVk8Vj3rJ1uHQKUpGeOa3s452y1pT/O2q5axvCP4XpkzgUrtsyDrl7Ry66XdXNLTWnNfIiIitVKgUuafxZOdEag458jl3VSdlfnWnk7QvjwR6pl4zCPuWcUaMEHtOzXG03v7uWNjD7dc0h25HxERkXpSoFKWrFJCv1B0FJ0jUWNV2kyuwJee2s/ly9v4pcuqpzo757AIheV+/42X1lRH5dhQhqNDmam9NJlcgVOjk/R21bc4nYiISBjao1IWn8rKOXtGpehg7eJmetpqqy3imZHJFaY2y1bzkz39/NVjuxkcz4bqv9Zib6dGJzGDRS1JAL7+3GEe2Xa04lKYiIjIa0WBSlnMM67p7ThnBiEZ93jndb3cGPL8nZnCZP0Uii504NF3epxdNdQ/OTU6yaKW5NT7XrasjUyuwJ6T52ZBiYiIvFYUqJSZGW/atJSrVnbMS/+eZ3hmc9ZR8ffIhC3X//N9A3z/5eORxpbNFxmayJ1VkXbT8jY8M7YfGYrUp4iISD0oUJnD4HiWf91+jIP94zX3FY8ZhQqnM0+XzRcxC1eZFkq1VLL5IsU5AqFK+scmcQ66y8s+AM3JOCs60xwdyoTuT0REpF4UqEzz3ZeO8r3tx866NpLJs/3IMKdD7hmpJObZOZt1Z/JrtoTdUFtLLZWu5iS/du0KNpQLvvk6mhJk80UyORV/ExGR80NZP9OcHJlk5oSEvxRTax0VgHdsXjFnP9l8MdIpzX4Z/Wy+SFMy3MGE6USM9UvOrZvS05ZiZVcT2UKxpsMORUREolKgMk085jE2mT/r2plKsbWlJwOBiri9/arl5OZYHqpkqox+vgCEq8MynMnRmozjzUjBvm51F9et7go9FhERkXoJ9E93M+s2s63TfnaZWd7MFpnZFjN7ysy2le/dOUs/N5fb7TKzH5jZyvp9lNpVKpqWy5de12NGZSJbYHRGIDRTR3PirE2tQbWl45FSqJ1zfO1nB/mn5/pCPysiIjLfAs2oOOf6gc3+azP7KPAG4DTwDeBe59yjZrYReNTMLnPOTUzvw8w84GvA7znnHi/38UngPfX5KLVLxr1z6obUc+nn68/3MZkr8oHb11Vtc2p0knQiRmsq3GTXpuXtbFreHnpMo5N5MrkC3a3Jc+5l80V+urefxa1JrlwxP9lQIiIis4n67Xsf8AWgG+hxzj0K4JzbBQwCb6/wzA1A3jn3ePn13wC/ZmbpiGOou7jnkS+6szJnlrSluG51J63p2lfJ4p7NWUfl4acP8tiOaGnGUZwaLW0SrjSLE/OM5w+eZq9qqYiIyHkSOlAxs9uALuBbzrlTwFEzu7t8bwtwGbC2wqOrgQP+C+fcCDAMrKjwHh8xsz7/Z3R0NOwwI7lqZTtv3rT0rGurFjXzxsuW0J4Ot++jklg5EKomXygVe4uymXZ0Ms9PXj0VOo361GjptOhqgUprKs5IZvblKhERkfkSZUblPuAh55z/7fUO4ANm9jzwYeBJoKZvNufcg865Xv+ntfW1Ocl3TXcLV/d2nLOptF5KMyrVN8r6+2OSEQ5AnMwVeHrfAH2nQwYqI6VApdLSD0B7U4LhTC70eEREROoh1HqGmbUCdwNb/GvOuW3A26a12QFsr/D4QWDNtHZtQAdwJNyQX1s/efUUh06P8+vX99Z8nk7MK1WmrXbwoH8OUJT9MKmEn/UTLmNoOJOjoykxlTU0U3s6zuHTE6W06dfoBGkRERFf2G+ee4Btzrmd/gUzWz7t998DxoAfVHj2WSBhZr9Ufv0h4BHn3AVT+vTZAwN8+olXp5ZDAPrHshwdyhCrwyxL3DOc45xaLb6p8vkRAoKoBd/uvnEVv3Xz6qr3/SWvEc2qiIjIeRB2h+h9wOdmXLvfzN4LGLADeKdzzgGY2QPACufcx51zRTN7H/A35Q20R4Dfrm349eUcTOaKZ1WPjVoptpI7Ny3hzk1LqBbz5It++fzwgUrcM2KeleuoBGdmsxZzW7WomYJzJDSbIiIi50GoQMU5d1uFa58APlGl/WdmvP4pcE2Y93wt+Us7uWkpyrlCtEqxlVRbXvEt72jiw2/agAt/XA9mRiruhZpRGcnkODQwwcquJjqaKm8WXrWomVWLmiveExERmW/6Z/I0/kGA0wOVbMGFPiCwmtNjWfadGjur/5nMLPJm3o3L2lgdIqg4NpThe9uPcXRoYu7GIiIi54EClWn8JZfpKcS5fLFuyx7bjwzzz88fPqdMv+/0WJZdx0cYz0ZLmvqly5ZwyyXdgdv7VXJnKy5XLDr+/pmD/GDna1fbRURExKezfqaJe+fOqNyxcTGl7Te18zfkVqulsr9/jCdeOcl7buylOTn//2vGJkv7WWYLVDzPGJrIRVqOEhERqZUClWmWdzTxnht7WdRypqbI+iVtdes/Xl5Cqlad1k9Pjron5tkDpznQP8Zdm1cGWj7yZ1TmCora06qlIiIi54eWfqZpSsbo7Wqet9mMuWZUain4BqWlowP941NpznMZm8yTSnhzvl97U4KxycI55yCJiIjMNwUq0xSLbuqQPijNcPzVY7vrtj/DnynJVsnMyRZK7xv1AMRUolxLJRcsoGhLx1nZ2RSoHaBS+iIi8prT0s80I5k8X3xqHzeu7eL1G3rIlc/esTrtUWlOxmhLx3FVNnxk87XNqPjpz6VaKnOfTfTLVy4L1K9f9G04k6OrpXKpfRERkfmgQGWa+Iz0ZP+/UWc4Zrqkp5VLeqqfW9SSitHdmpza1BtW1Oq0c1nb3cKvXbuCnrZzDy4UERGZTwpUpjkTqJRmNrJTgcr8HFI40+s39PD6DT2Rn59a+glQnXYkk+OZ/QNsWNI2Z0G3juYEHc21nx4tIiISlvaoTJPwynVUyoGKH7DUq45KvlDkuYOn2XNytC79zbSkLc0dGxfT3TL3zMfgeI5th4bOOtdoNsWiYyIbrjy/iIhIrRSoTON5Rtwz8sXy0k+N6cLn9G/GD185yY6jwxXvP3tggG2HBiP3v6glyQ1rFgXaRzKWnbvY23Sff3Ivj7xwQR90LSIiC5CWfmaIx7ypmZTlnWnee/NqWtP1+TN5XukAwEyVrJznDw7Skopz7arOurzfbPzquC0BA5WWVJzhCdVSERGR15YClRl+93Vrp+qdpOIxlrTPfpBgWOmEx0Su8hJKtlCkq4bZm/FsnoefPsjly9q5fcPiWduOlqvSBg1U2tMJTo5MUii6qb+PiIjIfNPSzwzpRGwqy2ciW2BoPFfXQmdNiRiZCns9nHNkazxXKBHzGMnkpyrOzmZqRiUZLBBrb0rgHIH6FhERqRcFKjMcG8pwsH8cgJeODPHFp/bRP5atW/9NyRgTucI5tVTyRYdzte2HiXtGzLNAWT+rFzWzeVUn8YDv5xd9q8fyz9GhCYbGtYwkIiJz09LPDD/adZLBiSz333Hp1GbaetVRAVjR2UTMM4oOpmc9T53zE4++rGJmpOJeoDoqV63sCNX39KJvtRjJ5PiHX/TR2ZzgfTevCXQmkYiINC4FKjPEYzavdVS2rF1U8bpnxqbl7Sxrn7uk/WyCBiphrelu5v47LqE54FJRNdsODVEoOvpHs+w8NsIVK9rrNEIREVmItPQzQyLmkSsUcc6dqaNSxxmVapqSMd521bKav7iT8RiTVTbr+jK5Al/92QGeO3g6cL+JmEdLKo5Z9KDNOcfuEyN0NidIJTx+tre/6knSIiIioBmVcyRihnNQKLq6l9AHODw4wbZDg9ywpoul7em69et742VzV7Ydm8xzcmRy6vDFoI4PZ8jkCqzpbok0NjPjfbesYXgix/HhSZqSMbTyIyIis9GMygzxcnXaXKEUqPgbVOtlIpvnlWMjnB4/e4PuK8dG+Jethzld48bdFZ1NrJjjROSxcmpy0GJvvsd2nOD7L9d2knQi5tHdmuKKFe2sW9xS0wyNiIgsfApUZvBPOC44x7+5dgUPvPHSuvbvn3A8sxz9iZEMe0+O1dx/oegYm8xTnGVJZTRksTdfR1OC0cl8pHTtV0+M8vgrJxjPnp3efGRwgt3HR0L3JyIijUGBygy3rV/MB19/Ca3l/Rj13p/SVN6MOrPo21jIAmzVPPXqKT77o72MzFLvJGz5fF9HuZbKcCZcLRXnHM/sH+ClvqGzrheKjm+/eJTHdp4IlFItIiKNR4HKLPadGqPv9Hhd+2xKlAKVmftDxibzJOMeyRoPQEzF5z5BuZYZFYChkLVUjgxlODaUYdPydpqTZ94z5hk3r+tmIltg9/H5OahRREQubgpUZjg5Mskv9g8wNJHjsR3HeXL3qbr2n54KVM5ePhnL5kPPcFSSKvc/WeU8IYArl7fzliuW0pwIl2ocNVDZfrg0k3L9mq5z7q3rKW3MPTI4EapPERFpDMr6meHYUIYf7z7F4tYU2UIxcOXWoGKecfMli1jcmjrr+uhkniVttWcBnZlRqR6oLGlPsyRCxlFHU4KOpgRht78eH87Q3pRgUYVTnVtTcTqaEgpURESkIgUqM8TLxd3yxSL5gqtrsTffbZeefWCgc47b1y+uuZgaBFv6yUcMwDqaE3zg9nWhnsnmi/SPZVm/pLVqmxWdTew4OszYZL7mPToiIrKw6FthBn/zbCZXpFB0NZ29E5SZcU1vZ136mlr6qTKj4pzj00/sYf2SVn7l6uV1ec/ZxDzj16/vnQoAK9mwtJXmZAyVfhMRkZkUqMzgz6D4WTnzUZX28VdOcLB/nPfftrbufS9tS3H/HZdM7YWZaSJXoFB0UzMvYe0+PsLhwQnesLEnUA2UmGesWtQ8a5tLe1q5tKf6jIuIiDQubaadwV8SmcwV6W5N0pqufyw3mSsyMJadqny76/gIn/nhHvafqr2OSrxc6r5akbqoGT++A/3jPH9w8Jz06mpOT/ucc5l5orSIiIhmVGZoTsRYu7iZpe0pbt+weO4HIpheSyUR8xjJ5JnIFkjUmJoMpS/7w4MTJONexc25UavS+jqaz2T+TE81ruYbzx8mETN++9a1s7b72d5+th4a5AOvW1dziraIiCwc+kaYoaslyTuv62XD0rZ5e490+Ys4U65OO1ae5WgN8MUfxD89e5if7umveG+sxhmVMCnKE9kCQxM5egJkM8U9YyJb4PhwJtK4RERkYVKgUsXgeJaf7e3nxEj9vzhnVqc9EzzUnvVjZnQ0xRkcrxxIjNb4XlOBSpX+p/ODjqXtqTlaMnU+0WGlKYuIyDSB/lltZt3AY9MuNQOXAEuA9cBfASkgDXzJOfdfq/TzfuCjQAFwwH9yzn078ujnQa5Q5PGdJ5jIFdh7coz2dKIu9U2ma5pR9G10Mk86EatbsJN69wAAF7hJREFUzZZFrSn2nRyjUHTn7FW5bnUn65e00lkOOMLyA5UgZfSPlQOVZR1z//2WtqeJe6Z6KiIicpZAgYpzrh/Y7L82s48Cb3DODZjZZ4GPO+e+aWaLgJ1m9i3n3MvT+yjf+x/ARufcMTO7Hfg6pWDnguGZsf3IMH5CSzJe/zoqyzub+LVrV0zNNIxN5mmtw2yKr7slyZ4To5wez55TWC4Vj5Fqjf5eqbjHlSvaWR4g+Dg+nMEzo6d17hmVmGcs60hzdChDsejwajyxemgix7GhDBuXtuqEZhGRi1jUTRH3AX9a/t0BfhGQFiALDFR4xgMMaAOOlZ/pi/j+88azUrBSLGegzEd6cmsqflYBtF+9dgWFWU47DsuvADswdm6gsv/UGD1tqch7VMyMX75yWaC2mVyBnrZU4JmilZ1N9J2e4NToZKTKuVDaTPzS4WF+tPsk2XyR0cnF3LBmUaS+RETk/Av9bWVmtwFdwLfKl34X+Bcz+3OgB/iQc+7YzOecc6fM7AHgOTMbAJqAN1d5j48AH/Ffd3R0hB1mZGZGPGZk8/MXqEDp5OBC0ZGMe+cEE7Xqbk3S3pQ4J/gZz+b5xvOHuXJFe+Bgoxb3bFlNPmBqMsBVvR1sWt5OZ3O0ZSmA5w8N8sNXTtKWjtOcjPHk7n5WdjYHWn4SEZELT5Rv4fuAh5xz/iaFPwH+1Dm3GrgS+Aszu2LmQ2bWAXwYuMk5t6bczzfM7JwDYJxzDzrnev2f1tbXthjY9LL58xGoFIqOv3psN4/tOE6uUGRwPBvqC30uS9rS3Hf7OjYtbz/r+smRSQB62moLjF4+MszXnj4QaENtmH037ekEXS3JmpZqrlzRzg1runjfLWv4P65Zjmew56ROZhYRuViF+hY2s1bgbuCL5deLgXc65x4GcM7tBX4GvK7C428BBp1zO8ptHwHagTWRRz9P4l7pz7J+SetUhk49xTwjGfeYyBU4NpThS0/t58XyCcPzqV6BSq5Q5MTwJIMT2aptdh8fYduhwcDF3nwjmRy7jo+ELv42ksnhnCMVj3HHxh7SiRhL2tK875Y1vG79/NTDERGR+Rd2uuAeYJtzbmf59WlgzMzuhKnA5WbgpQrP7gU2m9mycttbKS09HYoy8Pl0dW8Hd2zs4deuXRG5MNpc0okYE7nCVLpwvd9n78lRfrDz+FnLPyfqFKgEqaXyQt8QP959kljI2ZHnDg7y/71wlFOj1YOgmXKFIl97+iDfeemcFUe6yvt1CkXHsSHVaBERudiEDVTuA77gv3DOFSjNsPw3M9sG/Aj4pHPupwBm9oCZ/Vm57XPAXwA/KLf9n8DdzrkL7ttjy9pF3LCma17foykRYyJbqLkAWzVHBjNsOzTE4PiZL/yTI5N0NidIxWubJZorUHHOcXwkw5K2dOjsnUsWtwCwL8RxAjuPjjCRLcyaifSdl47yz1sPh57hERGR8yvUt6Nz7rYK1x4FbqjS/jMzXn8K+FSY9zxffr5vgIGxSd521fycMNyU9Dg9Xqz57J1qpmf+dLemKJY37nbVsFHV196UwKx6oDI4nmMyV2RJgEJv/3979xrb1nkecPz/nENKJHUXZd2vtuzYcezYjePY6eKlTbomzXrvUgTr0hUYin0o0G4Yhi4rumIXDPvSDdtQYEObpuhtyLouSS/r1q1pkzqNE8d2bDeJL5ElWZYt2bpLFMXbuw/nkD6SKNGWKJGJnh9sgOS58LyPDg8fvue9LNRcHaTEZ3Hh2jT7u3L31jHGcPziGCU+i1ubK5dcr7UmxLmhaX49OMmetvzMVK2UUmrt6ci0WTx/7iqHz1/jzaurnyRwKUG/TSyRynzZl+W5LUxduZOojMw4NSqWJTyyvz0viZdtCeWlviUTld4RJ24t7mizN7vvjnCIyxNRIrHcg8r1jUQYmY6xq6Vq2Zqinc2VBEtsjvWNkcpjV3CllFJrSxOVLKbcUVd9qxx0bDn3bN3Epw9tJpE0BEvyNyptmtN7BkZuoq3Hzbj3lk3c070p67I3r87gs4SOcNmK9t1VV4Yx0HstknPd4xfHEIHbc9SS+G2L21urmZiNc25YewEppdRbhc6enMVaJihp6Vs9H3lHC/Fk/n/h+22LyoCf0RmnAe3JgXFm5pLs76pdNKz+SnTXLz1p49b6cpqrAyueBbmrrowW9xbQclIpQ9DvY3tjRabdzHL2tFXzSt8oR/tGVzRirTGGIxdG3V5JIAIP3ta06sbJSimllqaJShbpsVPWcuT1aDzJyEyMmpCfUJ5mTV5oV2sV6V6+vx6cZHI2zoHN+RulNRp3GgOHFwxYl6t2I5dQiY+H72zLuZ5lCQ/c1njDXZmDJTZ722tIGWewPZ99439gYww/P3uVE/3jlJf6CPjnJ1HxZApbZNVD/yullJpPE5Us0l9gwtp96fSPRvjRycts3lTGO7vr8j46LTi9l8Cpebg2NUdrbTBv894kU4ZvvNBLRcDPI/vbMvuNxpME/Plrb2OMyXrM0XgSnyX4bOumyrTSMVVOX5rkRP84rTVBPrinZV5tTzJl+MGrg5T6bN67syHvt/GUUmoj0ytqFukalW2NS9/eWK30DMo9V2c4e2Vqzd4HYDQSI5EybCrP3zDytiV015czNBll0B2fZC6R5KvP9/DsG8Or3n80nuTJoxf55flrWZc/d/YqT7zQm+nefbPmEkmmorlH1k3b0VTBXZtr+dDelkW3pIxxelSdHZriP49fYi6RXNExKaWUWkwTlSw2byrjgdsauesGuseuVKnn1kG+uyanTczG+faRPn7iDoSW77YUe9udsWaO948BTg+ceNJkBllbjVKfxeRsnDezNHwdnYnx2uVJqoJ+QivoLTUbS/L4L3t5+sRgzqkLLo3PAs5UAHdvqcs6pYLPtnjfbU3saatmYGyWZ04Mrni8lmg8yelLEwxPXR9eKN9TLCil1FuJ3vrJor4iQH3F2k5iF/TcHlmrRCXotxmenMs8r89zolJbVkJXXRnnh6eZiFxPKrZsWllvHy8RoauujJMDE4zNxOYlPy/2jGCMcxtnJbeygiU2t7dVcaRnlMNvjvCb27L3Xnr14jg/e2OYd22vzzn2imUJ997i7OfExXF+eHKQ9+9uvqHbQMYYzg9Pc3pwgv6RWVLGsKetmvrtzjn45NGLJFKGfR217G2vXrOJMpVSqhjpFS+LSCzB1w9f4OXe0TV7D2+islbD9Jf4LCrd3jC3t1Wtalbipextr8YYONY/Rs+1GRqrAlQE8vM+Xe4ota8OjGfGPhmeinLmyhRddWU0r2CclrQDXWGaqwMc6xujN8souD1Xp3n2zDA1IT+3NNzYLUARJ1nZ2VzJeCRONJG7FmQiEuepE5f44cnL9I/M0hEO8Vs7Gzi4JQw47Ytua6ki4LM5fP4a33ihl9OXJlY0FkwimeLKRJS+kRnODk1lLbdSShUbrVHJ4vXLk4xH4pwamMg0SM037y/tstL8T3yYFi4rYWYuwb3b6vPWkNarvTZEd305s/EksUSK7vr8zXTdVhuiKujneP84I9MxPnpHK796cwSAu90v8pWyLOGBnU1860gf//PaFT5xoCPT+2poMsqPT10m4Lf50N6Wm5qYUkS4f0cDc4kUwRIbYwwjM7ElG0tPzcXpG4lwa3Ml92ytW9QDzLKEu7fUcVdXmJMD4xy5MMpPXxvitcuTPLwvd88ogCsTUY72jdI3EiHmSZ5aqoN0usngSxdGGRyfpbUmSHs4xKby0ps+X5Ipw3Q0wXQsgTEGv225/50BAtfi/FNKvf1popLFejUHOLglzNmhKcrWqHsyOLdnLlybYWI2npe2IwuJCO+/vZlX+kY5NzRN96b8JSp+2+J3D7RzrG+c6pAfYwwNlQFCJT7qK1d/a64q5Oe+HfX816krvHpxgoNbwhzrH+MXZ67is4QP3N5MdejmY2ZZkkluekciPHX8Eu21Ifa0VxOZS3JhZIYtm8rY2VxFa02IRw92ZqY8WIptCXvba9jRVMmx/rF5NXJnh6aIJVIE/BalPpu5RJKBsVl+o7sOn20RS6Q4PzxNU1WA9toyykt9lPgsygPXz7upaJz+0Ygzx9I5CJXYtNaE2N1aRVttCGMMY5E4cwknIY3GU4xHYoxFYrxrez2lPpvB8Vm+98rAomP3WcJn3t0NwMXRCGeuTFER8FER8FMZdI6l1GdnxsIZm4kxGY0TjaeIxpNE40lm40naakNscc+vn58ZZmI2TqnPosTnJEQBv01XXVkmKRyajGKJ4LedbuMmBUljqAz48NkWc4kkl8ZmSaSc7urJlMEYSBnD7tYqRITJaJwrE1F8luC3LXy2YIsz+3n63IjGk8y5CaBxu76njFOjmS7Ttek5ZmOLG1lXhfxUujWQ45EYKYMzkadcHx4h6Lfx2xbGGCZnExic/RtjMIAxUBPy47MtEskU47PxTH9FEcESpwdjZdBJFuNJJ65eBrBEMrW76bgv5LetzK3qSCxBwq3Z876f35bMKNGRWCIzMaq3DtBbpqkFDeLT+wqV+LAtIZkyzGY5FrheGx1PpjJ/Ay9LyCT/c4nkvDGr0u9jW5LpqRiNJzNlml/u+WXKVqEZ8Fn43DJFsvytRSDgs7EsIbWgTN4cPn28iSXLdP36EkukSKSyr+MtUyrLEA4+y8p0DFiqd2Wx0EQli/TJX7fGA3kd2BzmwObV1Qzkkh68rndkZk0SlbQ7OmrZ2VyV167JAKU+O3MbBMh7vLY3VlLqs+kMhwDYVF7K9sYKbmupWtWtpbSKgDMg3dmhafpHnZF2LRFqPQlQriTFK+C3uXvL9S7WkViC/z59JevFdWtDBS3VQVpqgnz60OZlx+u5b0cDh7ZtYnB8lv7RCH0jEc4OTWVqyIyBb7zQm3Xbd3TUUF9hU1NWwh0dNZSV+rAE4klDIpnCsiRzEbwyGeXUpYlF++isC/Hhva0AHLkwyuuXJxetIyKZROXyRJThyblFF+CKgC+TqPzbSxezXqB/72AHdeWlTEcTPH1iMGuZdrdWOcc7EeVHJy8vWl5bVsIn7+4E4Hj/OC/2jCxaZ1tDBQ/tdqaseLFnhHNDixuGH9gczpzfz7w6mHUk6Yd2N7HNvf34+OELWY/30YMdhMtLGZ+N881f9WVd53P3bwWcnoY/PrW4TOHyEh496JTpWP8YR3oW3/r2lulnbwznLNP3XhnIWaavPb98mcYisSXL9Efv2QY4k5hm+zt5y/RKX+4y/e/rQ3kp078+17NsmUZvoEw9N1Cmo32jeSnTXCKV92t3PmmiksWOpgoMJq+3MQplf1ctwRKbXS1Va/5exXyiLyfdFgac201ttaG87buuvJQHdzXxzq1xzlyZojLgpyMcylusgn6b39nXRiSWYC6RcseXsWipCWYmoLQtuaFBBf22RUe4jI5wGfdsdX6tpX9kWZZwcEsYn+X8snRqFPzzZuMuL/VxaImGyWn7OmrY2VzJVDTBVDTOZDRBPJGiytN+6tamSpqrAwT8NqU+i6DfJlBiE/LE7JH97Znai1gyRSzh/PpM104YY7i7O0wi6daWGIMl839plgd8vOfWBvy2hW05NQ+2JQjXE6umqgAP7W4inkyRSBoSqRTJlNMrLa25OpCZbd1yazAsSwh7EtBdLVW01cw/r1LG0OiZ8XtXSxWRmPPrN513GmMybctEhH2dNQjOe+DWlIiQ+YUdKrE5sDmMweD+wxgwXP/FXBPyO23LmF8T4p1vrKU6mPW2d7j8epm66sqoCPgzAy6mU0JvmbY3VjITS8x7H2Bee7k7Omoy23oHb0z/nQJ+mz3tyzdmrw76562Tfj9vR4WmquDi/Zj5vSE7w2VZOzd4y3RLQwWR2sU1JtWe0bEXNr43GASh1FumHA30q4P+rPvx1sA3VgYy6xhPnZW3M0hnuCxr78gGz6Sx+RitfC3JjY7qWUitra1mYGBxlbJSSimlipeIXDLGtK5mH9rrRymllFJFSxMVpZRSShUtTVSUUkopVbQ0UVFKKaVU0dJERSmllFJFSxMVpZRSShUtTVSUUkopVbQ0UVFKKaVU0dJERSmllFJFSxMVpZRSShUtTVSUUkopVbQ0UVFKKaVU0dJERSmllFJFSxMVpZRSShUtMcYU+hhyEpE54Gqhj2OdlAPThT6IIqbxyU1jlJvGKDeN0fI0PrmVA0FjTOlqduLL08GsqdUW8q1ERAaMMa2FPo5ipfHJTWOUm8YoN43R8jQ+ubkxql7tfvTWj1JKKaWKliYqSimllCpamqgUny8X+gCKnMYnN41Rbhqj3DRGy9P45JaXGL0lGtMqpZRSamPSGhWllFJKFS1NVJRSSilVtDRRWSci8o8i0isiRkT2eF4vFZF/FpFzInJKRL7lWbZVRF4QkbMi8rKI7CzM0a+PZWL0PhE5JiInROS0iHzSs6xeRH7ixu+0iBwqzNGvPREJiMhT7vnwqoj8VES63WVLxkFjlInR1z2vHxaROz3bhUTkuyJy3l3nY4UrxdpaLkaedd4tIkkR+ZznNY2Rs0xE5EvuslMi8qxnuw3xWcsRn/0i8qKIHBeR10XkTz3brewcMsbo/3X4DxwCWoFeYI/n9b8H/onr7YUaPct+Bvy++/hjwMuFLsd6xwgQYBTY7T7vBKJAhfv8ceBL7uM7gQHAX+iyrFF8AsD7POfKZ4Cf54qDxigTow8APvfxbwO9nu2+CDzhPu4ChoFwocuz3jFyn1cBLwE/AD6nMVp0Hn0W+D5Q4j73XrM3xGctR3xOAB9wH9e658mtqzmHCl7gjfZ/wZdwGTAJVGZZr95dlr6wCnAF6C50GdY5RgKMAIfc57uBS56LxPSCC8VLwP2FLsM6xWlf+st2uThojK4nJJ7X64C45/P1a+CAZ/mTwB8U+vgLESPgmzhJ3RMLEhWNkfN4ANi2xHob8rO2ID7HgUfdx21uvBpXcw7prZ/C2oJTW/CYiBwVkedF5D53WRtw2RiTADDOX7UfaC/MoRaGW+6PA98XkT7gl8AnjTExEQnj/Fq54tmkl40To88CTy8XB42RE6MlXv9x+vOFE48+z/JeNmCM3Kr4lDHmmSzrbfgYiUgl0AB8UESOuP8/DrDBP2vez9mngL8SkX7gLPCYJyYrOofeEkPov435gA7gNWPM50VkL/DTt3tblJshIj7gC8BHjDHPue0KnhGRXcCG7VsvIo8B3cB9QLDAh1OUFsTI+/ongIdxbjVuaN4YiUgjzmft3oIeVJFZcB4FcK7bQWPMXSLSCbwgIm/g1BxsOFk+Z58H/swY8x0R2Qz8QkSOGmNeW+l7aI1KYfUDKeDbAMaY48AFYBdwEWhyv6gREcHJPPsLc6gFswdoNsY8B2CMeRnngrDXGDMCJNwLbFonb/MYicifAB8BHjTGRJaLg8bIiZHn9Y8DfwG8xxgz5NmkH+dHQ1onGy9GdwBNwAkR6cVpF/dFEfkbd5MNHyNjzCjO7Z1vARhjeoHDwJ0b8bO2MD4iUgd82BjzHQBjTA/wIvBOd5MVnUOaqBSQMeYa8H/AewFEpAungdHrxphh4BjwCXf1jwIDxpjzhTjWAkonbDsA3JblW4Az7vJ/B/7QXXYn0AL8ogDHuS5E5I+BR3C+aMc9i5aLg8bIef1h4K9x2gwsvDh6Y9SFU6vw1LoccAFki5Ex5kfGmAZjTKcxphP4HvCXxpg/dzfb8DFyfRd4wF2nFtgPnHSXbZjP2hLxGQNmROTd7jp1wF3AaXf5is4hHZl2nYjIvwAPAY04jUOnjDHdbtXY13Aa96VwLgz/4W5zC06DtjBOw9pPGWNOFeDw18UyMXoEeAwnPhbwt+mMXUQacBr/dQEx4DPGmGez7f+tTkRacRK3HmDKfXnOrYJeMg4ao0yM4jgN0kc8m9xnjBkRkTKcHhv7gCTwBWPMk+t46OtmuRgtWO8J4IQx5h/c5xoj5zwKA18HNruvf8UY8xV3uw3xWcsRn/uBv8O5ReYHvmqM+bK73YrOIU1UlFJKKVW09NaPUkoppYqWJipKKaWUKlqaqCillFKqaGmiopRSSqmipYmKUkoppYqWJipKKaWUKlqaqCillFKqaGmiopRSSqmipYmKUkoppYrW/wO4D/flPUiG6AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_figure(graph_val[0][150:], loss_val[150:])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dear professor, here's my current progress:\n",
        "1. I implement epsilon_Loss_penalty function. Now, the training process will iterate until it counts 5 consecutive times the loss difference between the current loss values with its previous loss < 1e-5. \n",
        "\n",
        "    Then, it finds the normalization constant c and apply to model.output weight layer and its bias. Then the algorithm updates the normalized output weight layer for the model. \n",
        "\n",
        "    The result: the training process only takes < 300 epochs to converge to a loss value of 78.7160. This occurs consistently after several re-runs. The problem: the constant c is 1 from my normalization function. I'm looking for a normalization from the PyTorch library to use instead. Please let me know if I can use this function for the normalization part: [PyTorch normalization function](https://pytorch.org/docs/stable/generated/torch.nn.functional.normalize.html)\n"
      ],
      "metadata": {
        "id": "3GZ9S2gz-JM1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjtOBhyZLspO"
      },
      "outputs": [],
      "source": [
        "# PLOT DATA\n",
        "def plot_figure(x_train, y_train, x_test=None, predicted=None):\n",
        "    plt.clf()\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.plot(x_train, y_train, '--', label='True data', alpha=0.5)\n",
        "    if predicted != None:\n",
        "        plt.plot(x_test, predicted, '--', label='Predictions', alpha=0.5)\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "yOAl63W0qaq_",
        "outputId": "0966c5cc-146b-4f15-d330-522a5eafc1e6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_figure(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el6VL83Qddfj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPn9pDNlm88W"
      },
      "source": [
        "# HOW TO TRAIN THE NN MODEL:\n",
        "1. Reset adam_optimizer: \n",
        "```adam_opt.zero_grad()```\n",
        "2. Calculate loss\n",
        "3. Update the optimizer: \n",
        "```adam_opt.step()```\n",
        "\n",
        "\n",
        "        weight = weight - lr * gradient\n",
        "\n",
        "-> use lr and gradient to \"improve\" weight layer. \n",
        "\n",
        "Explanation:\n",
        "```adam_opt.step()```: Update the model's parameters \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VfCCteSFrC8"
      },
      "outputs": [],
      "source": [
        "model_p = Nonlinear_2(4)\n",
        "adam_opt = torch.optim.Adam(model_p.parameters(), \n",
        "                                    lr=learningRate, \n",
        "                                    betas=(0.9, 0.999), \n",
        "                                    eps=1e-08, \n",
        "                                    weight_decay=0, \n",
        "                                    amsgrad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUjHqCmkNRzB",
        "outputId": "a8eba8cf-5f24-45cf-f588-85f437d798fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.5799], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x_i = torch.tensor([-2.5], requires_grad=True, dtype=torch.float)\n",
        "u_xi = model_p(x_i)\n",
        "print(u_xi)\n",
        "u_xi.backward()\n",
        "u_prime = x_i.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A2qd2M1OXQ5",
        "outputId": "224f75ed-733d-4edc-f3ad-3d78dac7f66a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Hidden layers: \n",
            "Parameter containing:\n",
            "tensor([[-0.4363],\n",
            "        [-0.2556],\n",
            "        [ 0.6323],\n",
            "        [-0.8666]], requires_grad=True)\n",
            "\n",
            "\n",
            "- Hidden layers gradients (derivative of Loss w.r.t model params): \n",
            "None\n",
            "Parameter containing:\n",
            "tensor([ 0.9463, -0.8034, -0.8181,  0.5812], requires_grad=True)\n",
            "\n",
            "\n",
            "Output layers: \n",
            "Parameter containing:\n",
            "tensor([[-0.3294, -0.3360,  0.0453, -0.0283]], requires_grad=True)\n",
            "None\n",
            "Parameter containing:\n",
            "tensor([0.4850], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Access the model's parameters:\n",
        "# Model's hidden layer weight and bias\n",
        "print(\"- Hidden layers: \")\n",
        "print(model_p.hidden.weight)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"- Hidden layers gradients (derivative of Loss w.r.t model params): \")\n",
        "print(model_p.hidden.weight.grad)\n",
        "print(model_p.hidden.bias)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Model's output layer weight and bias\n",
        "print(\"Output layers: \")\n",
        "print(model_p.output.weight)\n",
        "print(model_p.hidden.weight.grad)\n",
        "print(model_p.output.bias)\n",
        "\n",
        "# model.zero_grad()\n",
        "\n",
        "i, o = (x_train[0], y_train[0])\n",
        "i = Variable(torch.from_numpy(i))\n",
        "o = Variable(torch.from_numpy(o))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "YI4vusv8w_40",
        "outputId": "9797e6a2-cd0f-49fd-db59-c2fda0e16171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear(in_features=1, out_features=4, bias=True)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-d51b9355ea65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgiven_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_pp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOWER_BOUND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUPPER_BOUND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_POINTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "model_pp = copy.deepcopy(model_p)\n",
        "print(model_pp.hidden)\n",
        "loss = epsilon_Loss(given_fn, model_pp, LOWER_BOUND, UPPER_BOUND, N_POINTS)\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBY2VlCqSVjE"
      },
      "outputs": [],
      "source": [
        "model_pp.hidden.weight.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "ELHbc3btUq9f",
        "outputId": "1c256f77-a69d-4303-85c0-4c502ab692c8"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-123-52a0569421b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA_91qCHWhjs",
        "outputId": "20a262ba-f9a2-4cf1-b0c4-110596eb2f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Hidden layers: \n",
            "Parameter containing:\n",
            "tensor([[-0.4363],\n",
            "        [-0.2556],\n",
            "        [ 0.6323],\n",
            "        [-0.8666]], requires_grad=True)\n",
            "\n",
            "\n",
            "- Hidden layers gradients (derivative of Loss w.r.t model params): \n",
            "None\n",
            "Parameter containing:\n",
            "tensor([ 0.9463, -0.8034, -0.8181,  0.5812], requires_grad=True)\n",
            "\n",
            "\n",
            "Output layers: \n",
            "Parameter containing:\n",
            "tensor([[-0.3294, -0.3360,  0.0453, -0.0283]], requires_grad=True)\n",
            "None\n",
            "Parameter containing:\n",
            "tensor([0.4850], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(\"- Hidden layers: \")\n",
        "print(model_p.hidden.weight)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"- Hidden layers gradients (derivative of Loss w.r.t model params): \")\n",
        "print(model_p.hidden.weight.grad)\n",
        "print(model_p.hidden.bias)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Model's output layer weight and bias\n",
        "print(\"Output layers: \")\n",
        "print(model_p.output.weight)\n",
        "print(model_p.hidden.weight.grad)\n",
        "print(model_p.output.bias)\n",
        "\n",
        "# model.zero_grad()\n",
        "\n",
        "i, o = (x_train[0], y_train[0])\n",
        "i = Variable(torch.from_numpy(i))\n",
        "o = Variable(torch.from_numpy(o))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jQeEbaMHkeA",
        "outputId": "32d5985d-7d0f-4048-c555-7ef3785149ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 0., 0.])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkmedTJ6J1Hr",
        "outputId": "01a83e6e-280c-4a72-d9c4-bff991258866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)\n",
        "\n",
        "t = 3*a**3 - b**2\n",
        "\n",
        "external_grad = torch.tensor([1., 1.])\n",
        "t.backward(gradient=external_grad)\n",
        "\n",
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyuNXuz-iLBZ"
      },
      "source": [
        "https://neptune.ai/blog/pytorch-loss-functions\n",
        "https://stackoverflow.com/questions/53980031/pytorch-custom-loss-function\n",
        "https://stackoverflow.com/questions/65947284/loss-with-custom-backward-function-in-pytorch-exploding-loss-in-simple-mse-exa\n",
        "https://www.youtube.com/watch?v=ma2KXWblllc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0PS0efCiJCp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PyTorch_.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "8c21580189d9a9d7f1e3fef63ff70de58083dda94aa35d8ed937f03fa405217e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}