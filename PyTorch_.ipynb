{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the NN is 1D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "INPUT_SIZE = 1\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "LOWER_BOUND = -10\n",
    "UPPER_BOUND = 10\n",
    "\n",
    "def quad_fn(a, b, c, x):\n",
    "\treturn a*x**2 + b*x + c\n",
    "\n",
    "def x_square(x: torch.tensor) -> torch.Tensor:\n",
    "\treturn x**2\n",
    "\n",
    "# DEFINE GIVEN FUNCTION V(x): ax^2 + bx + c\n",
    "given_fn = x_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM LOSS FUNCTION:\n",
    "def epsilon_Loss(v_x, model_u, lower_bound, upper_bound, n_points):\n",
    "    \"\"\"\n",
    "    GOAL: Epsilon function evaluated at u using discretized estimation\n",
    "    minimizing Epsilon(u) = \n",
    "    \n",
    "    ARGS: \n",
    "    n_points (int): number of discretized points on the interval [-L, L]\n",
    "    e.g.: -(L)|---|---|---|---|(L) interval has n_points = 5\n",
    "\n",
    "    v_x (torch.Tensor): function instance\n",
    "    model_u (torch.Tensor): model output\n",
    "    \"\"\"\n",
    "    sum = 0\n",
    "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "        u_xi = model_u(x_i)\n",
    "        u_xi.backward()\n",
    "        u_prime = x_i.grad\n",
    "        v_xi = v_x(x_i)\n",
    "        sum += torch.abs(u_prime)**2 + v_xi*(u_xi**2)\n",
    "\n",
    "    return 0.5*sum\n",
    "\n",
    "# NORMALIZE MODEL u(x) OUTPUT:\n",
    "def normalize_u(model_u, lower_bound, upper_bound, n_points):\n",
    "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
    "    h = discrete_points[1] - discrete_points[0]\n",
    "    s = 0\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "        t = model_u(x_i)**2\n",
    "        s += t\n",
    "    denom = torch.sqrt(h) * torch.sqrt(s)\n",
    "\n",
    "    return 1/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [-10.,  -5.,   0.,   5.,  10.]\n",
    "t = torch.tensor([p[0]], requires_grad=True)\n",
    "Q = t**2\n",
    "Q.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36.7810], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon_Loss(5, given_fn, model_u, -10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i = tensor([-10.], requires_grad=True)\n",
      "u_xi = tensor([0.0161], grad_fn=<AddBackward0>)\n",
      "u_prime = tensor([-0.0025])\n",
      "v_xi = tensor([100.], grad_fn=<PowBackward0>)\n",
      "tensor([0.0260], grad_fn=<AddBackward0>)\n",
      "-----\n",
      "x_i = tensor([-5.], requires_grad=True)\n",
      "u_xi = tensor([-0.0088], grad_fn=<AddBackward0>)\n",
      "u_prime = tensor([-0.0100])\n",
      "v_xi = tensor([25.], grad_fn=<PowBackward0>)\n",
      "tensor([0.0020], grad_fn=<AddBackward0>)\n",
      "-----\n",
      "x_i = tensor([0.], requires_grad=True)\n",
      "u_xi = tensor([-0.0283], grad_fn=<AddBackward0>)\n",
      "u_prime = tensor([0.0047])\n",
      "v_xi = tensor([0.], grad_fn=<PowBackward0>)\n",
      "tensor([2.2252e-05], grad_fn=<AddBackward0>)\n",
      "-----\n",
      "x_i = tensor([5.], requires_grad=True)\n",
      "u_xi = tensor([-0.0865], grad_fn=<AddBackward0>)\n",
      "u_prime = tensor([-0.0065])\n",
      "v_xi = tensor([25.], grad_fn=<PowBackward0>)\n",
      "tensor([0.1872], grad_fn=<AddBackward0>)\n",
      "-----\n",
      "x_i = tensor([10.], requires_grad=True)\n",
      "u_xi = tensor([-0.1009], grad_fn=<AddBackward0>)\n",
      "u_prime = tensor([-0.0014])\n",
      "v_xi = tensor([100.], grad_fn=<PowBackward0>)\n",
      "tensor([1.0188], grad_fn=<AddBackward0>)\n",
      "-----\n",
      "tensor([1.2340], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "v_x = given_fn\n",
    "l_b = -10\n",
    "u_b = 10\n",
    "n_points = 5\n",
    "model_u = Nonlinear_2(20)\n",
    "\n",
    "sum = 0\n",
    "d_points = np.linspace(l_b, u_b, n_points)\n",
    "for i in d_points:\n",
    "    x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "    print(\"x_i = \" + str(x_i))\n",
    "    u_xi = model_u(x_i)\n",
    "    print(\"u_xi = \" + str(u_xi))\n",
    "    u_xi.backward()\n",
    "    u_prime = x_i.grad\n",
    "    print(\"u_prime = \" + str(u_prime))\n",
    "    v_xi = v_x(x_i)\n",
    "    print(\"v_xi = \" + str(v_xi))\n",
    "    foo = torch.abs(u_prime)**2 + v_xi*(u_xi**2)\n",
    "    print(foo)\n",
    "    print('-----')\n",
    "    sum += foo\n",
    "print(sum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EstQ_ILqQeOt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 1) (80, 1) (20, 1) (20, 1)\n"
     ]
    }
   ],
   "source": [
    "# CREATING DATASET:\n",
    "x_values = [i for i in np.arange(LOWER_BOUND, UPPER_BOUND, math.pi/50)]\n",
    "y_values = [given_fn(i) for i in x_values]\n",
    "\n",
    "x_train, x_test=train_test_split(x_values,test_size=0.2)\n",
    "y_train = [given_fn(i) for i in x_train]\n",
    "y_test = [given_fn(i) for i in x_test]\n",
    "\n",
    "x_train = np.array(x_train, dtype=np.float32).reshape(-1, 1)\n",
    "y_train = np.array(y_train, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "x_test = np.array(x_test, dtype=np.float32).reshape(-1, 1)\n",
    "y_test = np.array(y_test, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sjtOBhyZLspO"
   },
   "outputs": [],
   "source": [
    "# PLOT DATA\n",
    "def plot_performance(x_train, y_train, x_test, predicted=None):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 6), dpi=80)\n",
    "    plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "    if predicted != None:\n",
    "        plt.plot(x_test, predicted, '--', label='Predictions', alpha=0.5)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "06QWhlkpQ5Eh"
   },
   "outputs": [],
   "source": [
    "# CREATING MODEL CLASS\n",
    "class Nonlinear_2(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        # One hidden layer with n nodes\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1, n)\n",
    "        self.output = nn.Linear(n, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x, use_tanh_fn = False, activation_on_output = False):\n",
    "        if use_tanh_fn == True:\n",
    "            x = self.hidden(x)\n",
    "            x = self.tanh(x)\n",
    "            x = self.output(x)\n",
    "            \n",
    "        else:\n",
    "            x = self.hidden(x)\n",
    "            x = self.sigmoid(x)\n",
    "            x = self.output(x)\n",
    "            \n",
    "        if activation_on_output == False:\n",
    "            return x\n",
    "        else:\n",
    "            if use_tanh_fn == True:\n",
    "                return self.tanh(x)\n",
    "            else:       \n",
    "                return self.sigmoid(x)\n",
    "        # output is a linear combination of the hidden layers because \n",
    "        # we perform regression ???\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE HYPER-PARAMETERS\n",
    "batch_size = 50\n",
    "learningRate = 0.05\n",
    "num_epochs = 1000\n",
    "# num_epochs = int(num_iters/(len(x_train)/batch_size))\n",
    "\n",
    "#INIT PARAMETERS: \n",
    "v_x = given_fn\n",
    "l_b = -10\n",
    "u_b = 10\n",
    "n_points = 5\n",
    "\n",
    "#INIT MODEL\n",
    "model = Nonlinear_2(20)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# INIT OPTIMIZER CLASS\n",
    "# What is an optimizer: \n",
    "# SGD:\n",
    "# SGD_optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "# Adam:\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.learningRate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "# INIT LOSS FUNCTION: MSE\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nonlinear_2(\n",
       "  (hidden): Linear(in_features=1, out_features=20, bias=True)\n",
       "  (output): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRANING MODEL\n",
    "x_epochs = [i for i in range(num_epochs)]\n",
    "y_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    for (input, output) in (zip(x_train, y_train)):\n",
    "        # if torch.cuda.is_available():\n",
    "        #     input = Variable(torch.from_numpy(input).cuda())\n",
    "        #     output = Variable(torch.from_numpy(output).cuda())\n",
    "        # else:\n",
    "        input = Variable(torch.from_numpy(input))\n",
    "        output = Variable(torch.from_numpy(output))\n",
    "    \n",
    "        adam_optimizer.zero_grad()\n",
    "        res = model(input)\n",
    "        loss = criterion(res, output)\n",
    "        loss.backward()\n",
    "        adam_optimizer.step()\n",
    "    y_loss.append(loss)\n",
    "    if epoch % 50 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c21580189d9a9d7f1e3fef63ff70de58083dda94aa35d8ed937f03fa405217e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
