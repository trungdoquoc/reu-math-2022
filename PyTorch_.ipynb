{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpofWgGancyb"
      },
      "source": [
        "The input of the NN is 1D. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e-NAhHF3ncye"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.functional import normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn.functional import normalize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "INPUT_SIZE = 1\n",
        "OUTPUT_SIZE = 1\n",
        "\n",
        "LAMBDA_PEN = 100\n",
        "LOWER_BOUND = -6\n",
        "UPPER_BOUND = 6\n",
        "N_POINTS = 1001\n",
        "\n",
        "\n",
        "def quad_fn(a, b, c, x):\n",
        "    return a*x**2 + b*x + c\n",
        "\n",
        "def x_square(x: torch.tensor) -> torch.Tensor:\n",
        "    return x**2\n",
        "\n",
        "# DEFINE GIVEN FUNCTION V(x): ax^2 + bx + c\n",
        "given_fn = x_square"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EstQ_ILqQeOt",
        "outputId": "b3b8f870-8936-4eed-ac0c-829e3d4a1c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1001, 1) (1001, 1)\n"
          ]
        }
      ],
      "source": [
        "# CREATING DATASET:\n",
        "x_values = [i for i in np.linspace(LOWER_BOUND, UPPER_BOUND, N_POINTS)]\n",
        "y_values = [given_fn(i) for i in x_values]\n",
        "\n",
        "x_train = np.array(x_values, dtype=np.float32).reshape(-1, 1)\n",
        "y_train = np.array(y_values, dtype=np.float32).reshape(-1, 1)\n",
        "\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SOtFslmCncyg"
      },
      "outputs": [],
      "source": [
        "# CUSTOM LOSS FUNCTION:\n",
        "# def EpsilonLoss(nn.Module):\n",
        "\n",
        "def epsilon_Loss(v_x, model_u, lower_bound, upper_bound, n_points):\n",
        "    \"\"\"\n",
        "    GOAL: Epsilon function evaluated at u using discretized estimation\n",
        "    minimizing Epsilon(u) = \n",
        "    \n",
        "    ARGS: \n",
        "    n_points (int): number of discretized points on the interval [-L, L]\n",
        "    e.g.: -(L)|---|---|---|---|(L) interval has n_points = 5\n",
        "\n",
        "    v_x (torch.Tensor): function instance\n",
        "    model_u (torch.Tensor): model output\n",
        "    \"\"\"\n",
        "    sum = 0\n",
        "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
        "    h = discrete_points[1] - discrete_points[0]\n",
        "    for i in discrete_points:\n",
        "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
        "        u_xi = model_u(x_i)\n",
        "\n",
        "        u_prime = model_u.u_prime_2(x_i)\n",
        "        \n",
        "        v_xi = v_x(x_i)\n",
        "        t = torch.square(u_prime) + v_xi*(u_xi**2)\n",
        "        sum += t\n",
        "    return 0.5*h*sum\n",
        "\n",
        "def epsilon_Loss_penalty(v_x, model_u, lambda_pen,\n",
        "                         lower_bound, upper_bound, n_points):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    sum = 0\n",
        "    pen = 0\n",
        "\n",
        "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
        "    h = discrete_points[1] - discrete_points[0]\n",
        "    for i in discrete_points:\n",
        "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
        "        u_xi = model_u(x_i)\n",
        "\n",
        "        u_prime = model_u.u_prime_2(x_i)\n",
        "        \n",
        "        v_xi = v_x(x_i)\n",
        "        t = torch.square(u_prime) + v_xi*(u_xi**2)\n",
        "        sum += t\n",
        "    epsilon_fn = 0.5*h*sum\n",
        "    \n",
        "    temp = 0\n",
        "    for i in discrete_points:\n",
        "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
        "        temp += torch.square(model_u(x_i))\n",
        "    \n",
        "    pen = lambda_pen * torch.square((temp*h-1))\n",
        "    return epsilon_fn + pen \n",
        "\n",
        "# NORMALIZE MODEL u(x) OUTPUT:\n",
        "def normalize_u(model_u, lower_bound, upper_bound, n_points):\n",
        "    \"\"\"\n",
        "    Normalize model.output weight by: \n",
        "    model.output *= c\n",
        "    where,\n",
        "    scalar c = 1/denom\n",
        "    \"\"\"\n",
        "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
        "    h = discrete_points[1] - discrete_points[0]\n",
        "    s = 0\n",
        "    for i in discrete_points:\n",
        "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
        "        s += model_u(x_i)**2\n",
        "    denom = math.sqrt(h) * torch.sqrt(s)\n",
        "    return 1/denom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXgpE3Jpbr2S",
        "outputId": "52dead7e-48c6-4e7d-c790-11102347771f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10.0"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "06QWhlkpQ5Eh"
      },
      "outputs": [],
      "source": [
        "# CREATING MODEL CLASS\n",
        "class Nonlinear(nn.Module):\n",
        "    def __init__(self, n):\n",
        "        # One hidden layer with n nodes\n",
        "        super().__init__()\n",
        "        self.hidden = nn.Linear(1, n)\n",
        "        self.output = nn.Linear(n, 1)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.target_fn = given_fn\n",
        "\n",
        "    def forward(self, x, use_tanh_fn = False, activation_on_output = False):\n",
        "        if use_tanh_fn == True:\n",
        "            x = self.hidden(x)\n",
        "            x = self.tanh(x)\n",
        "            x = self.output(x)\n",
        "            \n",
        "        else:\n",
        "            x = self.hidden(x)\n",
        "            x = self.sigmoid(x)\n",
        "            x = self.output(x)\n",
        "            \n",
        "        if activation_on_output == False:\n",
        "            return x\n",
        "        else:\n",
        "            if use_tanh_fn == True:\n",
        "                return self.tanh(x)\n",
        "            else:       \n",
        "                return self.sigmoid(x)\n",
        "        # output is a linear combination of the hidden layers because \n",
        "        # we perform regression ???\n",
        "        return x\n",
        "\n",
        "    def u_prime_2(self, input):\n",
        "        i_tensor = torch.tensor([input.item()], requires_grad=True, dtype=torch.float)\n",
        "        clone_model = copy.deepcopy(self)\n",
        "        res = clone_model(i_tensor)\n",
        "        res.backward()\n",
        "        del clone_model\n",
        "        \n",
        "        return i_tensor.grad\n",
        "\n",
        "    def u_prime(self, input):\n",
        "        \"\"\"\n",
        "        NN with 1 hidden node layer is of the form:\n",
        "        u(x) = SUM_i_to_N(a_i * sigmoid(w.x + b))\n",
        "\n",
        "        where\n",
        "        a_i is the corresponding weight of self.output layerq\n",
        "        w is self.hidden.weight vector\n",
        "        b is self.hidden.bias vector\n",
        "        sigmoid(w.x + b) is the sigmoid-activated hidden vector\n",
        "\n",
        "        Formula of u'(x) (for 1 hidden layer NN):\n",
        "        u'(x) = SUM_i_to_N(w_i*a_i*sigmoid'(w_i*x+b))\n",
        "        Note: sigmoid'(w_i*x +b) = sigmoid(w_i*x+b)*(1-sigmoid(w_i*x+b))\n",
        "        \"\"\"\n",
        "        a_i = self.output.weight.data\n",
        "        w_i = torch.transpose(self.hidden.weight.data, 0, 1)\n",
        "        wi_ai = w_i * a_i\n",
        "\n",
        "        hid_layer = self.hidden(input)\n",
        "        hid_layer_T = torch.reshape(hid_layer, (list(hid_layer.shape)[0], 1))\n",
        "        m = hid_layer_T * (1-hid_layer_T)\n",
        "\n",
        "        return wi_ai @ m\n",
        "    \n",
        "# TRANING MODEL\n",
        "    def train_network_with_penalty(self, num_epochs, v_x, optimizer, lambda_pen,\n",
        "                                    lower_bound, upper_bound, n_points):\n",
        "        # For plotting loss value over epochs:\n",
        "        x_epochs = []\n",
        "        y_loss = []\n",
        "        y_loss_pen = []\n",
        "\n",
        "        # stopping criterion:\n",
        "        stop_counter = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            optimizer.zero_grad()\n",
        "            loss_pen = epsilon_Loss_penalty(v_x, self, lambda_pen,\n",
        "                                        lower_bound, upper_bound, n_points)\n",
        "            loss = epsilon_Loss(v_x, self,\n",
        "                                lower_bound, upper_bound, n_points)\n",
        "            y_loss_pen.append(loss_pen)\n",
        "            y_loss.append(loss)\n",
        "            x_epochs.append(epoch)\n",
        "            #check if need to stop training:\n",
        "            if epoch > 0 and stop_counter >= 5:\n",
        "                c = normalize_u(self, lower_bound, upper_bound, n_points)\n",
        "                self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
        "                self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
        "                print(\"c value = \" + str(c))\n",
        "                print(\"LOSS VALUE = \" \n",
        "                      + str(epsilon_Loss(v_x, self, lower_bound, upper_bound, n_points)))\n",
        "                break\n",
        "            elif epoch > 0 and stop_counter < 5:\n",
        "                if torch.abs(y_loss[epoch-1]-loss) <= 1e-5:\n",
        "                    stop_counter += 1\n",
        "                else:\n",
        "                    stop_counter = 0\n",
        "\n",
        "            print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
        "            loss.backward()\n",
        "            # Calculate the derivative(loss) w.r.t the model's parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        return (x_epochs, y_loss_pen, y_loss)\n",
        "\n",
        "    def train_network(self, num_epochs, v_x, optimizer,\n",
        "                                    lower_bound, upper_bound, n_points):\n",
        "        # For plotting loss value over epochs:\n",
        "        x_epochs = []\n",
        "        y_loss = []\n",
        "        \n",
        "        # stopping criterion:\n",
        "        stop_counter = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            if epoch > 0 and epoch % 50 == 0:\n",
        "                c = normalize_u(self, lower_bound, upper_bound, n_points)\n",
        "                print(\"Pre normalize: \")\n",
        "                print(self.output.weight.data)\n",
        "                print(self.output.bias.data)\n",
        "                print(\"After normalize: \")\n",
        "                self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
        "                self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
        "                print(self.output.weight.data)\n",
        "                print(self.output.bias.data)\n",
        "                print(\"c value = \" + str(c))\n",
        "            optimizer.zero_grad()\n",
        "            loss = epsilon_Loss(v_x, self,\n",
        "                                lower_bound, upper_bound, n_points)\n",
        "            y_loss.append(loss)\n",
        "            x_epochs.append(epoch)\n",
        "            #check if need to stop training:\n",
        "            if epoch > 0 and stop_counter >= 5:\n",
        "                print(\"LOSS VALUE = \" \n",
        "                      + str(epsilon_Loss(v_x, self, lower_bound, upper_bound, n_points)))\n",
        "                break\n",
        "            elif epoch > 0 and stop_counter < 5:\n",
        "                if torch.abs(y_loss[epoch-1]-loss) <= 1e-5:\n",
        "                    stop_counter += 1\n",
        "                else:\n",
        "                    stop_counter = 0\n",
        "\n",
        "            print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return (x_epochs, y_loss)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GCG5zWf9sJ0d",
        "outputId": "646a4abe-4592-40d7-cff3-7ee3c05fde40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1237])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2saw960759Td",
        "outputId": "15fd19c7-cccc-403f-dd6f-1b223c54c84d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([78.7160], grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l = epsilon_Loss(given_fn, model, \n",
        "                            LOWER_BOUND, UPPER_BOUND, N_POINTS)\n",
        "l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgx4K0yugVxX",
        "outputId": "ae40cf07-991b-46ce-e62b-da3c83c7990a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([78.7160], grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = epsilon_Loss_penalty(given_fn, model, LAMBDA_PEN, \n",
        "                            LOWER_BOUND, UPPER_BOUND, N_POINTS)\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hbBxk8_fncyl"
      },
      "outputs": [],
      "source": [
        "# DEFINE HYPER-PARAMETERS\n",
        "batch_size = 50\n",
        "learningRate = 0.05\n",
        "num_epochs = 2000\n",
        "\n",
        "#INIT MODEL\n",
        "model = Nonlinear(20)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "# INIT OPTIMIZER CLASS\n",
        "# What is an optimizer: \n",
        "# SGD:\n",
        "# SGD_optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
        "# Adam:\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), \n",
        "                                    lr=learningRate, \n",
        "                                    betas=(0.9, 0.999), \n",
        "                                    eps=1e-08, \n",
        "                                    weight_decay=0, \n",
        "                                    amsgrad=False)\n",
        "\n",
        "# INIT LOSS FUNCTION: MSE\n",
        "# criterion = epsilon_Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sjtOBhyZLspO"
      },
      "outputs": [],
      "source": [
        "# PLOT DATA\n",
        "def plot_figure(x_val, y_val, x_test=None, predicted=None):\n",
        "    plt.clf()\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.plot(x_val, y_val, '--', label='True data', alpha=0.5)\n",
        "    if predicted != None:\n",
        "        plt.plot(x_test, predicted, '--', label='Predictions', alpha=0.5)\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WZbaJ3MsB08r",
        "outputId": "0effac96-28c0-4fce-8f8d-382a57371e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 1.254576563835144\n",
            "epoch 1, loss 43.2580451965332\n",
            "epoch 2, loss 157.80340576171875\n",
            "epoch 3, loss 9.330690383911133\n",
            "epoch 4, loss 89.2890853881836\n",
            "epoch 5, loss 64.87334442138672\n",
            "epoch 6, loss 2.2080342769622803\n",
            "epoch 7, loss 59.819942474365234\n",
            "epoch 8, loss 46.619415283203125\n",
            "epoch 9, loss 1.1598074436187744\n",
            "epoch 10, loss 26.683921813964844\n",
            "epoch 11, loss 42.08333969116211\n",
            "epoch 12, loss 11.504769325256348\n",
            "epoch 13, loss 2.893078565597534\n",
            "epoch 14, loss 24.02597427368164\n",
            "epoch 15, loss 22.86964988708496\n",
            "epoch 16, loss 3.6503665447235107\n",
            "epoch 17, loss 3.4405057430267334\n",
            "epoch 18, loss 15.969564437866211\n",
            "epoch 19, loss 14.203802108764648\n",
            "epoch 20, loss 2.6920676231384277\n",
            "epoch 21, loss 1.66078782081604\n",
            "epoch 22, loss 9.333247184753418\n",
            "epoch 23, loss 9.674241065979004\n",
            "epoch 24, loss 2.6188573837280273\n",
            "epoch 25, loss 0.5258735418319702\n",
            "epoch 26, loss 4.998000621795654\n",
            "epoch 27, loss 6.740937232971191\n",
            "epoch 28, loss 2.90854811668396\n",
            "epoch 29, loss 0.15831907093524933\n",
            "epoch 30, loss 2.142836332321167\n",
            "epoch 31, loss 4.315968990325928\n",
            "epoch 32, loss 2.774106502532959\n",
            "epoch 33, loss 0.36055299639701843\n",
            "epoch 34, loss 0.7565727233886719\n",
            "epoch 35, loss 2.5084428787231445\n",
            "epoch 36, loss 2.3713483810424805\n",
            "epoch 37, loss 0.7096060514450073\n",
            "epoch 38, loss 0.19840659201145172\n",
            "epoch 39, loss 1.2346328496932983\n",
            "epoch 40, loss 1.7442759275436401\n",
            "epoch 41, loss 0.8917883038520813\n",
            "epoch 42, loss 0.15346617996692657\n",
            "epoch 43, loss 0.5513768196105957\n",
            "epoch 44, loss 1.1470608711242676\n",
            "epoch 45, loss 0.8846437931060791\n",
            "epoch 46, loss 0.24778145551681519\n",
            "epoch 47, loss 0.21794822812080383\n",
            "epoch 48, loss 0.6562865376472473\n",
            "epoch 49, loss 0.7237273454666138\n",
            "Pre normalize: \n",
            "tensor([[-7.0217, -3.2179, -6.6804,  2.1419,  0.7138, -0.1173, -6.1477, -4.1769,\n",
            "          7.2658,  4.7902, -4.2670, -8.0958,  6.5276, -1.6924, -8.0738, -5.8581,\n",
            "         -6.0254, 11.8386, 13.3166, -0.0549]])\n",
            "tensor([5.7238])\n",
            "After normalize: \n",
            "tensor([[-19.3233,  -8.8555, -18.3842,   5.8944,   1.9644,  -0.3229, -16.9183,\n",
            "         -11.4945,  19.9951,  13.1824, -11.7426, -22.2793,  17.9635,  -4.6574,\n",
            "         -22.2185, -16.1211, -16.5817,  32.5793,  36.6467,  -0.1510]])\n",
            "tensor([15.7516])\n",
            "c value = tensor([2.7519], grad_fn=<MulBackward0>)\n",
            "epoch 50, loss 2.4882943630218506\n",
            "epoch 51, loss 7.897414207458496\n",
            "epoch 52, loss 1.2747082710266113\n",
            "epoch 53, loss 4.935919761657715\n",
            "epoch 54, loss 4.1294779777526855\n",
            "epoch 55, loss 1.4064791202545166\n",
            "epoch 56, loss 4.878413677215576\n",
            "epoch 57, loss 1.1981202363967896\n",
            "epoch 58, loss 3.057220220565796\n",
            "epoch 59, loss 2.4470040798187256\n",
            "epoch 60, loss 1.1780191659927368\n",
            "epoch 61, loss 2.982067108154297\n",
            "epoch 62, loss 0.8341024518013\n",
            "epoch 63, loss 2.1182916164398193\n",
            "epoch 64, loss 1.5499649047851562\n",
            "epoch 65, loss 1.0444831848144531\n",
            "epoch 66, loss 1.7999093532562256\n",
            "epoch 67, loss 0.7210657596588135\n",
            "epoch 68, loss 1.54567551612854\n",
            "epoch 69, loss 0.9187931418418884\n",
            "epoch 70, loss 0.886988639831543\n",
            "epoch 71, loss 1.284218192100525\n",
            "epoch 72, loss 0.5682294964790344\n",
            "epoch 73, loss 1.0961971282958984\n",
            "epoch 74, loss 0.7014404535293579\n",
            "epoch 75, loss 0.763785719871521\n",
            "epoch 76, loss 0.8251089453697205\n",
            "epoch 77, loss 0.49456271529197693\n",
            "epoch 78, loss 0.8837539553642273\n",
            "epoch 79, loss 0.48603013157844543\n",
            "epoch 80, loss 0.6146799325942993\n",
            "epoch 81, loss 0.652870774269104\n",
            "epoch 82, loss 0.4390742778778076\n",
            "epoch 83, loss 0.6124436855316162\n",
            "epoch 84, loss 0.4013299345970154\n",
            "epoch 85, loss 0.527212381362915\n",
            "epoch 86, loss 0.45104339718818665\n",
            "epoch 87, loss 0.36625343561172485\n",
            "epoch 88, loss 0.5051378011703491\n",
            "epoch 89, loss 0.3425588607788086\n",
            "epoch 90, loss 0.3956025242805481\n",
            "epoch 91, loss 0.38345617055892944\n",
            "epoch 92, loss 0.3284985423088074\n",
            "epoch 93, loss 0.3665190637111664\n",
            "epoch 94, loss 0.2992222309112549\n",
            "epoch 95, loss 0.3383845090866089\n",
            "epoch 96, loss 0.31536805629730225\n",
            "epoch 97, loss 0.2756107747554779\n",
            "epoch 98, loss 0.32036733627319336\n",
            "epoch 99, loss 0.2745712399482727\n",
            "Pre normalize: \n",
            "tensor([[-19.3313,  -8.8616, -18.3912,   5.8837,   1.9334,  -0.3201, -16.9177,\n",
            "         -11.4936,  19.9959,  13.1809, -11.7410, -22.2854,  17.9545,  -4.6396,\n",
            "         -22.2179, -16.1204, -16.5632,  32.5699,  36.6444,  -0.1605]])\n",
            "tensor([15.7457])\n",
            "After normalize: \n",
            "tensor([[-60.2502, -27.6189, -57.3202,  18.3378,   6.0260,  -0.9977, -52.7277,\n",
            "         -35.8222,  62.3215,  41.0812, -36.5934, -69.4572,  55.9592, -14.4604,\n",
            "         -69.2468, -50.2427, -51.6227, 101.5112, 114.2102,  -0.5003]])\n",
            "tensor([49.0748])\n",
            "c value = tensor([3.1167], grad_fn=<MulBackward0>)\n",
            "epoch 100, loss 2.5864689350128174\n",
            "epoch 101, loss 91.82223510742188\n",
            "epoch 102, loss 7033.89404296875\n",
            "epoch 103, loss 581.3701171875\n",
            "epoch 104, loss 6449.0595703125\n",
            "epoch 105, loss 2587.932373046875\n",
            "epoch 106, loss 376.7325134277344\n",
            "epoch 107, loss 2868.1435546875\n",
            "epoch 108, loss 3041.6328125\n",
            "epoch 109, loss 1128.59716796875\n",
            "epoch 110, loss 298.36456298828125\n",
            "epoch 111, loss 1085.2115478515625\n",
            "epoch 112, loss 1722.3004150390625\n",
            "epoch 113, loss 1150.353271484375\n",
            "epoch 114, loss 340.10540771484375\n",
            "epoch 115, loss 237.84414672851562\n",
            "epoch 116, loss 627.0059814453125\n",
            "epoch 117, loss 912.614990234375\n",
            "epoch 118, loss 793.9974365234375\n",
            "epoch 119, loss 406.2776184082031\n",
            "epoch 120, loss 106.87335968017578\n",
            "epoch 121, loss 123.4325180053711\n",
            "epoch 122, loss 354.6684265136719\n",
            "epoch 123, loss 492.780517578125\n",
            "epoch 124, loss 380.7919616699219\n",
            "epoch 125, loss 151.61651611328125\n",
            "epoch 126, loss 23.297653198242188\n",
            "epoch 127, loss 77.64672088623047\n",
            "epoch 128, loss 219.32969665527344\n",
            "epoch 129, loss 295.1484375\n",
            "epoch 130, loss 240.50770568847656\n",
            "epoch 131, loss 115.49798583984375\n",
            "epoch 132, loss 26.334041595458984\n",
            "epoch 133, loss 27.311344146728516\n",
            "epoch 134, loss 85.66948699951172\n",
            "epoch 135, loss 127.9951171875\n",
            "epoch 136, loss 113.75160217285156\n",
            "epoch 137, loss 62.44870376586914\n",
            "epoch 138, loss 20.975526809692383\n",
            "epoch 139, loss 16.821077346801758\n",
            "epoch 140, loss 41.20669937133789\n",
            "epoch 141, loss 66.22400665283203\n",
            "epoch 142, loss 70.76123809814453\n",
            "epoch 143, loss 53.206695556640625\n",
            "epoch 144, loss 27.641359329223633\n",
            "epoch 145, loss 11.284147262573242\n",
            "epoch 146, loss 12.209041595458984\n",
            "epoch 147, loss 24.63050079345703\n",
            "epoch 148, loss 34.87513732910156\n",
            "epoch 149, loss 33.30341720581055\n",
            "Pre normalize: \n",
            "tensor([[-60.5157, -27.8773, -57.5840,  17.9906,   5.1546,  -0.5004, -52.8230,\n",
            "         -35.9080,  62.1992,  40.9249, -36.6996, -69.7137,  55.6454, -14.2066,\n",
            "         -69.3441, -50.3352, -50.8915, 101.2252, 114.0324,  -0.2128]])\n",
            "tensor([48.3004])\n",
            "After normalize: \n",
            "tensor([[-30.9261, -14.2465, -29.4279,   9.1940,   2.6342,  -0.2557, -26.9948,\n",
            "         -18.3505,  31.7864,  20.9143, -18.7551, -35.6267,  28.4372,  -7.2602,\n",
            "         -35.4378, -25.7234, -26.0077,  51.7304,  58.2754,  -0.1087]])\n",
            "tensor([24.6836])\n",
            "c value = tensor([0.5110], grad_fn=<MulBackward0>)\n",
            "epoch 150, loss 5.698246479034424\n",
            "epoch 151, loss 3.288083076477051\n",
            "epoch 152, loss 2.229526996612549\n",
            "epoch 153, loss 2.4915900230407715\n",
            "epoch 154, loss 3.6786422729492188\n",
            "epoch 155, loss 5.2037811279296875\n",
            "epoch 156, loss 6.487699508666992\n",
            "epoch 157, loss 7.122416973114014\n",
            "epoch 158, loss 6.9590983390808105\n",
            "epoch 159, loss 6.106123447418213\n",
            "epoch 160, loss 4.854531288146973\n",
            "epoch 161, loss 3.561267614364624\n",
            "epoch 162, loss 2.5339040756225586\n",
            "epoch 163, loss 1.949291706085205\n",
            "epoch 164, loss 1.8255051374435425\n",
            "epoch 165, loss 2.0466933250427246\n",
            "epoch 166, loss 2.4238054752349854\n",
            "epoch 167, loss 2.7660815715789795\n",
            "epoch 168, loss 2.9394562244415283\n",
            "epoch 169, loss 2.8952033519744873\n",
            "epoch 170, loss 2.6667141914367676\n",
            "epoch 171, loss 2.3411865234375\n",
            "epoch 172, loss 2.020566463470459\n",
            "epoch 173, loss 1.786237120628357\n",
            "epoch 174, loss 1.677777886390686\n",
            "epoch 175, loss 1.6893603801727295\n",
            "epoch 176, loss 1.7808676958084106\n",
            "epoch 177, loss 1.897173523902893\n",
            "epoch 178, loss 1.9878469705581665\n",
            "epoch 179, loss 2.0214579105377197\n",
            "epoch 180, loss 1.9910542964935303\n",
            "epoch 181, loss 1.9114625453948975\n",
            "epoch 182, loss 1.810551643371582\n",
            "epoch 183, loss 1.718443751335144\n",
            "epoch 184, loss 1.6579625606536865\n",
            "epoch 185, loss 1.6390299797058105\n",
            "epoch 186, loss 1.657716989517212\n",
            "epoch 187, loss 1.699586272239685\n",
            "epoch 188, loss 1.7457289695739746\n",
            "epoch 189, loss 1.7790032625198364\n",
            "epoch 190, loss 1.7889184951782227\n",
            "epoch 191, loss 1.773711919784546\n",
            "epoch 192, loss 1.7395507097244263\n",
            "epoch 193, loss 1.6972076892852783\n",
            "epoch 194, loss 1.658206820487976\n",
            "epoch 195, loss 1.6310731172561646\n",
            "epoch 196, loss 1.6192580461502075\n",
            "epoch 197, loss 1.6209560632705688\n",
            "epoch 198, loss 1.6305866241455078\n",
            "epoch 199, loss 1.6413242816925049\n",
            "Pre normalize: \n",
            "tensor([[-30.9211, -14.2408, -29.4222,   9.1984,   2.6062,  -0.2373, -27.0039,\n",
            "         -18.3595,  31.7781,  20.9042, -18.7638, -35.6210,  28.4417,  -7.2319,\n",
            "         -35.4467, -25.7324, -25.9733,  51.7346,  58.2649,  -0.0928]])\n",
            "tensor([24.6800])\n",
            "After normalize: \n",
            "tensor([[-53.1589, -24.4825, -50.5821,  15.8137,   4.4805,  -0.4080, -46.4245,\n",
            "         -31.5634,  54.6323,  35.9381, -32.2583, -61.2389,  48.8964, -12.4329,\n",
            "         -60.9392, -44.2386, -44.6529,  88.9411, 100.1679,  -0.1595]])\n",
            "tensor([42.4293])\n",
            "c value = tensor([1.7192], grad_fn=<MulBackward0>)\n",
            "epoch 200, loss 4.869255065917969\n",
            "epoch 201, loss 4.829702377319336\n",
            "epoch 202, loss 4.770677089691162\n",
            "epoch 203, loss 4.731313705444336\n",
            "epoch 204, loss 4.731353282928467\n",
            "epoch 205, loss 4.761424541473389\n",
            "epoch 206, loss 4.79530143737793\n",
            "epoch 207, loss 4.811328887939453\n",
            "epoch 208, loss 4.806535243988037\n",
            "epoch 209, loss 4.794467449188232\n",
            "epoch 210, loss 4.7914628982543945\n",
            "epoch 211, loss 4.803948879241943\n",
            "epoch 212, loss 4.824700832366943\n",
            "epoch 213, loss 4.840170860290527\n",
            "epoch 214, loss 4.84136962890625\n",
            "epoch 215, loss 4.829920291900635\n",
            "epoch 216, loss 4.815868854522705\n",
            "epoch 217, loss 4.809864044189453\n",
            "epoch 218, loss 4.816152095794678\n",
            "epoch 219, loss 4.8310136795043945\n",
            "epoch 220, loss 4.846352577209473\n",
            "epoch 221, loss 4.85595178604126\n",
            "epoch 222, loss 4.858985424041748\n",
            "epoch 223, loss 4.859310626983643\n",
            "epoch 224, loss 4.861695289611816\n",
            "epoch 225, loss 4.868519306182861\n",
            "epoch 226, loss 4.878529071807861\n",
            "epoch 227, loss 4.888718605041504\n",
            "epoch 228, loss 4.8969035148620605\n",
            "epoch 229, loss 4.902992248535156\n",
            "epoch 230, loss 4.908514499664307\n",
            "epoch 231, loss 4.914775848388672\n",
            "epoch 232, loss 4.9216461181640625\n",
            "epoch 233, loss 4.9275431632995605\n",
            "epoch 234, loss 4.9307379722595215\n",
            "epoch 235, loss 4.930532455444336\n",
            "epoch 236, loss 4.927828311920166\n",
            "epoch 237, loss 4.9243574142456055\n",
            "epoch 238, loss 4.9217023849487305\n",
            "epoch 239, loss 4.920492172241211\n",
            "epoch 240, loss 4.920472621917725\n",
            "epoch 241, loss 4.9211039543151855\n",
            "epoch 242, loss 4.922063827514648\n",
            "epoch 243, loss 4.923323631286621\n",
            "epoch 244, loss 4.925070762634277\n",
            "epoch 245, loss 4.9270853996276855\n",
            "epoch 246, loss 4.928840160369873\n",
            "epoch 247, loss 4.929528713226318\n",
            "epoch 248, loss 4.928523540496826\n",
            "epoch 249, loss 4.9257636070251465\n",
            "Pre normalize: \n",
            "tensor([[-53.1465, -24.4690, -50.5693,  15.8275,   4.4808,  -0.4010, -46.4345,\n",
            "         -31.5733,  54.6241,  35.9283, -32.2669, -61.2252,  48.9096, -12.4152,\n",
            "         -60.9488, -44.2484, -44.6453,  88.9534, 100.1579,  -0.1515]])\n",
            "tensor([42.4381])\n",
            "After normalize: \n",
            "tensor([[-44.5256, -20.4998, -42.3664,  13.2601,   3.7539,  -0.3360, -38.9024,\n",
            "         -26.4518,  45.7635,  30.1004, -27.0329, -51.2938,  40.9759, -10.4013,\n",
            "         -51.0622, -37.0709, -37.4033,  74.5242,  83.9112,  -0.1269]])\n",
            "tensor([35.5542])\n",
            "c value = tensor([0.8378], grad_fn=<MulBackward0>)\n",
            "epoch 250, loss 3.45438289642334\n",
            "epoch 251, loss 3.450840950012207\n",
            "epoch 252, loss 3.447258472442627\n",
            "epoch 253, loss 3.443732976913452\n",
            "epoch 254, loss 3.4402451515197754\n",
            "epoch 255, loss 3.436711072921753\n",
            "epoch 256, loss 3.4331448078155518\n",
            "epoch 257, loss 3.429532051086426\n",
            "epoch 258, loss 3.4259817600250244\n",
            "epoch 259, loss 3.4224352836608887\n",
            "epoch 260, loss 3.418797492980957\n",
            "epoch 261, loss 3.4149293899536133\n",
            "epoch 262, loss 3.410672664642334\n",
            "epoch 263, loss 3.4060041904449463\n",
            "epoch 264, loss 3.400911331176758\n",
            "epoch 265, loss 3.3955113887786865\n",
            "epoch 266, loss 3.3899271488189697\n",
            "epoch 267, loss 3.38433837890625\n",
            "epoch 268, loss 3.3787460327148438\n",
            "epoch 269, loss 3.3732221126556396\n",
            "epoch 270, loss 3.367780923843384\n",
            "epoch 271, loss 3.3624041080474854\n",
            "epoch 272, loss 3.3570563793182373\n",
            "epoch 273, loss 3.3517422676086426\n",
            "epoch 274, loss 3.3463642597198486\n",
            "epoch 275, loss 3.3409218788146973\n",
            "epoch 276, loss 3.3353195190429688\n",
            "epoch 277, loss 3.3294456005096436\n",
            "epoch 278, loss 3.323336362838745\n",
            "epoch 279, loss 3.3169662952423096\n",
            "epoch 280, loss 3.310358762741089\n",
            "epoch 281, loss 3.3035616874694824\n",
            "epoch 282, loss 3.296668529510498\n",
            "epoch 283, loss 3.2897324562072754\n",
            "epoch 284, loss 3.2827961444854736\n",
            "epoch 285, loss 3.2758781909942627\n",
            "epoch 286, loss 3.2689995765686035\n",
            "epoch 287, loss 3.2621572017669678\n",
            "epoch 288, loss 3.2553300857543945\n",
            "epoch 289, loss 3.2484865188598633\n",
            "epoch 290, loss 3.2416067123413086\n",
            "epoch 291, loss 3.2346224784851074\n",
            "epoch 292, loss 3.227524757385254\n",
            "epoch 293, loss 3.2202811241149902\n",
            "epoch 294, loss 3.212926149368286\n",
            "epoch 295, loss 3.2054388523101807\n",
            "epoch 296, loss 3.1978728771209717\n",
            "epoch 297, loss 3.190258741378784\n",
            "epoch 298, loss 3.182612657546997\n",
            "epoch 299, loss 3.1749846935272217\n",
            "Pre normalize: \n",
            "tensor([[-44.5168, -20.4902, -42.3574,  13.2699,   3.7559,  -0.3306, -38.9082,\n",
            "         -26.4577,  45.7585,  30.0947, -27.0377, -51.2841,  40.9852, -10.3865,\n",
            "         -51.0677, -37.0766, -37.3975,  74.5330,  83.9054,  -0.1208]])\n",
            "tensor([35.5618])\n",
            "After normalize: \n",
            "tensor([[-44.1823, -20.3363, -42.0392,  13.1702,   3.7277,  -0.3281, -38.6159,\n",
            "         -26.2589,  45.4147,  29.8686, -26.8346, -50.8989,  40.6773, -10.3085,\n",
            "         -50.6841, -36.7981, -37.1165,  73.9730,  83.2750,  -0.1199]])\n",
            "tensor([35.2946])\n",
            "c value = tensor([0.9925], grad_fn=<MulBackward0>)\n",
            "epoch 300, loss 3.1199796199798584\n",
            "epoch 301, loss 3.112532138824463\n",
            "epoch 302, loss 3.1051182746887207\n",
            "epoch 303, loss 3.0977141857147217\n",
            "epoch 304, loss 3.0903372764587402\n",
            "epoch 305, loss 3.0829317569732666\n",
            "epoch 306, loss 3.0754945278167725\n",
            "epoch 307, loss 3.068023204803467\n",
            "epoch 308, loss 3.060490608215332\n",
            "epoch 309, loss 3.0528833866119385\n",
            "epoch 310, loss 3.0452370643615723\n",
            "epoch 311, loss 3.0375728607177734\n",
            "epoch 312, loss 3.029883861541748\n",
            "epoch 313, loss 3.0221810340881348\n",
            "epoch 314, loss 3.01448917388916\n",
            "epoch 315, loss 3.00679874420166\n",
            "epoch 316, loss 2.9991238117218018\n",
            "epoch 317, loss 2.9914495944976807\n",
            "epoch 318, loss 2.983764886856079\n",
            "epoch 319, loss 2.9760756492614746\n",
            "epoch 320, loss 2.96838641166687\n",
            "epoch 321, loss 2.960669755935669\n",
            "epoch 322, loss 2.9529263973236084\n",
            "epoch 323, loss 2.9451451301574707\n",
            "epoch 324, loss 2.937359571456909\n",
            "epoch 325, loss 2.9295480251312256\n",
            "epoch 326, loss 2.9217233657836914\n",
            "epoch 327, loss 2.913911819458008\n",
            "epoch 328, loss 2.9060890674591064\n",
            "epoch 329, loss 2.8982834815979004\n",
            "epoch 330, loss 2.8904836177825928\n",
            "epoch 331, loss 2.882707118988037\n",
            "epoch 332, loss 2.8749496936798096\n",
            "epoch 333, loss 2.8671700954437256\n",
            "epoch 334, loss 2.8594205379486084\n",
            "epoch 335, loss 2.8516440391540527\n",
            "epoch 336, loss 2.8438756465911865\n",
            "epoch 337, loss 2.836082935333252\n",
            "epoch 338, loss 2.8282859325408936\n",
            "epoch 339, loss 2.820491313934326\n",
            "epoch 340, loss 2.8126957416534424\n",
            "epoch 341, loss 2.8048856258392334\n",
            "epoch 342, loss 2.7970800399780273\n",
            "epoch 343, loss 2.7892749309539795\n",
            "epoch 344, loss 2.7814886569976807\n",
            "epoch 345, loss 2.7737159729003906\n",
            "epoch 346, loss 2.7659552097320557\n",
            "epoch 347, loss 2.7581892013549805\n",
            "epoch 348, loss 2.7504255771636963\n",
            "epoch 349, loss 2.7426702976226807\n",
            "Pre normalize: \n",
            "tensor([[-44.1750, -20.3283, -42.0318,  13.1784,   3.7297,  -0.3233, -38.6204,\n",
            "         -26.2634,  45.4109,  29.8643, -26.8383, -50.8908,  40.6851, -10.2946,\n",
            "         -50.6883, -36.8024, -37.1115,  73.9803,  83.2705,  -0.1144]])\n",
            "tensor([35.3014])\n",
            "After normalize: \n",
            "tensor([[-46.3416, -21.3253, -44.0932,  13.8247,   3.9126,  -0.3392, -40.5145,\n",
            "         -27.5515,  47.6381,  31.3290, -28.1546, -53.3867,  42.6805, -10.7995,\n",
            "         -53.1743, -38.6074, -38.9317,  77.6087,  87.3546,  -0.1200]])\n",
            "tensor([37.0327])\n",
            "c value = tensor([1.0490], grad_fn=<MulBackward0>)\n",
            "epoch 350, loss 3.009767532348633\n",
            "epoch 351, loss 3.001187562942505\n",
            "epoch 352, loss 2.9925105571746826\n",
            "epoch 353, loss 2.9837632179260254\n",
            "epoch 354, loss 2.9749341011047363\n",
            "epoch 355, loss 2.966027021408081\n",
            "epoch 356, loss 2.957089900970459\n",
            "epoch 357, loss 2.9480977058410645\n",
            "epoch 358, loss 2.939100980758667\n",
            "epoch 359, loss 2.930086135864258\n",
            "epoch 360, loss 2.9210875034332275\n",
            "epoch 361, loss 2.912066698074341\n",
            "epoch 362, loss 2.9030396938323975\n",
            "epoch 363, loss 2.8940513134002686\n",
            "epoch 364, loss 2.8850467205047607\n",
            "epoch 365, loss 2.8760318756103516\n",
            "epoch 366, loss 2.867021322250366\n",
            "epoch 367, loss 2.8579864501953125\n",
            "epoch 368, loss 2.8489620685577393\n",
            "epoch 369, loss 2.83992600440979\n",
            "epoch 370, loss 2.8308722972869873\n",
            "epoch 371, loss 2.821840286254883\n",
            "epoch 372, loss 2.8127949237823486\n",
            "epoch 373, loss 2.80377459526062\n",
            "epoch 374, loss 2.7947590351104736\n",
            "epoch 375, loss 2.785764217376709\n",
            "epoch 376, loss 2.776794672012329\n",
            "epoch 377, loss 2.7678349018096924\n",
            "epoch 378, loss 2.7588717937469482\n",
            "epoch 379, loss 2.7499170303344727\n",
            "epoch 380, loss 2.7409956455230713\n",
            "epoch 381, loss 2.7320895195007324\n",
            "epoch 382, loss 2.7231743335723877\n",
            "epoch 383, loss 2.7143001556396484\n",
            "epoch 384, loss 2.705427885055542\n",
            "epoch 385, loss 2.6965789794921875\n",
            "epoch 386, loss 2.687744140625\n",
            "epoch 387, loss 2.6789345741271973\n",
            "epoch 388, loss 2.6701407432556152\n",
            "epoch 389, loss 2.6613574028015137\n",
            "epoch 390, loss 2.652621269226074\n",
            "epoch 391, loss 2.643899917602539\n",
            "epoch 392, loss 2.635193109512329\n",
            "epoch 393, loss 2.6265056133270264\n",
            "epoch 394, loss 2.6178412437438965\n",
            "epoch 395, loss 2.609172821044922\n",
            "epoch 396, loss 2.6005542278289795\n",
            "epoch 397, loss 2.5919289588928223\n",
            "epoch 398, loss 2.5833518505096436\n",
            "epoch 399, loss 2.5747764110565186\n",
            "Pre normalize: \n",
            "tensor([[-46.3343, -21.3173, -44.0858,  13.8329,   3.9148,  -0.3343, -40.5186,\n",
            "         -27.5556,  47.6346,  31.3251, -28.1579, -53.3786,  42.6882, -10.7853,\n",
            "         -53.1781, -38.6114, -38.9264,  77.6159,  87.3505,  -0.1144]])\n",
            "tensor([37.0400])\n",
            "After normalize: \n",
            "tensor([[-49.5903, -22.8153, -47.1838,  14.8050,   4.1899,  -0.3578, -43.3660,\n",
            "         -29.4920,  50.9820,  33.5264, -30.1367, -57.1297,  45.6881, -11.5432,\n",
            "         -56.9151, -41.3248, -41.6619,  83.0703,  93.4889,  -0.1224]])\n",
            "tensor([39.6430])\n",
            "c value = tensor([1.0703], grad_fn=<MulBackward0>)\n",
            "epoch 400, loss 2.939570665359497\n",
            "epoch 401, loss 2.9297053813934326\n",
            "epoch 402, loss 2.9197163581848145\n",
            "epoch 403, loss 2.9096317291259766\n",
            "epoch 404, loss 2.8994224071502686\n",
            "epoch 405, loss 2.889094591140747\n",
            "epoch 406, loss 2.87870454788208\n",
            "epoch 407, loss 2.8682684898376465\n",
            "epoch 408, loss 2.857799530029297\n",
            "epoch 409, loss 2.847343921661377\n",
            "epoch 410, loss 2.836909532546997\n",
            "epoch 411, loss 2.8264875411987305\n",
            "epoch 412, loss 2.8160817623138428\n",
            "epoch 413, loss 2.8056859970092773\n",
            "epoch 414, loss 2.7953040599823\n",
            "epoch 415, loss 2.784895420074463\n",
            "epoch 416, loss 2.774465799331665\n",
            "epoch 417, loss 2.764049768447876\n",
            "epoch 418, loss 2.7536003589630127\n",
            "epoch 419, loss 2.7431387901306152\n",
            "epoch 420, loss 2.732717514038086\n",
            "epoch 421, loss 2.7222981452941895\n",
            "epoch 422, loss 2.711906909942627\n",
            "epoch 423, loss 2.7015511989593506\n",
            "epoch 424, loss 2.691216230392456\n",
            "epoch 425, loss 2.680922269821167\n",
            "epoch 426, loss 2.670654535293579\n",
            "epoch 427, loss 2.660428285598755\n",
            "epoch 428, loss 2.650214910507202\n",
            "epoch 429, loss 2.6400091648101807\n",
            "epoch 430, loss 2.6298465728759766\n",
            "epoch 431, loss 2.619694471359253\n",
            "epoch 432, loss 2.6095597743988037\n",
            "epoch 433, loss 2.5994791984558105\n",
            "epoch 434, loss 2.589420795440674\n",
            "epoch 435, loss 2.5794055461883545\n",
            "epoch 436, loss 2.5694360733032227\n",
            "epoch 437, loss 2.5595061779022217\n",
            "epoch 438, loss 2.549617052078247\n",
            "epoch 439, loss 2.5397591590881348\n",
            "epoch 440, loss 2.5299315452575684\n",
            "epoch 441, loss 2.5201237201690674\n",
            "epoch 442, loss 2.5103678703308105\n",
            "epoch 443, loss 2.500614881515503\n",
            "epoch 444, loss 2.4908955097198486\n",
            "epoch 445, loss 2.4812090396881104\n",
            "epoch 446, loss 2.4715416431427\n",
            "epoch 447, loss 2.461932420730591\n",
            "epoch 448, loss 2.4523427486419678\n",
            "epoch 449, loss 2.44279408454895\n",
            "Pre normalize: \n",
            "tensor([[-49.5827, -22.8069, -47.1761,  14.8136,   4.1924,  -0.3524, -43.3698,\n",
            "         -29.4958,  50.9788,  33.5228, -30.1398, -57.1212,  45.6962, -11.5288,\n",
            "         -56.9187, -41.3285, -41.6563,  83.0779,  93.4851,  -0.1164]])\n",
            "tensor([39.6511])\n",
            "After normalize: \n",
            "tensor([[-53.8295, -24.7603, -51.2167,  16.0824,   4.5515,  -0.3826, -47.0844,\n",
            "         -32.0221,  55.3451,  36.3940, -32.7212, -62.0136,  49.6101, -12.5162,\n",
            "         -61.7938, -44.8683, -45.2242,  90.1935, 101.4921,  -0.1263]])\n",
            "tensor([43.0472])\n",
            "c value = tensor([1.0856], grad_fn=<MulBackward0>)\n",
            "epoch 450, loss 2.8679981231689453\n",
            "epoch 451, loss 2.8566842079162598\n",
            "epoch 452, loss 2.845223903656006\n",
            "epoch 453, loss 2.833566427230835\n",
            "epoch 454, loss 2.8217689990997314\n",
            "epoch 455, loss 2.8098206520080566\n",
            "epoch 456, loss 2.797804594039917\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-9029d7377396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madam_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOWER_BOUND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUPPER_BOUND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_POINTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-c7a424b33c57>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(self, num_epochs, v_x, optimizer, lower_bound, upper_bound, n_points)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             loss = epsilon_Loss(v_x, self,\n\u001b[0;32m--> 135\u001b[0;31m                                 lower_bound, upper_bound, n_points)\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0my_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mx_epochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-614fe69b05b7>\u001b[0m in \u001b[0;36mepsilon_Loss\u001b[0;34m(v_x, model_u, lower_bound, upper_bound, n_points)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mu_xi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mu_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_u\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_prime_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mv_xi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c7a424b33c57>\u001b[0m in \u001b[0;36mu_prime_2\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mu_prime_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mi_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mclone_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictiter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce_ex__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mreductor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                         \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreductor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                         \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "graph_val = model.train_network(num_epochs, given_fn, adam_optimizer, LOWER_BOUND, UPPER_BOUND, N_POINTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p7j57wogg0DM",
        "outputId": "c6d34283-0c83-48df-8a7e-5fcb8274b3c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 29.638412475585938\n",
            "epoch 1, loss 0.7530441284179688\n",
            "epoch 2, loss 8.83231258392334\n",
            "epoch 3, loss 14.829105377197266\n",
            "epoch 4, loss 9.399107933044434\n",
            "epoch 5, loss 2.5251247882843018\n",
            "epoch 6, loss 0.783149778842926\n",
            "epoch 7, loss 3.5676965713500977\n",
            "epoch 8, loss 6.131922245025635\n",
            "epoch 9, loss 5.697638988494873\n",
            "epoch 10, loss 3.2091867923736572\n",
            "epoch 11, loss 1.0375869274139404\n",
            "epoch 12, loss 0.6671668887138367\n",
            "epoch 13, loss 1.7899820804595947\n",
            "epoch 14, loss 2.950183153152466\n",
            "epoch 15, loss 3.0038504600524902\n",
            "epoch 16, loss 1.9651466608047485\n",
            "epoch 17, loss 0.7191212177276611\n",
            "epoch 18, loss 0.17536747455596924\n",
            "epoch 19, loss 0.5628992915153503\n",
            "epoch 20, loss 1.3318476676940918\n",
            "epoch 21, loss 1.712721347808838\n",
            "epoch 22, loss 1.3891135454177856\n",
            "epoch 23, loss 0.6655987501144409\n",
            "epoch 24, loss 0.10835736244916916\n",
            "epoch 25, loss 0.06493289023637772\n",
            "epoch 26, loss 0.4339546859264374\n",
            "epoch 27, loss 0.8138361573219299\n",
            "epoch 28, loss 0.8680288791656494\n",
            "epoch 29, loss 0.5795365571975708\n",
            "epoch 30, loss 0.20512911677360535\n",
            "epoch 31, loss 0.023354578763246536\n",
            "epoch 32, loss 0.11108587682247162\n",
            "epoch 33, loss 0.31786248087882996\n",
            "epoch 34, loss 0.4297225773334503\n",
            "epoch 35, loss 0.35815370082855225\n",
            "epoch 36, loss 0.18807099759578705\n",
            "epoch 37, loss 0.06873288005590439\n",
            "epoch 38, loss 0.07206326723098755\n",
            "epoch 39, loss 0.1475798785686493\n",
            "epoch 40, loss 0.1949758529663086\n",
            "epoch 41, loss 0.16650767624378204\n",
            "epoch 42, loss 0.09911731630563736\n",
            "epoch 43, loss 0.059614889323711395\n",
            "epoch 44, loss 0.07203859835863113\n",
            "epoch 45, loss 0.1017376258969307\n",
            "epoch 46, loss 0.10239598900079727\n",
            "epoch 47, loss 0.06762576103210449\n",
            "epoch 48, loss 0.0326099768280983\n",
            "epoch 49, loss 0.030685752630233765\n",
            "epoch 50, loss 0.056496813893318176\n",
            "epoch 51, loss 0.0747603178024292\n",
            "epoch 52, loss 0.060948945581912994\n",
            "epoch 53, loss 0.027064982801675797\n",
            "epoch 54, loss 0.006036245729774237\n",
            "epoch 55, loss 0.014994855038821697\n",
            "epoch 56, loss 0.03853122517466545\n",
            "epoch 57, loss 0.04809630662202835\n",
            "epoch 58, loss 0.033302295953035355\n",
            "epoch 59, loss 0.010396413505077362\n",
            "epoch 60, loss 0.0016615737695246935\n",
            "epoch 61, loss 0.011415498331189156\n",
            "epoch 62, loss 0.024490607902407646\n",
            "epoch 63, loss 0.025417765602469444\n",
            "epoch 64, loss 0.014577154070138931\n",
            "epoch 65, loss 0.0045870947651565075\n",
            "epoch 66, loss 0.004371676594018936\n",
            "epoch 67, loss 0.010449335910379887\n",
            "epoch 68, loss 0.013566468842327595\n",
            "epoch 69, loss 0.010259932838380337\n",
            "epoch 70, loss 0.0053145089186728\n",
            "epoch 71, loss 0.0042007421143352985\n",
            "epoch 72, loss 0.006422328297048807\n",
            "epoch 73, loss 0.007507305592298508\n",
            "epoch 74, loss 0.005495905876159668\n",
            "epoch 75, loss 0.003012202912941575\n",
            "epoch 76, loss 0.0030491650104522705\n",
            "epoch 77, loss 0.0048718536272645\n",
            "epoch 78, loss 0.005376758053898811\n",
            "epoch 79, loss 0.0034737340174615383\n",
            "epoch 80, loss 0.0013928982662037015\n",
            "epoch 81, loss 0.0014960994012653828\n",
            "epoch 82, loss 0.0031909800600260496\n",
            "epoch 83, loss 0.003948760684579611\n",
            "epoch 84, loss 0.0027013125363737345\n",
            "epoch 85, loss 0.0010122311068698764\n",
            "epoch 86, loss 0.0007927929400466383\n",
            "epoch 87, loss 0.0018884253222495317\n",
            "epoch 88, loss 0.002592001808807254\n",
            "epoch 89, loss 0.002003022935241461\n",
            "epoch 90, loss 0.0009785189758986235\n",
            "epoch 91, loss 0.0007269805646501482\n",
            "epoch 92, loss 0.0012451699003577232\n",
            "epoch 93, loss 0.0015985197387635708\n",
            "epoch 94, loss 0.0013109331484884024\n",
            "epoch 95, loss 0.0008499289397150278\n",
            "epoch 96, loss 0.0007805106579326093\n",
            "epoch 97, loss 0.0009975687135010958\n",
            "epoch 98, loss 0.0010421830229461193\n",
            "epoch 99, loss 0.0008282057824544609\n",
            "epoch 100, loss 0.0006799691473133862\n",
            "epoch 101, loss 0.0007667739992029965\n",
            "epoch 102, loss 0.0008673586416989565\n",
            "epoch 103, loss 0.000760406197514385\n",
            "epoch 104, loss 0.0005692706326954067\n",
            "epoch 105, loss 0.0005460967076942325\n",
            "epoch 106, loss 0.0006790569750592113\n",
            "epoch 107, loss 0.0007287681219168007\n",
            "epoch 108, loss 0.0005988830816932023\n",
            "epoch 109, loss 0.0004621103289537132\n",
            "epoch 110, loss 0.0004839449538849294\n",
            "epoch 111, loss 0.0005893782363273203\n",
            "epoch 112, loss 0.0006009271019138396\n",
            "epoch 113, loss 0.0005003077676519752\n",
            "epoch 114, loss 0.00042569200741127133\n",
            "epoch 115, loss 0.0004523090901784599\n",
            "epoch 116, loss 0.0005027555162087083\n",
            "epoch 117, loss 0.00048763316590338945\n",
            "epoch 118, loss 0.00043164900853298604\n",
            "epoch 119, loss 0.0004103511164430529\n",
            "epoch 120, loss 0.00043027271749451756\n",
            "epoch 121, loss 0.00043802522122859955\n",
            "epoch 122, loss 0.00041450138087384403\n",
            "epoch 123, loss 0.0003942440089304\n",
            "epoch 124, loss 0.00039801938692107797\n",
            "epoch 125, loss 0.00040226118289865553\n",
            "epoch 126, loss 0.00038656385731883347\n",
            "epoch 127, loss 0.0003683741088025272\n",
            "epoch 128, loss 0.0003694243496283889\n",
            "epoch 129, loss 0.00037818096461705863\n",
            "epoch 130, loss 0.00037125626113265753\n",
            "epoch 131, loss 0.00035273635876365006\n",
            "epoch 132, loss 0.0003453445970080793\n",
            "epoch 133, loss 0.0003524963103700429\n",
            "epoch 134, loss 0.00035499889054335654\n",
            "epoch 135, loss 0.0003435210383031517\n",
            "epoch 136, loss 0.00033138817525468767\n",
            "epoch 137, loss 0.00033071008510887623\n",
            "epoch 138, loss 0.00033445103326812387\n",
            "epoch 139, loss 0.0003308281593490392\n",
            "epoch 140, loss 0.0003219328646082431\n",
            "epoch 141, loss 0.00031732465140521526\n",
            "c value = tensor([62.8300], grad_fn=<MulBackward0>)\n",
            "LOSS VALUE = tensor([1.2546], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "graph_val_pen = model.train_network_with_penalty(num_epochs, given_fn, adam_optimizer, LAMBDA_PEN, LOWER_BOUND, UPPER_BOUND, N_POINTS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Model result using epsilon loss function with penalty\n",
        "print(\"Penalty lambda value = \" + str(LAMBDA_PEN))\n",
        "print(\"Interval is [{lb}, {ub}]\".format(lb=LOWER_BOUND, ub=UPPER_BOUND))\n",
        "# Plot 1: The epsilon_loss with lambda penalty:\n",
        "\n",
        "# Plot 2: The epsilon_loss value when using train with penalty:\n",
        "\n",
        "# Plot 3: the model's output with input from the interval [LOWER_BOUND, UPPER_BOUND]:\n",
        "x_vals = [torch.tensor([i], requires_grad=True, dtype=torch.float) for i in x_train]\n",
        "\n",
        "model_output = []\n",
        "for i in x_vals:\n",
        "    model_output.append(model(i).detach().numpy().item())\n",
        "\n",
        "plot_figure(x_values, model_output)\n",
        "# model_output"
      ],
      "metadata": {
        "id": "Hl5KH2lsp642",
        "outputId": "3bfe090c-acdb-434c-f9e2-21d31242fa91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Penalty lambda value = 100\n",
            "Interval is [-6, 6]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "jd2I0gRqHO8Z",
        "outputId": "15978184-99da-4f52-d206-d0deff5669aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGMCAYAAAD0nYndAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde5ScV3nn++/z1rXv3Wq1rq2bLcmWr/JFvmEMMRAgJxkMATsJJDiYGCeTWWRYrEkyw+EMOcmsnJm1PJCZQwj3MeAkkwQSzOESbGzABoyxLdmWJUvWvXVXt/re1XXb54+qt9VqVXW/71vVltT1+6zVy13vu99du9p/1KO99/Nsc84hIiIiciHyzvcARERERKpRoCIiIiIXLAUqIiIicsFSoCIiIiIXLAUqIiIicsFSoCIiIiIXLAUqIiIicsGKn+8BBJFKpVxPT8/5HoaIiIiEcPjw4axzLlVLHxdFoNLT00NfX9/5HoaIiIiEYGYna+1DSz8iIiJywVKgIiIiIhesi2LpR0REJIpisYjOtJs/Zobnze+chwIVERFZcIrFIgcOHCCTyZzvoSx46XSaNWvWzFvAokBFREQWnBMnTuB5Hhs2bMDMzvdwFiznHIcPH+bEiRMsW7ZsXt5DgYqIiCwozjkGBwdZu3Yt8bi+5ubb0qVL2b9/P0uXLp2XoFCbaUVEZEFxzuGcI5FInO+hNIREIjH1N58PClRERGRB0ebZ80OBioiIyEVo8+bNbN68mSuuuIJYLDb1+p577nlN3v/d7343X/7yl+ds9+Uvf5mdO3fO/4BC0uKdiIjIPNq6dSsA+/fvZ/PmzVOvp8vn8+d9P82Xv/xlOjs7ufzyy8/rOGbSjIqIiMh5sHbtWv74j/+Ym266ife///088cQTbN68eer+Sy+9xNq1a6def+973+P222/nhhtu4KabbuLxxx+v2O/OnTu57bbbuPLKK7nrrrsYHh6euvfwww9z8803c91113HttdfyyCOPAPD5z3+eX/ziF/z7f//v2bx5M9/+9rd58cUXuf3227n++uu54oor+PM///P5+UPMQTMqIiKy4D300/0Vr/+ba1fQ2ZxkcDzLN7cdqdjmd25dC8D+U2P8aPfJivei6u/v5+mnn8bMeOKJJ6q227t3L//5P/9nvve979He3s6rr77K61//evbv308qdfaZf7/927/NAw88wH333ceLL77IjTfeyG/91m8B8Na3vpXf/M3fxMzYv38/t9xyCwcOHOCDH/wgX/3qV/mjP/oj7rrrLgBGRkZ47LHHSKVSTExMcNttt/HmN7+ZW265pabPHJYCFRERkfPk3nvvDZTS+93vfpdXX32VO+64Y+qa53kcPHiQDRs2TF0bHh5m69at3HvvvQBcffXV3H777VP39+3bx3vf+176+vqIx+MMDAywb9++iss9ExMT/MEf/AFbt27F8zwOHTrE1q1bFaiIiIjU21wzH53NyTnbrF3cwtrFLfUbFNDa2jr1ezwep1AoTL2eXlXXOcdb3vIWHn744dDvMT0Q+o3f+A3+8i//kne/+90ALFq0qGr13v/4H/8jixcv5vnnnycej/Oud73rvFT6bdg9KjuODvPPzx9mOJM730MRERHhkksu4cCBA5w8WVpe+spXvjJ1761vfSuPPvooL7zwwtS1n//85+f00d7eznXXXcdDDz0EwPbt23nyySen7p8+fZp169YB8NWvfpXTp0+f9ezQ0NBZbXt7e4nH47zyyit8//vfr9MnDadhA5XB8Rz7To2RzRfP91BERERYsWIF/+E//AduuukmbrnlFhYtWjR1b/369Tz88MN86EMf4tprr2XTpk188pOfrNjPQw89xGc/+1muuuoqPvaxj521XPSpT32Kd7/73Vx33XU8//zzrF69eure/fffz3/5L/9lajPtxz72Mb70pS9xzTXX8Cd/8ifceeed8/fhZ2EXQ2Gc3t5e19fXV9c+n97bz0/29PPeW1azpC1d175FROT8KRQK7Nq1i40bNxKLxc73cBa82f7eZnbYOddbS/8NO6PieaU1u4sgThMREWlYjRuolPcWFRWpiIiIXLAaNlDxd0EXFaeIiIhcsBo2UFnZ2cTrNyymLa0MbRGRhcT/h+jFsAdzIfD/zkHqwUTRsN/SS9vTLG3XJloRkYXG8zwSiQT9/f10d3fP2xeolIKU/v5+EokEnjc/cx8NG6iIiMjCtXr1ag4ePMjAwMD5HsqCl0gkzkpzrreGDVRePTHCD3ed4i2blrK6u/l8D0dEROoomUyyfv16isWiloDmkZnN20yKr2EDlVzBMTyRI1tQwTcRkYVqvr9EZf417P9BT5utRERELngNHKiU/qv0ZBERkQtXwwYqZ+qo1BapHBvK8N+/v4tDA+P1GJaIiIhM08CBSum/NQcqw6Ujr3ceG6l1SCIiIjJDwwYqy9rTvGPzClYvqi3jZ93iFgCS8Yb9U4qIiMybhs36aUnFuaSnteZ+WlNxzGB4IleHUYmIiMh0DTsN4Jwjmy9SqHE37fYjQzgHB7VHRUREpO4aNlA5MpTh/338VV7oG6ypn5FMHoBsXvVYRERE6q1hAxX/5Ida05Nz5YJx776ht7aORERE5BwNG6jUq+BbvlB6vqctVfOYRERE5GyBAxUz+xUze87MtprZS2b2/vL1JWb2XTPbXb5+R5XnrzazH5nZznK7L5pZU70+SFj1KviWL5ZmVIYmcoxn8zWOSkRERKYLFKhYqTraV4F7nXObgV8F/sbM2oC/BH7mnNsA/C7wsJklKnSTAf7QOXc5cC3QAvxxHT5DJPUq+JYrz6g8/PRBdh8frXlcIiIickaYpR8HdJZ/bwf6gUngbuAzAM65Z4AjwBvOedi53c65F8q/F4BngLVRB14rr04F365f08Wtl3YDMJxRirKIiEg9Baqj4pxzZnYP8HUzGwO6gHcBbUDCOXdsWvP9wOrZ+jOzFuCDwJ9GGXQ9dDYn+cDt60jVWKhtZWcTS9tS/HRPP8MTWvoRERGpp6BLP3HgY8C7nHNrgDcBXyFCwTgzSwJ/D/yrc+4bVdp8xMz6/J/R0fovqcQ8o6MpQToRq6kf5xzxmEdrKq4ZFRERkToLGmhsBlY4534EpSUeM+sDrgHyZrZs2qzKWuBgpU7Ke1f+HjgKfLjamznnHgQe9F/39vbW/YzjQtFxcmSSpmSMjqZKW2qCeeinB4jHjPamOEOqTisiIlJXQdc9DgHLzWwTgJmtBy4FXgH+AXigfH0LsBL44cwOyrMyfwcMAPe7WvOCazSRK/C3Pz/IcwdP19RPrlDEM6M9ncA5aq50KyIiImcE3aNy3MzuB/63mRUpBTh/6Jw7aGZ/DHzFzHYDWeB9zrkcgJn9GXDEOfcZ4B5K+1peAJ4vZ9085Zz7t3X/VAH4m2lrrqNSdMQ9461XLsPzOxUREZG6CLzHxDn3t8DfVrh+HPjlKs98fNrvXwO+FmGM88Iv+FassfJ9Ll8kEfMUpIiIiMyDhq1Ma3VIT3bOkS86EjGPiWyBF/uGODaUqdMIRUREpGEDlakS+jX04Rd7i8eMiVyBR3ccZ89JFX0TERGpl9DpxQuFZ0YiZlMBSxRxz/idW9cQj3k0J0tpzsPK/BEREambhg1UYp7xh3duqKkPzzO6W88cRticjKmWioiISB017NJPPeQKRU6NTpLJFQBob0qoOq2IiEgdNXSg8vKRYQ72j0d+vn80y1d+eoAXDw8B0J5OMDqZJ1+oMZVIREREgAZe+gF4bMdxVnc3s7q7OdLzuXJAEi+nJi/vTJMvFskVHPHaKvOLiIgIDR6oeJ7VlJ6cL1ehTcRKE1PXr+7i+tVddRmbiIiINPjSj1ltBd/8JR4/UBEREZH6auhvWM9qm1GZXkcFYCJb4Hvbj7H9yFBdxiciItLoGjxQgVqO+vEPIEx4pT+j55U26B4amKjH8ERERBpeQ+9RWdKWpiUV/U9wdW8HV61sn3qdisdIJ1RLRUREpF4aOlC567qVNfdhMyrbtjfFVZ1WRESkThp66adWJ4Yz7D4+QjZ/ZkduayrOeLZwHkclIiKycDR0oPLsgQF+sX8g8vPbjw7zrReOkp1W4C3ueRSKjmKxluMORUREBBp86Wf7kWGcgxvXLor0fN7P+vHOLP9sWNpKd2uyplOZRUREpKShAxUzo1BDIZVKdVQ2Lm2reVwiIiJS0tBLP7WmJ+eKDs+MmGdzNxYREZHQGjxQqbGEfqE4VezN99zB0/yvn+xnSJk/IiIiNWvwQKW2GZWWVJye1tRZ1zK5AgNjWZ2gLCIiUgcNvUdl3eJWxrP5yM+/9cpl51yLl6vUFpT1IyIiUrOGDlRuWhct22c2/n6VvAIVERGRmjX00k+tntk/wEuHzz6A0E9V1oyKiIhI7Ro6UPnpnn7+ZevhyM//Yv9pdh4bOeuaZlRERETqp6EDlRMjGQ70j0d+Pl8okpiR9bOqq5lfvWY5PW2pKk+JiIhIUA29R6WW9ORi0ZEvuqnNs76O5gQdzYl6DE9ERKThNfSMimeGc+AiBCu5ol+V9txib87prB8REZF6aPBApfTfKDGFf87P9PL5AIcGxvnko7t5ccYmWxEREQmvoQMVs1KkEmX5xzPj8mVtLGk/ey+Kp820IiIiddPQe1QuW9ZGT1sSz8Kf1dOUjPH2q5efc91PT66lNL+IiIiUNHSgsm5xC+sWt9S1z6n05IICFRERkVo19NJPLU6MZPjmtiMc6B8767oKvomIiNRP4EDFzH7FzJ4zs61m9pKZvb98fYmZfdfMdpev3zFLH79qZjvLbb9uZu31+BBR/XRPP5//8d5I5/2MTRbYc2KUkczZz54p+KZDCUVERGoVKFCx0q7TrwL3Ouc2A78K/I2ZtQF/CfzMObcB+F3gYTM7p5CImbUCXwDuKrc9Avyf9fkY0WQLRUYy+UizH7mCn5589p+wJRnnd25dMy/nCImIiDSaMEs/Dugs/94O9AOTwN3AZwCcc89QCkDeUOH5twPPO+d2ll9/GvjNCGOum1rSk88EKmdvxPU8o7s1RXOyobf/iIiI1EWgb1PnnDOze4Cvm9kY0AW8C2gDEs65Y9Oa7wdWV+hmNXBgRrvlZhZ3zoVfe6kDP9snSsG3anVUAE6NTpLwPFWoFRERqVHQpZ848DHgXc65NcCbgK8wT1lDZvYRM+vzf0ZHR+fjbfDnQiIVfCvvQYlXqEz7tZ8d5Ie7T9YwMhEREYHgSz+bgRXOuR/B1BJPH3ANkDezZdPargUOVujjILBmRrujlWZTnHMPOud6/Z/W1taAwwynloJvyzuauPXSbtrS586axGNGQZtpRUREahY0UDlEaZlmE4CZrQcuBV4B/gF4oHx9C7AS+GGFPr4LXG9ml5df/wHwd9GHXrvLl7XxzutW0pYOPzG0orOJWy7ppjV17rMxz1RHRUREpA6C7lE5bmb3A//bzIqUApw/dM4dNLM/Br5iZruBLPA+51wOwMz+DDjinPuMc27EzD4I/HN5Kekl4P3z8aGC6mpJ0tWSrHu/cc9UR0VERKQOAk8lOOf+FvjbCtePA79c5ZmPz3j9TeCbIcc4bwpFR75YJOF5U2f0BPXj3SfZe3KM37xpNcn42RNTMc901o+IiEgdNHRl2m19g3z68T0cHc6EfnYkk2dgLDtV4G26uGcoTBEREaldQxf78NOTixELvsU8qxiovO+WNVMbdUVERCS6hp5R8WOMKAcd5wuuYmoyoCBFRESkTho8UImenuzvbamk7/Q4rxwbqWlsIiIi0uBLPzZVQj98oJKdZUblF/tP03d6nMuWtdUyPBERkYbX0IHKVAn9CM++cWNP1QDHz/pxzmkZSEREpAYNHahc2tPKfa9fR1MiFvrZVYuaq96Le4ZzpdL8VSZdREREJICGDlSSce+cGihBZfNF4p5VrL/iZwLli0ViXvggSEREREoaejNtJlfg6NAE49nwhzd/5od7eOSFIxXv+XtXVJ1WRESkNg0dqBwenODvfn6I/afGQz1XLDoKRUciVvnP15ZO0NOWipT2LCIiImc09NJP1PTkXPlk5HiVsvtb1i5iy9pFtQ1OREREGntGJWrBN/9k5GozKiIiIlIfDf1NG3VGxQ9UqtVROTw4wVOvnmI4k6ttgCIiIg2uoQOVqAXf/KWfajMqx4cz/HzfAMMTClRERERq0dB7VGKekYx7oYuydTUn+cDr1lVNbfb3rijrR0REpDYNHags72ji3/7S+tDPxTyjozkx631QoCIiIlKrhl76iWoiW+D4cIZMrlDxfrx8WKECFRERkdo0dKCSyRV46fAQJ4YzoZ47dHqch58+yMGByvVXzlSmVaAiIiJSi4YOVMYm83z/5eO8enI01HO5wux1VNqb4ly+rI22dEOvrImIiNSsob9Jp05PDjnxkZujjsqStjRvv3p5TWMTERGRBp9RiV5HZfb0ZBEREamPhv6mtfKnD7uVJDdHwbehiRzf3HaEnceGaxmeiIhIw2vsQKX837AzKlDan5LwKv/5CkXHnhOjDIxmaxidiIiINPQelbjnsawjTXvITa+3XtrNrZd2V72vrB8REZH6aOhApSkZ4zdvWl33flWZVkREpD4aeuknqgP9Y7x8ZBhXZclIMyoiIiL10dCBSqHo+NGuk6E3vW49NMijO45XPSPozIxKseYxioiINLKGDlScczx74DT7T42Fei5XcFUzfqA0o3Lbpd1sWNpW6xBFREQaWkPvUTlTRyXcc/lCkeQsNVTMjJsvqb7ZVkRERIJp6BkVf+UmbHpyruiqls8XERGR+mnwQMXwzCLNqMTnqEr7988c5FsvHKlhdCIiItLQSz8AnlE1e6eaRS1J0onYrG1GMvnQAZCIiIicLVCgYmbdwGPTLjUDlwBLgEuBTwKtgAM+4pz7QZV+3g98FCiU2/4n59y3I4++Dq5Y0U5XSzLUM+/YvHLONnHPlJ4sIiJSo0CBinOuH9jsvzazjwJvAE4D3wDudc49amYbgUfN7DLn3MT0PsxsEfA/gI3OuWNmdjvwdUrBznnzpk1L56XfWMyjUFB6soiISC2i7lG5D/gC0A30OOceBXDO7QIGgbdXeS8D/JzdTqAv4vufN845nnjlBDuOzl57RTMqIiIitQsdqJjZbUAX8C3n3CngqJndXb63BbgMWDvzuXLbB4DnzOwA8EXg3sgjr5PvvHiUH+46Gbh90cHzBwfnrL0S80wl9EVERGoUZTPtfcBDzrl8+fU7gP/HzP4U2A48CeRnPmRmHcCHgZucczvM7NeAb5jZJudcdkbbjwAf8V93dHREGGYwhwcnaAtxKGG+XG3WmyM9+bZLuxWoiIiI1ChUoGJmrcDdwBb/mnNuG/C2aW12UApYZnoLMOic21F+7hEz+yKwBtg9vaFz7kHgQf91b2/vvH3jW8j0ZL8qfqxK+Xxfb1dzDaMSERERCL/0cw+wzTm3079gZsun/f57wBhQKetnL7DZzJaV295KKVA6FHbQ9eRZuIJvhXLb2Cwl9AGKRUc2Xwyd+iwiIiJnhF36uQ/43Ixr95vZeyltlN0BvNOVv53N7AFghXPu486558zsL4AfmFmO0vLQ3c65TG0foTZhC775yzlzzaj868vH2HF0hD+8cz2JOYIaERERqSxUoOKcu63CtU8An6jS/jMzXn8K+FSY95xvYQu+JWLGVSs7WNaRnrVdzCtNVhWKjjlqw4mIiEgVDV+Z9vo1XYRZnWlOxnnLFXPXXvHPAlKKsoiISHQNH6hcuWJ+Mopi5UBFmT8iIiLRNfShhFGcGp3k68/18eqJkVnbxRWoiIiI1KzhA5VHth3ha08fCNx+IlvgQP84o5OFWdvFppZ+VEZfREQkqoZf+pnIFRjNnFOfrio/lXmurJ9rV3Vy+bJ2WkMUkxMREZGzNfy3aNT0ZG+Ouah0IkZa6T4iIiI1afiln7AF36ZmVOYooZ/JFTg+nCGTm32JSERERKpToGIWqo5KPmDBt32nxnj46YP0nZ6oaXwiIiKNrOEDFTNCLf0saUtzx8bFLG5NzdpOWT8iIiK1a/g9Krdc0s3mVZ2B2y9qSbKoZdGc7ZT1IyIiUruGD1SWts9eCj+q+LQS+iIiIhJNwy/95AtFMrlC4H0qL/QN8vkf7+XE8OxnKfqnK6uEvoiISHQNH6g8uuMEf/3EnsABRSZXZCSTZ67Wcc+IexbqHCERERE5W8Mv/fjJO0FTlKfqqMyR9bO0Pc2/e9OGmsYmIiLS6Bp+RsUPOILOfPgBTXyOOioiIiJSOwUq5XgjaKBypjLt7IFKrlDk5SPDHBlUHRUREZGoFKiUZ1QCL/0ErEybKxT53vZj7Dg6XNsARUREGpj2qITco3JtbyfrultIx2eP8WIq+CYiIlKzhg9UbrmkmxvWdNGSDPanKBV8S87ZTnVUREREatfwgUrYU46z+SIORzLmYbNk/nhWmq1RHRUREZHoGn6Pykgmx5HBCXKFYKXuv//ycT79+J4525mV6qhoRkVERCS6hg9UXjo8zN8/c4ihiVyg9vlikZhns86m+Ja2p+loStQ6RBERkYbV8Es/XsjNtEXn5sz48b3nxlVRhyUiIiJoRmWqHkrwOipzV6UVERGR+lCgEnZGpeiIBfyrPXvgNM/sH4g4MhEREWn4QMWmCr4Fa19wLvCMys5jw7zYNxR1aCIiIg2v4feoxD0jOUfxtunu2rxyqjrtXGKmrB8REZFaNHygck1vJ9f0dgZu35QMXnMl5lngoEZERETO1fBLP2EdG8pwcmQyUNt4TDMqIiIitWj4QOX0WJaXDg8FrqPyrReO8NiO44HaxjyPfEGBioiISFQNH6gcHcrw/ZePc2o02CxJoeimUprnsnpRM1esaMdp+UdERCSSht+jUj47MHAwUXCOWMCsn82rgu99ERERkXMFmlExs24z2zrtZ5eZ5c1skZltMbOnzGxb+d6ds/TTZWZfKz+/3cz+sn4fJRovZHpyseiIx1TwTURE5LUQaEbFOdcPbPZfm9lHgTcAp4FvAPc65x41s43Ao2Z2mXNuokJXXwSecs69t9zPslo/QK3CFnzLF4PXUXnp8BC7jo/wtquW0Zxs+MkrERGR0KLuUbkP+ALQDfQ45x4FcM7tAgaBt898wMzWAzcCD/rXnHPHIr5/3UwVfAtweHKx6EjFY4HrrgyO5zjQP042H+xkZhERETlb6EDFzG4DuoBvOedOAUfN7O7yvS3AZcDaCo9eAfQBf21mz5rZv5rZdZFHXifpRIzlHWnSibn/FJ5n/P4bL+WtVwabCPIPL8wrRVlERCSSKDMq9wEPOefy5dfvAD5gZs8DHwaeBPIVnosDNwF/55y7AfjvwLfMLDGzoZl9xMz6/J/R0dEIwwxmZWcTv3HTai7paa173/5eFtVSERERiSbUxgkzawXuBrb415xz24C3TWuzA9he4fGDwGHn3OPl575jZklgDfDq9IbOuQeZtkTU29t7QXzT5wpFdhwdZnFrihWdTXO214yKiIhIbcLOqNwDbHPO7fQvmNnyab//HjAG/KDCs88Cw2Z2TbntTYABh8IOup6GxnP8cNdJDg2Mz9k2kyvw2I4T7D4RbIYnXg5UCir6JiIiEknYVJT7gM/NuHa/mb2XUtCxA3inKxclMbMHgBXOuY8755yZvR/4nJk1AZPArzvnglVamydj2TzPHThNayrOqkXNs7b1l3CC1lFZ1pHmjo2L6Wg+Z3VLREREAggVqDjnbqtw7RPAJ6q0/8yM188CN4d5z/nmpxoHKfjmBypewHmoJW1plrSlI49NRESk0TV8CX2/jkqQxRn/JOSgMyoiIiJSm4YPVPALvgXY8OrXWokFPOvn0MA4n//xXnYcHY46OhERkYbW8IFKmBL6Mc9Y2dlEWzr4npORTJ5JFXwTERGJpOHruqcTMa5a2UFPW2rOtj1tKe7esipw3/7Mi+qoiIiIRNPwgUprKs5brlg6L33HFaiIiIjUpOGXfsI4MZLh8VdOcGI4E6j9mYJvWvoRERGJouEDlYlsga8/18e2Q4Nzth0cz7H14CCDE7lAfcfLecyaUREREYmm4Zd+is5xoH+crubknG2n6qgETE9uTsW467qVdDap4JuIiEgUDR+onMn6CV7wLWh6ciLmsW5xS/TBiYiINLiGD1T8yZEgqzNhS+g758gWihhGMt7wq2wiIiKhNfy3Z6gZFReuhL5z8OnH9/D9l49HHp+IiEgjU6Dil9APEKgsbklx9coOWlPBJqI8z/DMlPUjIiISUcMv/cQ8446NPXS3zL2ZdnV3M6u7Zz9heaZ4zJT1IyIiElHDBypmxg1ruuat/5hn5BWoiIiIRNLwSz9hvNA3yD8928foZD7wM3HPAh14KCIiIudSoAJ86al9fPelY3O2GxjLcnBgPNDGW59mVERERKJr+KUfgPFsgYnc3LMkfoASND0Z4D03rgrVXkRERM5QoEKplkqQxJxCuU3Qgm9A4AwhEREROZeWfijVUgmyOBO2Mi1A/+gkR4cmIo5MRESksSlQAYxgBd+iLP088cpJ/vn5I1GHJiIi0tC0LkF5RiVAoHLF8naWdaTxQsyoxDyjoIJvIiIikShQAd521TISsbknl9YubmEt4Q4Z9LN+nHOYNtWKiIiEokAFWLUoXLXZMOKe4Vzp0MOY4hQREZFQtEcFyOaLZHKFOdv9y9bDfOVnB0L17W+8rfW8n5+8eopn9g/U1IeIiMjFRjMqwN89cxCA37l17aztJrIFJgMENNMl4x7pRCxQ+vNsnt5XClJuWN0Vao+MiIjIxUyBCqXzfgqFuSOJgnN4IfeZvPGyJbzxsiVRhwZwVgn+k6OTLG1P19SfiIjIxUJLP4BnpT0kcykWXagaKvXiecZd160EoO/0+Gv+/iIiIueLAhVK6clB6qjkiy70ssuJkQwv9A0ykQ23ZDRTb1cTcc8YHM/V1I+IiMjFREs/lGZUgpwzWCg6UvFwgcqB/nGe3H2KJW1pmpKxSOMbncxzbCjDb928mu7WVKQ+RERELkaaUaG0RyXIjMqvXL2cOy8Pt9/EXyoqhDhxeaajgxM8su0IJ0cnI/chIiJyMdKMCvCr1xjTzXAAACAASURBVCwPNKOyorMpdN9xP1ApRA9UxsvLRnHPY+uhQVpTMdYvaYvcn4iIyMVCMypAczJOS4BTjsezebL5cHnG9aijMlFOiW5JxfjxrpO80DcUuS8REZGLSaBAxcy6zWzrtJ9dZpY3s0VmtsXMnjKzbeV7dwbo7xNm5sxsc+0foXYDY1mODM59wvHnf7yP77x0NFTfca/0Jy4ESSuqwg9U2tIJlnWkOTI4UVN/IiIiF4tAgYpzrt85t9n/AT4LfAc4DXwD+L+cc9cCdwNfNrOqayRmdhOwBQhX4nUe/Xj3Sf7p2b5Z2zjnKERIT25OxljZ2UQqHm0jLTCVMdSUiLFqUTO5guPYcCZyfyIiIheLqEs/9wFfALqBHufcowDOuV3AIPD2Sg+ZWTPwP4EPRXzfeVHaTDt7G/9+LGTBt1WLmrl7yypWd0c/TygV9+huTRLzjN6uUgzYN6B6KiIisvCF3kxrZrcBXcC3nHN5MztqZnc75/63mW0BLgPWVnn8vwJ/7Zw7dCGdJFwq+Db7Ccf+Usv5KPj2pk1Lp35f1p4m7hmHTk9w82s+EhERkddWlBmV+4CHnHP58ut3AB8ws+eBDwNPAvmZD5nZW4A1zrkvzfUGZvYRM+vzf0ZHRyMMMzi/LP5smT9RA5XhTI7HXznBgf6xyOObLh7zuGFtFxuXttalPxERkQtZqBkVM2ultA9li3/NObcNeNu0NjuA7RUevxO43sz2l1/3At82sw855x6Z3tA59yDwoP+6t7d3XneO+rFH0Tk8qsyolKOYsJVpM9kCWw8O0pKMs6a7JfTYnHP8ZE8/S9vTrF9SCk5uu3Rx6H5EREQuRmFnVO4BtjnndvoXzGz5tN9/DxgDfjDzQefcnzrnVjrn1jrn1gJ9wK/MDFLOh0TMI5XwZt2n0pKM8aE3XMKtl3SH6rvW9OTJfJGf7xtg78n5nVUSERG5EIXdo3If8LkZ1+43s/cCBuwA3ulcafrBzB4AVjjnPl7zSOfRmzYtPWsfSCVmRnMyfH28WtOTM+XU5Onl9wfHs3zj+cNcvbKDG9cuitSviIjIxSDUN69z7rYK1z4BfKJK+8/M0tfaMO99vmXzRY4PZ+hoTtCeTgR+LhbzZ1SiBSp+VdrmaYFKrHw44ejkOVuBREREFhRVpgWODE7wQt8guUL15ZnB8Sz/+GwfrxwbCdV3rSX0/WJv6cSZQMWvyTIZskquiIjIxUaBCrDr+AiP7TgxtcxSiT8j4oVMq457xtUrOyKdEwRnir1NX3ZKxAzPbNbxioiILAQ6lJAzwcdsqzNR05PjMY83XzH7/pfZtKXjbFzaRkfTmeUmMyOV8DSjIiIiC54CFabXUakeqRTL98JWpq3Vmu6WimnNqbgCFRERWfgUqDC9jkr1Nv6Mihdhseyb247Q2ZTgjo09EUZX2Zs3LT0vVXJFREReSwpUYKpsfnGWGRUzoykZIxkLH6kcGZxgMuJ+kqdePcVIJs/brlp21vVVi6KfHSQiInKxUKBCaR/Iys6mqQydStYtbuGBN1waqf+4Z5HrqBwcGGesQhpysejIFoqk4l7V84lEREQudgpUgKtWdnDVyo556z/mWU11VKYXe/M9tvMELx0e4vffeOlZqcsiIiILidKTAzo5Msm2Q4OMZHKhn61lRiWTK9BUIRBJJ0r/6yZz2lArIiILlwIVoO/0OI+/coKh8epByOHBCX6w8wSDs7SpJuZ5kWZUcoUi2XzxrKq0vjNF31RLRUREFi4t/QCnRrNsPTjIhiWtdDRXLo8ftY4KwA1ruiIdSlipKq0vFS/PqChFWUREFjAFKpROUwSYJelnKiNotg231Vy2rC3CqCAZ87hjYw9L2lLn3Ev5Sz+aURERkQVMgQrTK9NWj1TyBb+OymuXYZNOxLhhTVfFe/7ST0Z7VEREZAHTHhXAAhR8q6Uy7fdfPs7f/HDPrJVvw+rtauKDr1/H5RFna0RERC4GClQINqPSnk6wsquJRDz8nyxfKDKeLcwaCFWy7dAgn//xXk4MZ865l4h5tKUTxCMUoBMREblYaOkH6GpJcPXKDtrS1f8cV/d2cHVvtFor/gbcfLFIzAte82R0Ms9IJl8xGCkWHcdHMiRjHt2t5+5hERERWQgUqADLO5pY3tE0b/3HY6VAJWwtlYlsaaNspToqBef4u58fYtPy9nPK64uIiCwUWjcIaPuRIR7feYJ8Ifzm1Vj5JMOwtVTGcwU8s6nibtPFPSPmmbJ+RERkQVOgAhwdmuCfnu3jQP9Y1TYH+8fZemgw0rk6/gbcQiFcoJLJFkgnKp/lY2ak4p7qqIiIyIKmpR9KKb4HB8ZnrXdSKG+0jZKdfNXKdtZ0N9M6yx6YSiZyhYpVaX0KVEREZKFToMKZ4GO27OFC0RHzLNKMSmdzks7mZOjn3nrlsqkAqZJUIlbxZGUREZGFQoEKwdKTi85FKp8PpfTkbKFIMuaFSide1pGe9X5LKs5kTntURERk4VKgwpmCb7PtICkUzwQ0Yb18dJjHdpzg16/vZXV3c6BnCkXHRPnk5GoB0r+5dkWk8YiIiFwstJmWYDMql/S0cNXK9kj9T6+jEtTp8Syf+9Fent7XH+k9RUREFgLNqADtTQnecFkPvZ3Va6lcv7rymTtBxMvpyWHqqMxWQ8V3fDjD4cEJrljeXvGEZRERkYudAhWgNRWvKRCZi78tJUwdlYny3pPmZPX/RQf6x3nq1VP0djYpUBERkQVJSz8BfefFo/zr9mORno3N04xKqnzukFKURURkoVKgApweK+0H+cX+gaptjg5lODk6Gan/uBe+hL4/o5JOVv9flEr4gYoyf0REZGHS0g+lbJ/RyTzZWWYmis5NVZgNa3lHmg+94RKSIVKT/VmSVHy2GZXSvUxOMyoiIrIwKVDhTMG32SY8CkWHF7GOSjxk/RSA113azU1rF00t71RyZulHMyoiIrIwKVCBqWqzs6Un54vRZ1RyhSLHhjK0peOBK9SWgpvZ27Qk4/R2NdGS0v9GERFZmLRHhekzKrNUpi1Gr0w7PlngH5/t48XDQ4GfOTo0Qd/p8VnbdDQneM+Nq7h8WbT6LiIiIhe6QIGKmXWb2dZpP7vMLG9mi8xsi5k9ZWbbyvfurNLHCjP7npm9YmYvmNk/mVlPfT9ONH7Bt9nO+nnjZUu4prcjUv+xmF/wLfhm2h/vPsW3Xjga6f1EREQWikBrBs65fmCz/9rMPgq8ATgNfAO41zn3qJltBB41s8uccxMzuikA/7dz7slyH/8N+G/AvTV/ihqlEzHedf1K2tKJqm2ujhikwLSsn0LwQCVXPhtoLk+8coK2dJwb1iyKPD4REZELVdSln/uALwDdQI9z7lEA59wuYBB4+8wHnHPH/SCl7GlgbcT3r6uYZ6zpbmFRS/gTjoPwZ2xmOwl5pmy+SGKWjbS+V46NsOfEWOSxiYiIXMhCBypmdhvQBXzLOXcKOGpmd5fvbQEuY44AxMxiwB8C/xL2/eeDc45MrlA1PTmTK/DXT+zhh7tORuo/Sh2VXKFIKsCMSiruKetHREQWrCgzKvcBDznn8uXX7wA+YGbPAx8GngTy1R62UorNpyktG32qSpuPmFmf/zM6OhphmMEVio6/fmIPP9h5vOr9TK5AIcShgtN5ntGSqn4KciWlGZW526cTMVWmFRGRBStUXquZtQJ3A1v8a865bcDbprXZAWyfpZu/AlYBdznnKn7DOuceBB70X/f29gafiojgTHpy5fv+ko1fCj+K+++4NHBb5xxNyTgts5zz40slPPrHFKiIiMjCFLYAxz3ANufcTv+CmS13zh0t//57wBjwg0oPm9lfAespBSnZaEOuv7nSk4vlCCZqHZWwzIz7bl8XqG0qHiObL1KsoSCdiIjIhSpsoHIf8LkZ1+43s/cCBuwA3ulc6RvfzB4AVjjnPm5mrwP+HbATeLo8i7HPOffOWj5APZgZZtXTk/29JTVMqPDykWFinnHZsrbonVSwbnELbek4RefwUKAiIiILS6hAxTl3W4VrnwA+UaX9Z6b9/hRcuN+knlnVGZWppZ8aZlR+trefZNwLFKiMZ/O8fGSY3q5mlnWkZ227abmKvYmIyMKlyrRl3iwzKp1NSd5zYy+X1xAUxGMWOOtneCLPj3efmrMyrYiIyEKnQ2LK7n3duqk04pmScY/eruaa+o95FviU41yh1C4RID15z8lRnt47wBsv62FFZ1NNYxQREbnQKFApa53lYL98ochkvkgqHv4UZF/cs6lNuXPx042DBCq5QpHjwxnGJqtmhIuIiFy0tPRTdmwow4mRTMV7h05P8Nkf7WXnsZHI/cc8L/BZP/6MSjJAZdpU+Yhl1VIREZGFSIFK2SPbjvD4zhMV7/l7S6KengywtD3Fis7ZN8b6/Aq5Qc76SZWDGVWnFRGRhUhLP2Vm1Qu+FV3tgcrrNwQ/KLolFWft4mZaUrE526YT5RmVgPtfRERELiYKVMrinlVdmpmqo/IaFXxbv6SV9UtaA7U9M6OiQEVERBYeLf2UxWMeuSpf9vVY+tl5bJhHXz4+tf+kXtKJGG+5YqnqqYiIyIKkQKUsGfPIVzl00A9UqqUvB3FkcIIXDw+RL8y9ofaFvkH+dfsx8gGCmphnXLWyY87CcCIiIhcjLf2UpRIe8cnKcduVK9rZuLQtUBZONf6yUSkYmn3vyaGBCXYdH+HNm5YG7t85N3W4ooiIyEKhQKXsHZtXVr0Xj3nE597XOqt4+aCgINVps4UCiZgFPmTw4acPUigW+e1b19YyRBERkQuOln4CGBzPcmhgfCptOAp/f0uQWiq5vAs1e+OZNtOKiMjCpECl7MjgBFsPDVbc7Pry0WH+8dk+RjK5yP3HY6VAJciMymShGKgqrS+V8BSoiIjIgqRApWzX8REe33mCTO7cwmn+HtuaCr61pdm8unOq7slscvmQgUo8RjZfDHzooYiIyMVCe1TK/MCgUlaOnw0UdM9IJau7m1ndHexgw6t7O0JlGPm1VLL5Ik3JGjfTiIiIXEAUqJT5gUGuQoqyX5m2lvTkMLasXRSq/ZnzfgoKVEREZEFRoFKWKM9K5CrNqBRqr0x7aGCcn+7p59ZLu1m1KNjMSlDXrurgsmVts54ALSIicjHSHpWyhOcv/Zw7o5KIeTQlY4EOCaxmMl/g8OAEo5P5WdtlcgW+8rMDPLN/IHDfbekEPW0p4jWMT0RE5EKkf4KXtabjrOxqqriJ9ZcuX8IvXb6kpv5jAeuoZAtFTo1MMh5i1iVXKDKSydOcjAXarFtNsegYGM+yuDUVuQ8REZF60j/By9YtbuHuG1exorNpXvr397fMGaiU04wTseDLTIdPT/C/frKfV0+MRh8gsLVvkK/89AC7j4/U1I+IiEi9KFAJ4OUjw+w4OlxTH0ELvvl1XFIhCr6lEv4JyuemVoex7+QYAM8fGqypHxERkXpRoFI2NJHj8Z0nONA/ds69Z/YP8IsQe0YqCT+jEq6OCsBkrraibxuXtgFwbCjDRLa2oEdERKQeFKiUTeYKbD00yPHhyXPu5UJWiq2kLZ3gTZuWsHaOWir+jEqYEvr+7Eut1Wmv7u3gbVcto1B07DxW2wySiIhIPWgzbVk8Vj3rJ1uHQKUpGeOa3s452y1pT/O2q5axvCP4XpkzgUrtsyDrl7Ry66XdXNLTWnNfIiIitVKgUuafxZOdEag458jl3VSdlfnWnk7QvjwR6pl4zCPuWcUaMEHtOzXG03v7uWNjD7dc0h25HxERkXpSoFKWrFJCv1B0FJ0jUWNV2kyuwJee2s/ly9v4pcuqpzo757AIheV+/42X1lRH5dhQhqNDmam9NJlcgVOjk/R21bc4nYiISBjao1IWn8rKOXtGpehg7eJmetpqqy3imZHJFaY2y1bzkz39/NVjuxkcz4bqv9Zib6dGJzGDRS1JAL7+3GEe2Xa04lKYiIjIa0WBSlnMM67p7ThnBiEZ93jndb3cGPL8nZnCZP0Uii504NF3epxdNdQ/OTU6yaKW5NT7XrasjUyuwJ6T52ZBiYiIvFYUqJSZGW/atJSrVnbMS/+eZ3hmc9ZR8ffIhC3X//N9A3z/5eORxpbNFxmayJ1VkXbT8jY8M7YfGYrUp4iISD0oUJnD4HiWf91+jIP94zX3FY8ZhQqnM0+XzRcxC1eZFkq1VLL5IsU5AqFK+scmcQ66y8s+AM3JOCs60xwdyoTuT0REpF4UqEzz3ZeO8r3tx866NpLJs/3IMKdD7hmpJObZOZt1Z/JrtoTdUFtLLZWu5iS/du0KNpQLvvk6mhJk80UyORV/ExGR80NZP9OcHJlk5oSEvxRTax0VgHdsXjFnP9l8MdIpzX4Z/Wy+SFMy3MGE6USM9UvOrZvS05ZiZVcT2UKxpsMORUREolKgMk085jE2mT/r2plKsbWlJwOBiri9/arl5OZYHqpkqox+vgCEq8MynMnRmozjzUjBvm51F9et7go9FhERkXoJ9E93M+s2s63TfnaZWd7MFpnZFjN7ysy2le/dOUs/N5fb7TKzH5jZyvp9lNpVKpqWy5de12NGZSJbYHRGIDRTR3PirE2tQbWl45FSqJ1zfO1nB/mn5/pCPysiIjLfAs2oOOf6gc3+azP7KPAG4DTwDeBe59yjZrYReNTMLnPOTUzvw8w84GvA7znnHi/38UngPfX5KLVLxr1z6obUc+nn68/3MZkr8oHb11Vtc2p0knQiRmsq3GTXpuXtbFreHnpMo5N5MrkC3a3Jc+5l80V+urefxa1JrlwxP9lQIiIis4n67Xsf8AWgG+hxzj0K4JzbBQwCb6/wzA1A3jn3ePn13wC/ZmbpiGOou7jnkS+6szJnlrSluG51J63p2lfJ4p7NWUfl4acP8tiOaGnGUZwaLW0SrjSLE/OM5w+eZq9qqYiIyHkSOlAxs9uALuBbzrlTwFEzu7t8bwtwGbC2wqOrgQP+C+fcCDAMrKjwHh8xsz7/Z3R0NOwwI7lqZTtv3rT0rGurFjXzxsuW0J4Ot++jklg5EKomXygVe4uymXZ0Ms9PXj0VOo361GjptOhqgUprKs5IZvblKhERkfkSZUblPuAh55z/7fUO4ANm9jzwYeBJoKZvNufcg865Xv+ntfW1Ocl3TXcLV/d2nLOptF5KMyrVN8r6+2OSEQ5AnMwVeHrfAH2nQwYqI6VApdLSD0B7U4LhTC70eEREROoh1HqGmbUCdwNb/GvOuW3A26a12QFsr/D4QWDNtHZtQAdwJNyQX1s/efUUh06P8+vX99Z8nk7MK1WmrXbwoH8OUJT9MKmEn/UTLmNoOJOjoykxlTU0U3s6zuHTE6W06dfoBGkRERFf2G+ee4Btzrmd/gUzWz7t998DxoAfVHj2WSBhZr9Ufv0h4BHn3AVT+vTZAwN8+olXp5ZDAPrHshwdyhCrwyxL3DOc45xaLb6p8vkRAoKoBd/uvnEVv3Xz6qr3/SWvEc2qiIjIeRB2h+h9wOdmXLvfzN4LGLADeKdzzgGY2QPACufcx51zRTN7H/A35Q20R4Dfrm349eUcTOaKZ1WPjVoptpI7Ny3hzk1LqBbz5It++fzwgUrcM2KeleuoBGdmsxZzW7WomYJzJDSbIiIi50GoQMU5d1uFa58APlGl/WdmvP4pcE2Y93wt+Us7uWkpyrlCtEqxlVRbXvEt72jiw2/agAt/XA9mRiruhZpRGcnkODQwwcquJjqaKm8WXrWomVWLmiveExERmW/6Z/I0/kGA0wOVbMGFPiCwmtNjWfadGjur/5nMLPJm3o3L2lgdIqg4NpThe9uPcXRoYu7GIiIi54EClWn8JZfpKcS5fLFuyx7bjwzzz88fPqdMv+/0WJZdx0cYz0ZLmvqly5ZwyyXdgdv7VXJnKy5XLDr+/pmD/GDna1fbRURExKezfqaJe+fOqNyxcTGl7Te18zfkVqulsr9/jCdeOcl7buylOTn//2vGJkv7WWYLVDzPGJrIRVqOEhERqZUClWmWdzTxnht7WdRypqbI+iVtdes/Xl5Cqlad1k9Pjron5tkDpznQP8Zdm1cGWj7yZ1TmCora06qlIiIi54eWfqZpSsbo7Wqet9mMuWZUain4BqWlowP941NpznMZm8yTSnhzvl97U4KxycI55yCJiIjMNwUq0xSLbuqQPijNcPzVY7vrtj/DnynJVsnMyRZK7xv1AMRUolxLJRcsoGhLx1nZ2RSoHaBS+iIi8prT0s80I5k8X3xqHzeu7eL1G3rIlc/esTrtUWlOxmhLx3FVNnxk87XNqPjpz6VaKnOfTfTLVy4L1K9f9G04k6OrpXKpfRERkfmgQGWa+Iz0ZP+/UWc4Zrqkp5VLeqqfW9SSitHdmpza1BtW1Oq0c1nb3cKvXbuCnrZzDy4UERGZTwpUpjkTqJRmNrJTgcr8HFI40+s39PD6DT2Rn59a+glQnXYkk+OZ/QNsWNI2Z0G3juYEHc21nx4tIiISlvaoTJPwynVUyoGKH7DUq45KvlDkuYOn2XNytC79zbSkLc0dGxfT3TL3zMfgeI5th4bOOtdoNsWiYyIbrjy/iIhIrRSoTON5Rtwz8sXy0k+N6cLn9G/GD185yY6jwxXvP3tggG2HBiP3v6glyQ1rFgXaRzKWnbvY23Sff3Ivj7xwQR90LSIiC5CWfmaIx7ypmZTlnWnee/NqWtP1+TN5XukAwEyVrJznDw7Skopz7arOurzfbPzquC0BA5WWVJzhCdVSERGR15YClRl+93Vrp+qdpOIxlrTPfpBgWOmEx0Su8hJKtlCkq4bZm/FsnoefPsjly9q5fcPiWduOlqvSBg1U2tMJTo5MUii6qb+PiIjIfNPSzwzpRGwqy2ciW2BoPFfXQmdNiRiZCns9nHNkazxXKBHzGMnkpyrOzmZqRiUZLBBrb0rgHIH6FhERqRcFKjMcG8pwsH8cgJeODPHFp/bRP5atW/9NyRgTucI5tVTyRYdzte2HiXtGzLNAWT+rFzWzeVUn8YDv5xd9q8fyz9GhCYbGtYwkIiJz09LPDD/adZLBiSz333Hp1GbaetVRAVjR2UTMM4oOpmc9T53zE4++rGJmpOJeoDoqV63sCNX39KJvtRjJ5PiHX/TR2ZzgfTevCXQmkYiINC4FKjPEYzavdVS2rF1U8bpnxqbl7Sxrn7uk/WyCBiphrelu5v47LqE54FJRNdsODVEoOvpHs+w8NsIVK9rrNEIREVmItPQzQyLmkSsUcc6dqaNSxxmVapqSMd521bKav7iT8RiTVTbr+jK5Al/92QGeO3g6cL+JmEdLKo5Z9KDNOcfuEyN0NidIJTx+tre/6knSIiIioBmVcyRihnNQKLq6l9AHODw4wbZDg9ywpoul7em69et742VzV7Ydm8xzcmRy6vDFoI4PZ8jkCqzpbok0NjPjfbesYXgix/HhSZqSMbTyIyIis9GMygzxcnXaXKEUqPgbVOtlIpvnlWMjnB4/e4PuK8dG+Jethzld48bdFZ1NrJjjROSxcmpy0GJvvsd2nOD7L9d2knQi5tHdmuKKFe2sW9xS0wyNiIgsfApUZvBPOC44x7+5dgUPvPHSuvbvn3A8sxz9iZEMe0+O1dx/oegYm8xTnGVJZTRksTdfR1OC0cl8pHTtV0+M8vgrJxjPnp3efGRwgt3HR0L3JyIijUGBygy3rV/MB19/Ca3l/Rj13p/SVN6MOrPo21jIAmzVPPXqKT77o72MzFLvJGz5fF9HuZbKcCZcLRXnHM/sH+ClvqGzrheKjm+/eJTHdp4IlFItIiKNR4HKLPadGqPv9Hhd+2xKlAKVmftDxibzJOMeyRoPQEzF5z5BuZYZFYChkLVUjgxlODaUYdPydpqTZ94z5hk3r+tmIltg9/H5OahRREQubgpUZjg5Mskv9g8wNJHjsR3HeXL3qbr2n54KVM5ePhnL5kPPcFSSKvc/WeU8IYArl7fzliuW0pwIl2ocNVDZfrg0k3L9mq5z7q3rKW3MPTI4EapPERFpDMr6meHYUIYf7z7F4tYU2UIxcOXWoGKecfMli1jcmjrr+uhkniVttWcBnZlRqR6oLGlPsyRCxlFHU4KOpgRht78eH87Q3pRgUYVTnVtTcTqaEgpURESkIgUqM8TLxd3yxSL5gqtrsTffbZeefWCgc47b1y+uuZgaBFv6yUcMwDqaE3zg9nWhnsnmi/SPZVm/pLVqmxWdTew4OszYZL7mPToiIrKw6FthBn/zbCZXpFB0NZ29E5SZcU1vZ136mlr6qTKj4pzj00/sYf2SVn7l6uV1ec/ZxDzj16/vnQoAK9mwtJXmZAyVfhMRkZkUqMzgz6D4WTnzUZX28VdOcLB/nPfftrbufS9tS3H/HZdM7YWZaSJXoFB0UzMvYe0+PsLhwQnesLEnUA2UmGesWtQ8a5tLe1q5tKf6jIuIiDQubaadwV8SmcwV6W5N0pqufyw3mSsyMJadqny76/gIn/nhHvafqr2OSrxc6r5akbqoGT++A/3jPH9w8Jz06mpOT/ucc5l5orSIiIhmVGZoTsRYu7iZpe0pbt+weO4HIpheSyUR8xjJ5JnIFkjUmJoMpS/7w4MTJONexc25UavS+jqaz2T+TE81ruYbzx8mETN++9a1s7b72d5+th4a5AOvW1dziraIiCwc+kaYoaslyTuv62XD0rZ5e490+Ys4U65OO1ae5WgN8MUfxD89e5if7umveG+sxhmVMCnKE9kCQxM5egJkM8U9YyJb4PhwJtK4RERkYVKgUsXgeJaf7e3nxEj9vzhnVqc9EzzUnvVjZnQ0xRkcrxxIjNb4XlOBSpX+p/ODjqXtqTlaMnU+0WGlKYuIyDSB/lltZt3AY9MuNQOXAEuA9cBfASkgDXzJOfdfq/TzfuCjQAFwwH9yzn078ujnQa5Q5PGdJ5jIFdh7coz2dKIu9U2ma5pR9G10Mk86EatbsJN69wAAF7hJREFUzZZFrSn2nRyjUHTn7FW5bnUn65e00lkOOMLyA5UgZfSPlQOVZR1z//2WtqeJe6Z6KiIicpZAgYpzrh/Y7L82s48Cb3DODZjZZ4GPO+e+aWaLgJ1m9i3n3MvT+yjf+x/ARufcMTO7Hfg6pWDnguGZsf3IMH5CSzJe/zoqyzub+LVrV0zNNIxN5mmtw2yKr7slyZ4To5wez55TWC4Vj5Fqjf5eqbjHlSvaWR4g+Dg+nMEzo6d17hmVmGcs60hzdChDsejwajyxemgix7GhDBuXtuqEZhGRi1jUTRH3AX9a/t0BfhGQFiALDFR4xgMMaAOOlZ/pi/j+88azUrBSLGegzEd6cmsqflYBtF+9dgWFWU47DsuvADswdm6gsv/UGD1tqch7VMyMX75yWaC2mVyBnrZU4JmilZ1N9J2e4NToZKTKuVDaTPzS4WF+tPsk2XyR0cnF3LBmUaS+RETk/Av9bWVmtwFdwLfKl34X+Bcz+3OgB/iQc+7YzOecc6fM7AHgOTMbAJqAN1d5j48AH/Ffd3R0hB1mZGZGPGZk8/MXqEDp5OBC0ZGMe+cEE7Xqbk3S3pQ4J/gZz+b5xvOHuXJFe+Bgoxb3bFlNPmBqMsBVvR1sWt5OZ3O0ZSmA5w8N8sNXTtKWjtOcjPHk7n5WdjYHWn4SEZELT5Rv4fuAh5xz/iaFPwH+1Dm3GrgS+Aszu2LmQ2bWAXwYuMk5t6bczzfM7JwDYJxzDzrnev2f1tbXthjY9LL58xGoFIqOv3psN4/tOE6uUGRwPBvqC30uS9rS3Hf7OjYtbz/r+smRSQB62moLjF4+MszXnj4QaENtmH037ekEXS3JmpZqrlzRzg1runjfLWv4P65Zjmew56ROZhYRuViF+hY2s1bgbuCL5deLgXc65x4GcM7tBX4GvK7C428BBp1zO8ptHwHagTWRRz9P4l7pz7J+SetUhk49xTwjGfeYyBU4NpThS0/t58XyCcPzqV6BSq5Q5MTwJIMT2aptdh8fYduhwcDF3nwjmRy7jo+ELv42ksnhnCMVj3HHxh7SiRhL2tK875Y1vG79/NTDERGR+Rd2uuAeYJtzbmf59WlgzMzuhKnA5WbgpQrP7gU2m9mycttbKS09HYoy8Pl0dW8Hd2zs4deuXRG5MNpc0okYE7nCVLpwvd9n78lRfrDz+FnLPyfqFKgEqaXyQt8QP959kljI2ZHnDg7y/71wlFOj1YOgmXKFIl97+iDfeemcFUe6yvt1CkXHsSHVaBERudiEDVTuA77gv3DOFSjNsPw3M9sG/Aj4pHPupwBm9oCZ/Vm57XPAXwA/KLf9n8DdzrkL7ttjy9pF3LCma17foykRYyJbqLkAWzVHBjNsOzTE4PiZL/yTI5N0NidIxWubJZorUHHOcXwkw5K2dOjsnUsWtwCwL8RxAjuPjjCRLcyaifSdl47yz1sPh57hERGR8yvUt6Nz7rYK1x4FbqjS/jMzXn8K+FSY9zxffr5vgIGxSd521fycMNyU9Dg9Xqz57J1qpmf+dLemKJY37nbVsFHV196UwKx6oDI4nmMyV2RJgEJv/3979xrb1nkecPz/nENKJHUXZd2vtuzYcezYjePY6eKlTbomzXrvUgTr0hUYin0o0G4Yhi4rumIXDPvSDdtQYEObpuhtyLouSS/r1q1pkzqNE8d2bDeJL5ElWZYt2bpLFMXbuw/nkD6SKNGWKJGJnh9sgOS58LyPDg8fvue9LNRcHaTEZ3Hh2jT7u3L31jHGcPziGCU+i1ubK5dcr7UmxLmhaX49OMmetvzMVK2UUmrt6ci0WTx/7iqHz1/jzaurnyRwKUG/TSyRynzZl+W5LUxduZOojMw4NSqWJTyyvz0viZdtCeWlviUTld4RJ24t7mizN7vvjnCIyxNRIrHcg8r1jUQYmY6xq6Vq2Zqinc2VBEtsjvWNkcpjV3CllFJrSxOVLKbcUVd9qxx0bDn3bN3Epw9tJpE0BEvyNyptmtN7BkZuoq3Hzbj3lk3c070p67I3r87gs4SOcNmK9t1VV4Yx0HstknPd4xfHEIHbc9SS+G2L21urmZiNc25YewEppdRbhc6enMVaJihp6Vs9H3lHC/Fk/n/h+22LyoCf0RmnAe3JgXFm5pLs76pdNKz+SnTXLz1p49b6cpqrAyueBbmrrowW9xbQclIpQ9DvY3tjRabdzHL2tFXzSt8oR/tGVzRirTGGIxdG3V5JIAIP3ta06sbJSimllqaJShbpsVPWcuT1aDzJyEyMmpCfUJ5mTV5oV2sV6V6+vx6cZHI2zoHN+RulNRp3GgOHFwxYl6t2I5dQiY+H72zLuZ5lCQ/c1njDXZmDJTZ722tIGWewPZ99439gYww/P3uVE/3jlJf6CPjnJ1HxZApbZNVD/yullJpPE5Us0l9gwtp96fSPRvjRycts3lTGO7vr8j46LTi9l8Cpebg2NUdrbTBv894kU4ZvvNBLRcDPI/vbMvuNxpME/Plrb2OMyXrM0XgSnyX4bOumyrTSMVVOX5rkRP84rTVBPrinZV5tTzJl+MGrg5T6bN67syHvt/GUUmoj0ytqFukalW2NS9/eWK30DMo9V2c4e2Vqzd4HYDQSI5EybCrP3zDytiV015czNBll0B2fZC6R5KvP9/DsG8Or3n80nuTJoxf55flrWZc/d/YqT7zQm+nefbPmEkmmorlH1k3b0VTBXZtr+dDelkW3pIxxelSdHZriP49fYi6RXNExKaWUWkwTlSw2byrjgdsauesGuseuVKnn1kG+uyanTczG+faRPn7iDoSW77YUe9udsWaO948BTg+ceNJkBllbjVKfxeRsnDezNHwdnYnx2uVJqoJ+QivoLTUbS/L4L3t5+sRgzqkLLo3PAs5UAHdvqcs6pYLPtnjfbU3saatmYGyWZ04Mrni8lmg8yelLEwxPXR9eKN9TLCil1FuJ3vrJor4iQH3F2k5iF/TcHlmrRCXotxmenMs8r89zolJbVkJXXRnnh6eZiFxPKrZsWllvHy8RoauujJMDE4zNxOYlPy/2jGCMcxtnJbeygiU2t7dVcaRnlMNvjvCb27L3Xnr14jg/e2OYd22vzzn2imUJ997i7OfExXF+eHKQ9+9uvqHbQMYYzg9Pc3pwgv6RWVLGsKetmvrtzjn45NGLJFKGfR217G2vXrOJMpVSqhjpFS+LSCzB1w9f4OXe0TV7D2+islbD9Jf4LCrd3jC3t1Wtalbipextr8YYONY/Rs+1GRqrAlQE8vM+Xe4ota8OjGfGPhmeinLmyhRddWU0r2CclrQDXWGaqwMc6xujN8souD1Xp3n2zDA1IT+3NNzYLUARJ1nZ2VzJeCRONJG7FmQiEuepE5f44cnL9I/M0hEO8Vs7Gzi4JQw47Ytua6ki4LM5fP4a33ihl9OXJlY0FkwimeLKRJS+kRnODk1lLbdSShUbrVHJ4vXLk4xH4pwamMg0SM037y/tstL8T3yYFi4rYWYuwb3b6vPWkNarvTZEd305s/EksUSK7vr8zXTdVhuiKujneP84I9MxPnpHK796cwSAu90v8pWyLOGBnU1860gf//PaFT5xoCPT+2poMsqPT10m4Lf50N6Wm5qYUkS4f0cDc4kUwRIbYwwjM7ElG0tPzcXpG4lwa3Ml92ytW9QDzLKEu7fUcVdXmJMD4xy5MMpPXxvitcuTPLwvd88ogCsTUY72jdI3EiHmSZ5aqoN0usngSxdGGRyfpbUmSHs4xKby0ps+X5Ipw3Q0wXQsgTEGv225/50BAtfi/FNKvf1popLFejUHOLglzNmhKcrWqHsyOLdnLlybYWI2npe2IwuJCO+/vZlX+kY5NzRN96b8JSp+2+J3D7RzrG+c6pAfYwwNlQFCJT7qK1d/a64q5Oe+HfX816krvHpxgoNbwhzrH+MXZ67is4QP3N5MdejmY2ZZkkluekciPHX8Eu21Ifa0VxOZS3JhZIYtm8rY2VxFa02IRw92ZqY8WIptCXvba9jRVMmx/rF5NXJnh6aIJVIE/BalPpu5RJKBsVl+o7sOn20RS6Q4PzxNU1WA9toyykt9lPgsygPXz7upaJz+0Ygzx9I5CJXYtNaE2N1aRVttCGMMY5E4cwknIY3GU4xHYoxFYrxrez2lPpvB8Vm+98rAomP3WcJn3t0NwMXRCGeuTFER8FER8FMZdI6l1GdnxsIZm4kxGY0TjaeIxpNE40lm40naakNscc+vn58ZZmI2TqnPosTnJEQBv01XXVkmKRyajGKJ4LedbuMmBUljqAz48NkWc4kkl8ZmSaSc7urJlMEYSBnD7tYqRITJaJwrE1F8luC3LXy2YIsz+3n63IjGk8y5CaBxu76njFOjmS7Ttek5ZmOLG1lXhfxUujWQ45EYKYMzkadcHx4h6Lfx2xbGGCZnExic/RtjMIAxUBPy47MtEskU47PxTH9FEcESpwdjZdBJFuNJJ65eBrBEMrW76bgv5LetzK3qSCxBwq3Z876f35bMKNGRWCIzMaq3DtBbpqkFDeLT+wqV+LAtIZkyzGY5FrheGx1PpjJ/Ay9LyCT/c4nkvDGr0u9jW5LpqRiNJzNlml/u+WXKVqEZ8Fn43DJFsvytRSDgs7EsIbWgTN4cPn28iSXLdP36EkukSKSyr+MtUyrLEA4+y8p0DFiqd2Wx0EQli/TJX7fGA3kd2BzmwObV1Qzkkh68rndkZk0SlbQ7OmrZ2VyV167JAKU+O3MbBMh7vLY3VlLqs+kMhwDYVF7K9sYKbmupWtWtpbSKgDMg3dmhafpHnZF2LRFqPQlQriTFK+C3uXvL9S7WkViC/z59JevFdWtDBS3VQVpqgnz60OZlx+u5b0cDh7ZtYnB8lv7RCH0jEc4OTWVqyIyBb7zQm3Xbd3TUUF9hU1NWwh0dNZSV+rAE4klDIpnCsiRzEbwyGeXUpYlF++isC/Hhva0AHLkwyuuXJxetIyKZROXyRJThyblFF+CKgC+TqPzbSxezXqB/72AHdeWlTEcTPH1iMGuZdrdWOcc7EeVHJy8vWl5bVsIn7+4E4Hj/OC/2jCxaZ1tDBQ/tdqaseLFnhHNDixuGH9gczpzfz7w6mHUk6Yd2N7HNvf34+OELWY/30YMdhMtLGZ+N881f9WVd53P3bwWcnoY/PrW4TOHyEh496JTpWP8YR3oW3/r2lulnbwznLNP3XhnIWaavPb98mcYisSXL9Efv2QY4k5hm+zt5y/RKX+4y/e/rQ3kp078+17NsmUZvoEw9N1Cmo32jeSnTXCKV92t3PmmiksWOpgoMJq+3MQplf1ctwRKbXS1Va/5exXyiLyfdFgac201ttaG87buuvJQHdzXxzq1xzlyZojLgpyMcylusgn6b39nXRiSWYC6RcseXsWipCWYmoLQtuaFBBf22RUe4jI5wGfdsdX6tpX9kWZZwcEsYn+X8snRqFPzzZuMuL/VxaImGyWn7OmrY2VzJVDTBVDTOZDRBPJGiytN+6tamSpqrAwT8NqU+i6DfJlBiE/LE7JH97Znai1gyRSzh/PpM104YY7i7O0wi6daWGIMl839plgd8vOfWBvy2hW05NQ+2JQjXE6umqgAP7W4inkyRSBoSqRTJlNMrLa25OpCZbd1yazAsSwh7EtBdLVW01cw/r1LG0OiZ8XtXSxWRmPPrN513GmMybctEhH2dNQjOe+DWlIiQ+YUdKrE5sDmMweD+wxgwXP/FXBPyO23LmF8T4p1vrKU6mPW2d7j8epm66sqoCPgzAy6mU0JvmbY3VjITS8x7H2Bee7k7Omoy23oHb0z/nQJ+mz3tyzdmrw76562Tfj9vR4WmquDi/Zj5vSE7w2VZOzd4y3RLQwWR2sU1JtWe0bEXNr43GASh1FumHA30q4P+rPvx1sA3VgYy6xhPnZW3M0hnuCxr78gGz6Sx+RitfC3JjY7qWUitra1mYGBxlbJSSimlipeIXDLGtK5mH9rrRymllFJFSxMVpZRSShUtTVSUUkopVbQ0UVFKKaVU0dJERSmllFJFSxMVpZRSShUtTVSUUkopVbQ0UVFKKaVU0dJERSmllFJFSxMVpZRSShUtTVSUUkopVbQ0UVFKKaVU0dJERSmllFJFSxMVpZRSShUtMcYU+hhyEpE54Gqhj2OdlAPThT6IIqbxyU1jlJvGKDeN0fI0PrmVA0FjTOlqduLL08GsqdUW8q1ERAaMMa2FPo5ipfHJTWOUm8YoN43R8jQ+ubkxql7tfvTWj1JKKaWKliYqSimllCpamqgUny8X+gCKnMYnN41Rbhqj3DRGy9P45JaXGL0lGtMqpZRSamPSGhWllFJKFS1NVJRSSilVtDRRWSci8o8i0isiRkT2eF4vFZF/FpFzInJKRL7lWbZVRF4QkbMi8rKI7CzM0a+PZWL0PhE5JiInROS0iHzSs6xeRH7ixu+0iBwqzNGvPREJiMhT7vnwqoj8VES63WVLxkFjlInR1z2vHxaROz3bhUTkuyJy3l3nY4UrxdpaLkaedd4tIkkR+ZznNY2Rs0xE5EvuslMi8qxnuw3xWcsRn/0i8qKIHBeR10XkTz3brewcMsbo/3X4DxwCWoFeYI/n9b8H/onr7YUaPct+Bvy++/hjwMuFLsd6xwgQYBTY7T7vBKJAhfv8ceBL7uM7gQHAX+iyrFF8AsD7POfKZ4Cf54qDxigTow8APvfxbwO9nu2+CDzhPu4ChoFwocuz3jFyn1cBLwE/AD6nMVp0Hn0W+D5Q4j73XrM3xGctR3xOAB9wH9e658mtqzmHCl7gjfZ/wZdwGTAJVGZZr95dlr6wCnAF6C50GdY5RgKMAIfc57uBS56LxPSCC8VLwP2FLsM6xWlf+st2uThojK4nJJ7X64C45/P1a+CAZ/mTwB8U+vgLESPgmzhJ3RMLEhWNkfN4ANi2xHob8rO2ID7HgUfdx21uvBpXcw7prZ/C2oJTW/CYiBwVkedF5D53WRtw2RiTADDOX7UfaC/MoRaGW+6PA98XkT7gl8AnjTExEQnj/Fq54tmkl40To88CTy8XB42RE6MlXv9x+vOFE48+z/JeNmCM3Kr4lDHmmSzrbfgYiUgl0AB8UESOuP8/DrDBP2vez9mngL8SkX7gLPCYJyYrOofeEkPov435gA7gNWPM50VkL/DTt3tblJshIj7gC8BHjDHPue0KnhGRXcCG7VsvIo8B3cB9QLDAh1OUFsTI+/ongIdxbjVuaN4YiUgjzmft3oIeVJFZcB4FcK7bQWPMXSLSCbwgIm/g1BxsOFk+Z58H/swY8x0R2Qz8QkSOGmNeW+l7aI1KYfUDKeDbAMaY48AFYBdwEWhyv6gREcHJPPsLc6gFswdoNsY8B2CMeRnngrDXGDMCJNwLbFonb/MYicifAB8BHjTGRJaLg8bIiZHn9Y8DfwG8xxgz5NmkH+dHQ1onGy9GdwBNwAkR6cVpF/dFEfkbd5MNHyNjzCjO7Z1vARhjeoHDwJ0b8bO2MD4iUgd82BjzHQBjTA/wIvBOd5MVnUOaqBSQMeYa8H/AewFEpAungdHrxphh4BjwCXf1jwIDxpjzhTjWAkonbDsA3JblW4Az7vJ/B/7QXXYn0AL8ogDHuS5E5I+BR3C+aMc9i5aLg8bIef1h4K9x2gwsvDh6Y9SFU6vw1LoccAFki5Ex5kfGmAZjTKcxphP4HvCXxpg/dzfb8DFyfRd4wF2nFtgPnHSXbZjP2hLxGQNmROTd7jp1wF3AaXf5is4hHZl2nYjIvwAPAY04jUOnjDHdbtXY13Aa96VwLgz/4W5zC06DtjBOw9pPGWNOFeDw18UyMXoEeAwnPhbwt+mMXUQacBr/dQEx4DPGmGez7f+tTkRacRK3HmDKfXnOrYJeMg4ao0yM4jgN0kc8m9xnjBkRkTKcHhv7gCTwBWPMk+t46OtmuRgtWO8J4IQx5h/c5xoj5zwKA18HNruvf8UY8xV3uw3xWcsRn/uBv8O5ReYHvmqM+bK73YrOIU1UlFJKKVW09NaPUkoppYqWJipKKaWUKlqaqCillFKqaGmiopRSSqmipYmKUkoppYqWJipKKaWUKlqaqCillFKqaGmiopRSSqmipYmKUkoppYrW/wO4D/flPUiG6AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_figure(graph_val[0][150:], loss_val[150:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GZ9S2gz-JM1"
      },
      "source": [
        "Dear professor, here's my current progress:\n",
        "1. I implement epsilon_Loss_penalty function. Now, the training process will iterate until it counts 5 consecutive times the loss difference between the current loss values with its previous loss < 1e-5. \n",
        "\n",
        "    Then, it finds the normalization constant c and apply to model.output weight layer and its bias. Then the algorithm updates the normalized output weight layer for the model. \n",
        "\n",
        "    The result: the training process only takes < 300 epochs to converge to a loss value of 78.7160. This occurs consistently after several re-runs. The problem: the constant c is 1 from my normalization function. I'm looking for a normalization from the PyTorch library to use instead. Please let me know if I can use this function for the normalization part: [PyTorch normalization function](https://pytorch.org/docs/stable/generated/torch.nn.functional.normalize.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "yOAl63W0qaq_",
        "outputId": "0966c5cc-146b-4f15-d330-522a5eafc1e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_figure(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el6VL83Qddfj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPn9pDNlm88W"
      },
      "source": [
        "# HOW TO TRAIN THE NN MODEL:\n",
        "1. Reset adam_optimizer: \n",
        "```adam_opt.zero_grad()```\n",
        "2. Calculate loss\n",
        "3. Update the optimizer: \n",
        "```adam_opt.step()```\n",
        "\n",
        "\n",
        "        weight = weight - lr * gradient\n",
        "\n",
        "-> use lr and gradient to \"improve\" weight layer. \n",
        "\n",
        "Explanation:\n",
        "```adam_opt.step()```: Update the model's parameters \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VfCCteSFrC8"
      },
      "outputs": [],
      "source": [
        "model_p = Nonlinear_2(4)\n",
        "adam_opt = torch.optim.Adam(model_p.parameters(), \n",
        "                                    lr=learningRate, \n",
        "                                    betas=(0.9, 0.999), \n",
        "                                    eps=1e-08, \n",
        "                                    weight_decay=0, \n",
        "                                    amsgrad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUjHqCmkNRzB",
        "outputId": "a8eba8cf-5f24-45cf-f588-85f437d798fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.5799], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x_i = torch.tensor([-2.5], requires_grad=True, dtype=torch.float)\n",
        "u_xi = model_p(x_i)\n",
        "print(u_xi)\n",
        "u_xi.backward()\n",
        "u_prime = x_i.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A2qd2M1OXQ5",
        "outputId": "224f75ed-733d-4edc-f3ad-3d78dac7f66a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Hidden layers: \n",
            "Parameter containing:\n",
            "tensor([[-0.4363],\n",
            "        [-0.2556],\n",
            "        [ 0.6323],\n",
            "        [-0.8666]], requires_grad=True)\n",
            "\n",
            "\n",
            "- Hidden layers gradients (derivative of Loss w.r.t model params): \n",
            "None\n",
            "Parameter containing:\n",
            "tensor([ 0.9463, -0.8034, -0.8181,  0.5812], requires_grad=True)\n",
            "\n",
            "\n",
            "Output layers: \n",
            "Parameter containing:\n",
            "tensor([[-0.3294, -0.3360,  0.0453, -0.0283]], requires_grad=True)\n",
            "None\n",
            "Parameter containing:\n",
            "tensor([0.4850], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Access the model's parameters:\n",
        "# Model's hidden layer weight and bias\n",
        "print(\"- Hidden layers: \")\n",
        "print(model_p.hidden.weight)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"- Hidden layers gradients (derivative of Loss w.r.t model params): \")\n",
        "print(model_p.hidden.weight.grad)\n",
        "print(model_p.hidden.bias)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Model's output layer weight and bias\n",
        "print(\"Output layers: \")\n",
        "print(model_p.output.weight)\n",
        "print(model_p.hidden.weight.grad)\n",
        "print(model_p.output.bias)\n",
        "\n",
        "# model.zero_grad()\n",
        "\n",
        "i, o = (x_train[0], y_train[0])\n",
        "i = Variable(torch.from_numpy(i))\n",
        "o = Variable(torch.from_numpy(o))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA_91qCHWhjs",
        "outputId": "20a262ba-f9a2-4cf1-b0c4-110596eb2f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Hidden layers: \n",
            "Parameter containing:\n",
            "tensor([[-0.4363],\n",
            "        [-0.2556],\n",
            "        [ 0.6323],\n",
            "        [-0.8666]], requires_grad=True)\n",
            "\n",
            "\n",
            "- Hidden layers gradients (derivative of Loss w.r.t model params): \n",
            "None\n",
            "Parameter containing:\n",
            "tensor([ 0.9463, -0.8034, -0.8181,  0.5812], requires_grad=True)\n",
            "\n",
            "\n",
            "Output layers: \n",
            "Parameter containing:\n",
            "tensor([[-0.3294, -0.3360,  0.0453, -0.0283]], requires_grad=True)\n",
            "None\n",
            "Parameter containing:\n",
            "tensor([0.4850], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(\"- Hidden layers: \")\n",
        "print(model_p.hidden.weight)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"- Hidden layers gradients (derivative of Loss w.r.t model params): \")\n",
        "print(model_p.hidden.weight.grad)\n",
        "print(model_p.hidden.bias)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Model's output layer weight and bias\n",
        "print(\"Output layers: \")\n",
        "print(model_p.output.weight)\n",
        "print(model_p.hidden.weight.grad)\n",
        "print(model_p.output.bias)\n",
        "\n",
        "# model.zero_grad()\n",
        "\n",
        "i, o = (x_train[0], y_train[0])\n",
        "i = Variable(torch.from_numpy(i))\n",
        "o = Variable(torch.from_numpy(o))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jQeEbaMHkeA",
        "outputId": "32d5985d-7d0f-4048-c555-7ef3785149ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 0., 0.])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkmedTJ6J1Hr",
        "outputId": "01a83e6e-280c-4a72-d9c4-bff991258866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)\n",
        "\n",
        "t = 3*a**3 - b**2\n",
        "\n",
        "external_grad = torch.tensor([1., 1.])\n",
        "t.backward(gradient=external_grad)\n",
        "\n",
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyuNXuz-iLBZ"
      },
      "source": [
        "https://neptune.ai/blog/pytorch-loss-functions\n",
        "https://stackoverflow.com/questions/53980031/pytorch-custom-loss-function\n",
        "https://stackoverflow.com/questions/65947284/loss-with-custom-backward-function-in-pytorch-exploding-loss-in-simple-mse-exa\n",
        "https://www.youtube.com/watch?v=ma2KXWblllc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0PS0efCiJCp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PyTorch_.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "8c21580189d9a9d7f1e3fef63ff70de58083dda94aa35d8ed937f03fa405217e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}