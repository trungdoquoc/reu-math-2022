{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpofWgGancyb"
   },
   "source": [
    "The input of the NN is 1D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e-NAhHF3ncye"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.functional import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "INPUT_SIZE = 1\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "LAMBDA_PEN = 10000\n",
    "LOWER_BOUND = -6\n",
    "UPPER_BOUND = 6\n",
    "N_POINTS = 1001\n",
    "\n",
    "\n",
    "def quad_fn(a, b, c, x):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "def x_square(x: torch.tensor) -> torch.Tensor:\n",
    "    return x**2\n",
    "\n",
    "# DEFINE GIVEN FUNCTION V(x): ax^2 + bx + c\n",
    "given_fn = x_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EstQ_ILqQeOt",
    "outputId": "08a3abc2-5594-4418-a0d6-4957a74ab261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 1) (1001, 1)\n"
     ]
    }
   ],
   "source": [
    "# CREATING DATASET:\n",
    "x_values = [i for i in np.linspace(LOWER_BOUND, UPPER_BOUND, N_POINTS)]\n",
    "y_values = [given_fn(i) for i in x_values]\n",
    "\n",
    "x_train = np.array(x_values, dtype=np.float32).reshape(-1, 1)\n",
    "y_train = np.array(y_values, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SOtFslmCncyg"
   },
   "outputs": [],
   "source": [
    "# CUSTOM LOSS FUNCTION:\n",
    "# def EpsilonLoss(nn.Module):\n",
    "\n",
    "def epsilon_Loss(v_x, model_u, lower_bound, upper_bound, n_points):\n",
    "    \"\"\"\n",
    "    GOAL: Epsilon function evaluated at u using discretized estimation\n",
    "    minimizing Epsilon(u) = \n",
    "    \n",
    "    ARGS: \n",
    "    n_points (int): number of discretized points on the interval [-L, L]\n",
    "    e.g.: -(L)|---|---|---|---|(L) interval has n_points = 5\n",
    "\n",
    "    v_x (torch.Tensor): function instance\n",
    "    model_u (torch.Tensor): model output\n",
    "    \"\"\"\n",
    "    sum = 0\n",
    "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
    "    h = discrete_points[1] - discrete_points[0]\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "        u_xi = model_u(x_i)\n",
    "\n",
    "        u_prime = model_u.u_prime_2(x_i)\n",
    "        \n",
    "        v_xi = v_x(x_i)\n",
    "        t = torch.square(u_prime) + v_xi*(u_xi**2)\n",
    "        sum += t\n",
    "    return 0.5*h*sum\n",
    "\n",
    "def epsilon_Loss_penalty(v_x, model_u, lambda_pen,\n",
    "                         lower_bound, upper_bound, n_points):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    sum = 0\n",
    "    pen = 0\n",
    "\n",
    "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
    "    h = discrete_points[1] - discrete_points[0]\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "        u_xi = model_u(x_i)\n",
    "\n",
    "        u_prime = model_u.u_prime_2(x_i)\n",
    "        \n",
    "        v_xi = v_x(x_i)\n",
    "        t = torch.square(u_prime) + v_xi*(u_xi**2)\n",
    "        sum += t\n",
    "    epsilon_fn = 0.5*h*sum\n",
    "    \n",
    "    temp = 0\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "        temp += torch.square(model_u(x_i))\n",
    "    \n",
    "    pen = lambda_pen * torch.square((temp*h-1))\n",
    "    return epsilon_fn + pen \n",
    "\n",
    "# NORMALIZE MODEL u(x) OUTPUT:\n",
    "def normalize_u(model_u, lower_bound, upper_bound, n_points):\n",
    "    \"\"\"\n",
    "    Normalize model.output weight by: \n",
    "    model.output *= c\n",
    "    where,\n",
    "    scalar c = 1/denom\n",
    "    \"\"\"\n",
    "    discrete_points = np.linspace(lower_bound, upper_bound, n_points)\n",
    "    h = discrete_points[1] - discrete_points[0]\n",
    "    s = 0\n",
    "    for i in discrete_points:\n",
    "        x_i = torch.tensor([i], requires_grad=True, dtype=torch.float)\n",
    "        s += model_u(x_i)**2\n",
    "    denom = math.sqrt(h) * torch.sqrt(s)\n",
    "    return 1/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXgpE3Jpbr2S",
    "outputId": "52dead7e-48c6-4e7d-c790-11102347771f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bybUlm0CWPrU"
   },
   "outputs": [],
   "source": [
    "v = torch.tensor([10], requires_grad=True, dtype=torch.float)\n",
    "r = model.u_prime_2(v)\n",
    "r.item()\n",
    "\n",
    "# def u_prime_(input, model):\n",
    "#     i_clone = input.clone()\n",
    "#     clone = copy.deepcopy(self)\n",
    "#     res = clone(i_clone).backward()\n",
    "#     return i_clone.clone()\n",
    "\n",
    "def u_prime_2(input, model):\n",
    "    print(input)\n",
    "    # print(input.item())\n",
    "    # i_tensor = torch.tensor([input.item()], requires_grad=True, dtype=torch.float)\n",
    "    clone_model = copy.deepcopy(model)\n",
    "    res = clone_model(i_tensor)\n",
    "    res.backward()\n",
    "    del clone_model\n",
    "    \n",
    "    return i_tensor.grad\n",
    "\n",
    "u_prime_2(v, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "06QWhlkpQ5Eh"
   },
   "outputs": [],
   "source": [
    "# CREATING MODEL CLASS\n",
    "class Nonlinear(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        # One hidden layer with n nodes\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1, n)\n",
    "        self.output = nn.Linear(n, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.target_fn = given_fn\n",
    "\n",
    "    def forward(self, x, use_tanh_fn = False, activation_on_output = False):\n",
    "        if use_tanh_fn == True:\n",
    "            x = self.hidden(x)\n",
    "            x = self.tanh(x)\n",
    "            x = self.output(x)\n",
    "            \n",
    "        else:\n",
    "            x = self.hidden(x)\n",
    "            x = self.sigmoid(x)\n",
    "            x = self.output(x)\n",
    "            \n",
    "        if activation_on_output == False:\n",
    "            return x\n",
    "        else:\n",
    "            if use_tanh_fn == True:\n",
    "                return self.tanh(x)\n",
    "            else:       \n",
    "                return self.sigmoid(x)\n",
    "        # output is a linear combination of the hidden layers because \n",
    "        # we perform regression ???\n",
    "        return x\n",
    "\n",
    "    def u_prime_2(self, input):\n",
    "        i_tensor = torch.tensor([input.item()], requires_grad=True, dtype=torch.float)\n",
    "        clone_model = copy.deepcopy(self)\n",
    "        res = clone_model(i_tensor)\n",
    "        res.backward()\n",
    "        del clone_model\n",
    "        \n",
    "        return i_tensor.grad\n",
    "\n",
    "    def u_prime(self, input):\n",
    "        \"\"\"\n",
    "        NN with 1 hidden node layer is of the form:\n",
    "        u(x) = SUM_i_to_N(a_i * sigmoid(w.x + b))\n",
    "\n",
    "        where\n",
    "        a_i is the corresponding weight of self.output layerq\n",
    "        w is self.hidden.weight vector\n",
    "        b is self.hidden.bias vector\n",
    "        sigmoid(w.x + b) is the sigmoid-activated hidden vector\n",
    "\n",
    "        Formula of u'(x) (for 1 hidden layer NN):\n",
    "        u'(x) = SUM_i_to_N(w_i*a_i*sigmoid'(w_i*x+b))\n",
    "        Note: sigmoid'(w_i*x +b) = sigmoid(w_i*x+b)*(1-sigmoid(w_i*x+b))\n",
    "        \"\"\"\n",
    "        a_i = self.output.weight.data\n",
    "        w_i = torch.transpose(self.hidden.weight.data, 0, 1)\n",
    "        wi_ai = w_i * a_i\n",
    "\n",
    "        hid_layer = self.hidden(input)\n",
    "        hid_layer_T = torch.reshape(hid_layer, (list(hid_layer.shape)[0], 1))\n",
    "        m = hid_layer_T * (1-hid_layer_T)\n",
    "\n",
    "        return wi_ai @ m\n",
    "    \n",
    "# TRANING MODEL\n",
    "    def train_network_with_penalty(self, num_epochs, v_x, optimizer, lambda_pen,\n",
    "                                    lower_bound, upper_bound, n_points):\n",
    "        # For plotting loss value over epochs:\n",
    "        x_epochs = []\n",
    "        y_loss = []\n",
    "        \n",
    "        # stopping criterion:\n",
    "        stop_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            loss = epsilon_Loss_penalty(v_x, self, lambda_pen,\n",
    "                                        lower_bound, upper_bound, n_points)\n",
    "            y_loss.append(loss)\n",
    "            x_epochs.append(epoch)\n",
    "            #check if need to stop training:\n",
    "            if epoch > 0 and stop_counter >= 5:\n",
    "                c = normalize_u(self, lower_bound, upper_bound, n_points)\n",
    "                self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
    "                self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
    "                print(\"c value = \" + str(c))\n",
    "                print(\"LOSS VALUE = \" \n",
    "                      + str(epsilon_Loss(v_x, self, lower_bound, upper_bound, n_points)))\n",
    "                break\n",
    "            elif epoch > 0 and stop_counter < 5:\n",
    "                if torch.abs(y_loss[epoch-1]-loss) <= 1e-5:\n",
    "                    stop_counter += 1\n",
    "                else:\n",
    "                    stop_counter = 0\n",
    "\n",
    "            print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "            loss.backward()\n",
    "            # Calculate the derivative(loss) w.r.t the model's parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        return (x_epochs, y_loss)\n",
    "\n",
    "    def train_network(self, num_epochs, v_x, optimizer,\n",
    "                                    lower_bound, upper_bound, n_points):\n",
    "        # For plotting loss value over epochs:\n",
    "        x_epochs = []\n",
    "        y_loss = []\n",
    "        \n",
    "        # stopping criterion:\n",
    "        stop_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            if epoch > 0 and epoch % 50 == 0:\n",
    "                c = normalize_u(self, lower_bound, upper_bound, n_points)\n",
    "                print(\"Pre normalize: \")\n",
    "                print(self.output.weight.data)\n",
    "                print(self.output.bias.data)\n",
    "                print(\"After normalize: \")\n",
    "                self.output.weight.data.copy_(c.item() * self.output.weight.data)\n",
    "                self.output.bias.data.copy_(c.item() * self.output.bias.data)\n",
    "                print(self.output.weight.data)\n",
    "                print(self.output.bias.data)\n",
    "                print(\"c value = \" + str(c))\n",
    "            optimizer.zero_grad()\n",
    "            loss = epsilon_Loss(v_x, self,\n",
    "                                lower_bound, upper_bound, n_points)\n",
    "            y_loss.append(loss)\n",
    "            x_epochs.append(epoch)\n",
    "            #check if need to stop training:\n",
    "            if epoch > 0 and stop_counter >= 5:\n",
    "                print(\"LOSS VALUE = \" \n",
    "                      + str(epsilon_Loss(v_x, self, lower_bound, upper_bound, n_points)))\n",
    "                break\n",
    "            elif epoch > 0 and stop_counter < 5:\n",
    "                if torch.abs(y_loss[epoch-1]-loss) <= 1e-5:\n",
    "                    stop_counter += 1\n",
    "                else:\n",
    "                    stop_counter = 0\n",
    "\n",
    "            print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return (x_epochs, y_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2saw960759Td",
    "outputId": "15fd19c7-cccc-403f-dd6f-1b223c54c84d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([78.7160], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = epsilon_Loss(given_fn, model, \n",
    "                            LOWER_BOUND, UPPER_BOUND, N_POINTS)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wgx4K0yugVxX",
    "outputId": "ae40cf07-991b-46ce-e62b-da3c83c7990a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([78.7160], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = epsilon_Loss_penalty(given_fn, model, LAMBDA_PEN, \n",
    "                            LOWER_BOUND, UPPER_BOUND, N_POINTS)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hbBxk8_fncyl"
   },
   "outputs": [],
   "source": [
    "# DEFINE HYPER-PARAMETERS\n",
    "batch_size = 50\n",
    "learningRate = 0.05\n",
    "num_epochs = 2000\n",
    "# num_epochs = int(num_iters/(len(x_train)/batch_size))\n",
    "\n",
    "#INIT PARAMETERS: \n",
    "# v_x = given_fn\n",
    "l_b = -10\n",
    "u_b = 10\n",
    "n_points = 5\n",
    "\n",
    "#INIT MODEL\n",
    "model = Nonlinear(20)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# INIT OPTIMIZER CLASS\n",
    "# What is an optimizer: \n",
    "# SGD:\n",
    "# SGD_optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "# Adam:\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                    lr=learningRate, \n",
    "                                    betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, \n",
    "                                    weight_decay=0, \n",
    "                                    amsgrad=False)\n",
    "\n",
    "# INIT LOSS FUNCTION: MSE\n",
    "# criterion = epsilon_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZbaJ3MsB08r",
    "outputId": "70c00d24-aea7-4cf6-c1f3-7278a6acad72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.206820487976074\n",
      "epoch 1, loss 2.2062392234802246\n",
      "epoch 2, loss 2.2056500911712646\n",
      "epoch 3, loss 2.2050657272338867\n",
      "epoch 4, loss 2.2044878005981445\n",
      "epoch 5, loss 2.203901529312134\n",
      "epoch 6, loss 2.2033164501190186\n",
      "epoch 7, loss 2.202737808227539\n",
      "epoch 8, loss 2.202150821685791\n",
      "epoch 9, loss 2.201566696166992\n",
      "epoch 10, loss 2.2009904384613037\n",
      "epoch 11, loss 2.2004003524780273\n",
      "epoch 12, loss 2.199820041656494\n",
      "epoch 13, loss 2.1992337703704834\n",
      "epoch 14, loss 2.198648691177368\n",
      "epoch 15, loss 2.1980624198913574\n",
      "epoch 16, loss 2.1974761486053467\n",
      "epoch 17, loss 2.196892261505127\n",
      "epoch 18, loss 2.1963016986846924\n",
      "epoch 19, loss 2.195713996887207\n",
      "epoch 20, loss 2.195134401321411\n",
      "epoch 21, loss 2.1945412158966064\n",
      "epoch 22, loss 2.1939547061920166\n",
      "epoch 23, loss 2.193363666534424\n",
      "epoch 24, loss 2.192781686782837\n",
      "epoch 25, loss 2.192185640335083\n",
      "epoch 26, loss 2.1916096210479736\n",
      "epoch 27, loss 2.1910159587860107\n",
      "epoch 28, loss 2.1904232501983643\n",
      "epoch 29, loss 2.189840078353882\n",
      "epoch 30, loss 2.189251184463501\n",
      "epoch 31, loss 2.1886558532714844\n",
      "epoch 32, loss 2.1880621910095215\n",
      "epoch 33, loss 2.1874754428863525\n",
      "epoch 34, loss 2.18688702583313\n",
      "epoch 35, loss 2.1862947940826416\n",
      "epoch 36, loss 2.185704469680786\n",
      "epoch 37, loss 2.1851186752319336\n",
      "epoch 38, loss 2.18452787399292\n",
      "epoch 39, loss 2.183932304382324\n",
      "epoch 40, loss 2.1833460330963135\n",
      "epoch 41, loss 2.182750701904297\n",
      "epoch 42, loss 2.182156801223755\n",
      "epoch 43, loss 2.181570053100586\n",
      "epoch 44, loss 2.1809794902801514\n",
      "epoch 45, loss 2.1803877353668213\n",
      "epoch 46, loss 2.179795265197754\n",
      "epoch 47, loss 2.1792047023773193\n",
      "epoch 48, loss 2.1786093711853027\n",
      "epoch 49, loss 2.178020477294922\n",
      "Pre normalize: \n",
      "tensor([[ 63.9143,  32.5415, -23.8277,  39.4093,  53.0178,  91.4155, -76.2707,\n",
      "           9.1649, -36.5506,  26.9537,  -3.3574, -70.8409,   1.9468,  32.8858,\n",
      "           6.9773, -27.4001, -84.6210,  32.5219,  11.3357,  35.1452]])\n",
      "tensor([-59.6793])\n",
      "After normalize: \n",
      "tensor([[ 64.4426,  32.8105, -24.0246,  39.7351,  53.4561,  92.1711, -76.9011,\n",
      "           9.2406, -36.8528,  27.1765,  -3.3851, -71.4265,   1.9628,  33.1576,\n",
      "           7.0350, -27.6266, -85.3204,  32.7907,  11.4294,  35.4357]])\n",
      "tensor([-60.1726])\n",
      "c value = tensor([1.0083], grad_fn=<MulBackward0>)\n",
      "epoch 50, loss 2.2135701179504395\n",
      "epoch 51, loss 2.2129716873168945\n",
      "epoch 52, loss 2.2123711109161377\n",
      "epoch 53, loss 2.211761474609375\n",
      "epoch 54, loss 2.2111709117889404\n",
      "epoch 55, loss 2.2105634212493896\n",
      "epoch 56, loss 2.2099528312683105\n",
      "epoch 57, loss 2.2093544006347656\n",
      "epoch 58, loss 2.208742380142212\n",
      "epoch 59, loss 2.2081329822540283\n",
      "epoch 60, loss 2.207526922225952\n",
      "epoch 61, loss 2.2069194316864014\n",
      "epoch 62, loss 2.2062973976135254\n",
      "epoch 63, loss 2.2056941986083984\n",
      "epoch 64, loss 2.205078363418579\n",
      "epoch 65, loss 2.204465627670288\n",
      "epoch 66, loss 2.2038464546203613\n",
      "epoch 67, loss 2.2032368183135986\n",
      "epoch 68, loss 2.202617883682251\n",
      "epoch 69, loss 2.202008008956909\n",
      "epoch 70, loss 2.201397657394409\n",
      "epoch 71, loss 2.200779914855957\n",
      "epoch 72, loss 2.2001686096191406\n",
      "epoch 73, loss 2.1995556354522705\n",
      "epoch 74, loss 2.1989448070526123\n",
      "epoch 75, loss 2.1983284950256348\n",
      "epoch 76, loss 2.197706699371338\n",
      "epoch 77, loss 2.1971027851104736\n",
      "epoch 78, loss 2.19648814201355\n",
      "epoch 79, loss 2.195878028869629\n",
      "epoch 80, loss 2.1952645778656006\n",
      "epoch 81, loss 2.1946520805358887\n",
      "epoch 82, loss 2.194033145904541\n",
      "epoch 83, loss 2.1934268474578857\n",
      "epoch 84, loss 2.1928114891052246\n",
      "epoch 85, loss 2.1921932697296143\n",
      "epoch 86, loss 2.191575050354004\n",
      "epoch 87, loss 2.1909735202789307\n",
      "epoch 88, loss 2.190350294113159\n",
      "epoch 89, loss 2.189742088317871\n",
      "epoch 90, loss 2.189131259918213\n",
      "epoch 91, loss 2.1885178089141846\n",
      "epoch 92, loss 2.1879031658172607\n",
      "epoch 93, loss 2.187288522720337\n",
      "epoch 94, loss 2.1866815090179443\n",
      "epoch 95, loss 2.186065673828125\n",
      "epoch 96, loss 2.185457229614258\n",
      "epoch 97, loss 2.1848394870758057\n",
      "epoch 98, loss 2.1842241287231445\n",
      "epoch 99, loss 2.1836156845092773\n",
      "Pre normalize: \n",
      "tensor([[ 64.4433,  32.8088, -24.0156,  39.7333,  53.4544,  92.1719, -76.9030,\n",
      "           9.2389, -36.8520,  27.1729,  -3.3872, -71.4257,   1.9619,  33.1587,\n",
      "           7.0359, -27.6259, -85.3222,  32.7888,  11.4284,  35.4340]])\n",
      "tensor([-60.1743])\n",
      "After normalize: \n",
      "tensor([[ 64.7505,  32.9652, -24.1301,  39.9228,  53.7092,  92.6113, -77.2696,\n",
      "           9.2829, -37.0277,  27.3024,  -3.4033, -71.7662,   1.9712,  33.3168,\n",
      "           7.0694, -27.7576, -85.7290,  32.9451,  11.4829,  35.6029]])\n",
      "tensor([-60.4612])\n",
      "c value = tensor([1.0048], grad_fn=<MulBackward0>)\n",
      "epoch 100, loss 2.2038633823394775\n",
      "epoch 101, loss 2.2032415866851807\n",
      "epoch 102, loss 2.2026219367980957\n",
      "epoch 103, loss 2.2019965648651123\n",
      "epoch 104, loss 2.2013773918151855\n",
      "epoch 105, loss 2.2007503509521484\n",
      "epoch 106, loss 2.200115442276001\n",
      "epoch 107, loss 2.199495792388916\n",
      "epoch 108, loss 2.1988723278045654\n",
      "epoch 109, loss 2.1982622146606445\n",
      "epoch 110, loss 2.19762921333313\n",
      "epoch 111, loss 2.1970088481903076\n",
      "epoch 112, loss 2.1963753700256348\n",
      "epoch 113, loss 2.1957452297210693\n",
      "epoch 114, loss 2.1951255798339844\n",
      "epoch 115, loss 2.1944987773895264\n",
      "epoch 116, loss 2.193873882293701\n",
      "epoch 117, loss 2.193244218826294\n",
      "epoch 118, loss 2.192612886428833\n",
      "epoch 119, loss 2.191984176635742\n",
      "epoch 120, loss 2.191357374191284\n",
      "epoch 121, loss 2.1907248497009277\n",
      "epoch 122, loss 2.1900999546051025\n",
      "epoch 123, loss 2.1894619464874268\n",
      "epoch 124, loss 2.1888318061828613\n",
      "epoch 125, loss 2.1882076263427734\n",
      "epoch 126, loss 2.1875720024108887\n",
      "epoch 127, loss 2.1869454383850098\n",
      "epoch 128, loss 2.1863081455230713\n",
      "epoch 129, loss 2.185685157775879\n",
      "epoch 130, loss 2.1850552558898926\n",
      "epoch 131, loss 2.18442702293396\n",
      "epoch 132, loss 2.1837995052337646\n",
      "epoch 133, loss 2.1831626892089844\n",
      "epoch 134, loss 2.1825315952301025\n",
      "epoch 135, loss 2.1819019317626953\n",
      "epoch 136, loss 2.181269645690918\n",
      "epoch 137, loss 2.180633306503296\n",
      "epoch 138, loss 2.1800100803375244\n",
      "epoch 139, loss 2.1793768405914307\n",
      "epoch 140, loss 2.1787400245666504\n",
      "epoch 141, loss 2.178114652633667\n",
      "epoch 142, loss 2.177466869354248\n",
      "epoch 143, loss 2.176832914352417\n",
      "epoch 144, loss 2.17620587348938\n",
      "epoch 145, loss 2.175577163696289\n",
      "epoch 146, loss 2.1749446392059326\n",
      "epoch 147, loss 2.174309492111206\n",
      "epoch 148, loss 2.173677921295166\n",
      "epoch 149, loss 2.1730456352233887\n",
      "Pre normalize: \n",
      "tensor([[ 64.7513,  32.9635, -24.1210,  39.9210,  53.7075,  92.6120, -77.2715,\n",
      "           9.2812, -37.0269,  27.2988,  -3.4054, -71.7654,   1.9702,  33.3180,\n",
      "           7.0704, -27.7569, -85.7309,  32.9432,  11.4818,  35.6012]])\n",
      "tensor([-60.4629])\n",
      "After normalize: \n",
      "tensor([[ 65.0876,  33.1347, -24.2463,  40.1283,  53.9864,  93.0930, -77.6728,\n",
      "           9.3294, -37.2192,  27.4405,  -3.4231, -72.1381,   1.9804,  33.4910,\n",
      "           7.1071, -27.9011, -86.1761,  33.1143,  11.5414,  35.7861]])\n",
      "tensor([-60.7769])\n",
      "c value = tensor([1.0052], grad_fn=<MulBackward0>)\n",
      "epoch 150, loss 2.195033311843872\n",
      "epoch 151, loss 2.194396734237671\n",
      "epoch 152, loss 2.1937546730041504\n",
      "epoch 153, loss 2.1931076049804688\n",
      "epoch 154, loss 2.1924612522125244\n",
      "epoch 155, loss 2.1918201446533203\n",
      "epoch 156, loss 2.1911818981170654\n",
      "epoch 157, loss 2.19053316116333\n",
      "epoch 158, loss 2.1898953914642334\n",
      "epoch 159, loss 2.189249277114868\n",
      "epoch 160, loss 2.1886112689971924\n",
      "epoch 161, loss 2.1879568099975586\n",
      "epoch 162, loss 2.1873159408569336\n",
      "epoch 163, loss 2.1866655349731445\n",
      "epoch 164, loss 2.186020851135254\n",
      "epoch 165, loss 2.1853742599487305\n",
      "epoch 166, loss 2.1847240924835205\n",
      "epoch 167, loss 2.184075355529785\n",
      "epoch 168, loss 2.183426856994629\n",
      "epoch 169, loss 2.182781457901001\n",
      "epoch 170, loss 2.182129144668579\n",
      "epoch 171, loss 2.181490421295166\n",
      "epoch 172, loss 2.1808459758758545\n",
      "epoch 173, loss 2.180189847946167\n",
      "epoch 174, loss 2.179547071456909\n",
      "epoch 175, loss 2.178898572921753\n",
      "epoch 176, loss 2.1782469749450684\n",
      "epoch 177, loss 2.1776037216186523\n",
      "epoch 178, loss 2.1769602298736572\n",
      "epoch 179, loss 2.1763083934783936\n",
      "epoch 180, loss 2.17565655708313\n",
      "epoch 181, loss 2.1750121116638184\n",
      "epoch 182, loss 2.1743576526641846\n",
      "epoch 183, loss 2.1737117767333984\n",
      "epoch 184, loss 2.173069477081299\n",
      "epoch 185, loss 2.172416925430298\n",
      "epoch 186, loss 2.171769380569458\n",
      "epoch 187, loss 2.1711208820343018\n",
      "epoch 188, loss 2.170473575592041\n",
      "epoch 189, loss 2.169834852218628\n",
      "epoch 190, loss 2.169179916381836\n",
      "epoch 191, loss 2.1685328483581543\n",
      "epoch 192, loss 2.1678874492645264\n",
      "epoch 193, loss 2.1672470569610596\n",
      "epoch 194, loss 2.166598320007324\n",
      "epoch 195, loss 2.165947437286377\n",
      "epoch 196, loss 2.1652979850769043\n",
      "epoch 197, loss 2.164654016494751\n",
      "epoch 198, loss 2.164000988006592\n",
      "epoch 199, loss 2.1633496284484863\n",
      "Pre normalize: \n",
      "tensor([[ 65.0883,  33.1330, -24.2372,  40.1264,  53.9847,  93.0938, -77.6747,\n",
      "           9.3276, -37.2184,  27.4369,  -3.4252, -72.1374,   1.9794,  33.4921,\n",
      "           7.1081, -27.9003, -86.1780,  33.1124,  11.5403,  35.7844]])\n",
      "tensor([-60.7786])\n",
      "After normalize: \n",
      "tensor([[ 65.4559,  33.3201, -24.3741,  40.3530,  54.2895,  93.6195, -78.1133,\n",
      "           9.3803, -37.4286,  27.5919,  -3.4445, -72.5447,   1.9906,  33.6813,\n",
      "           7.1482, -28.0579, -86.6646,  33.2993,  11.6054,  35.9865]])\n",
      "tensor([-61.1218])\n",
      "c value = tensor([1.0056], grad_fn=<MulBackward0>)\n",
      "epoch 200, loss 2.187195301055908\n",
      "epoch 201, loss 2.1865394115448\n",
      "epoch 202, loss 2.185887098312378\n",
      "epoch 203, loss 2.1852376461029053\n",
      "epoch 204, loss 2.1845812797546387\n",
      "epoch 205, loss 2.1839239597320557\n",
      "epoch 206, loss 2.1832664012908936\n",
      "epoch 207, loss 2.182605028152466\n",
      "epoch 208, loss 2.1819422245025635\n",
      "epoch 209, loss 2.181281566619873\n",
      "epoch 210, loss 2.1806228160858154\n",
      "epoch 211, loss 2.1799588203430176\n",
      "epoch 212, loss 2.179306745529175\n",
      "epoch 213, loss 2.178640842437744\n",
      "epoch 214, loss 2.1779727935791016\n",
      "epoch 215, loss 2.1773149967193604\n",
      "epoch 216, loss 2.1766464710235596\n",
      "epoch 217, loss 2.175990104675293\n",
      "epoch 218, loss 2.175321578979492\n",
      "epoch 219, loss 2.1746606826782227\n",
      "epoch 220, loss 2.17399001121521\n",
      "epoch 221, loss 2.173328161239624\n",
      "epoch 222, loss 2.1726620197296143\n",
      "epoch 223, loss 2.171997547149658\n",
      "epoch 224, loss 2.171334981918335\n",
      "epoch 225, loss 2.170668363571167\n",
      "epoch 226, loss 2.1700069904327393\n",
      "epoch 227, loss 2.1693389415740967\n",
      "epoch 228, loss 2.168678045272827\n",
      "epoch 229, loss 2.1680121421813965\n",
      "epoch 230, loss 2.1673402786254883\n",
      "epoch 231, loss 2.1666765213012695\n",
      "epoch 232, loss 2.166013479232788\n",
      "epoch 233, loss 2.1653518676757812\n",
      "epoch 234, loss 2.164686679840088\n",
      "epoch 235, loss 2.164017677307129\n",
      "epoch 236, loss 2.1633505821228027\n",
      "epoch 237, loss 2.16267991065979\n",
      "epoch 238, loss 2.1620213985443115\n",
      "epoch 239, loss 2.1613523960113525\n",
      "epoch 240, loss 2.1606945991516113\n",
      "epoch 241, loss 2.1600182056427\n",
      "epoch 242, loss 2.159348964691162\n",
      "epoch 243, loss 2.158679485321045\n",
      "epoch 244, loss 2.1580114364624023\n",
      "epoch 245, loss 2.1573433876037598\n",
      "epoch 246, loss 2.156680107116699\n",
      "epoch 247, loss 2.156017541885376\n",
      "epoch 248, loss 2.155343532562256\n",
      "epoch 249, loss 2.154677152633667\n",
      "Pre normalize: \n",
      "tensor([[ 65.4566,  33.3183, -24.3649,  40.3511,  54.2878,  93.6202, -78.1153,\n",
      "           9.3785, -37.4276,  27.5881,  -3.4467, -72.5440,   1.9895,  33.6824,\n",
      "           7.1492, -28.0571, -86.6665,  33.2974,  11.6043,  35.9847]])\n",
      "tensor([-61.1237])\n",
      "After normalize: \n",
      "tensor([[ 65.8535,  33.5204, -24.5126,  40.5958,  54.6169,  94.1878, -78.5889,\n",
      "           9.4354, -37.6546,  27.7554,  -3.4676, -72.9838,   2.0016,  33.8866,\n",
      "           7.1925, -28.2272, -87.1920,  33.4993,  11.6746,  36.2029]])\n",
      "tensor([-61.4943])\n",
      "c value = tensor([1.0061], grad_fn=<MulBackward0>)\n",
      "epoch 250, loss 2.180204153060913\n",
      "epoch 251, loss 2.1795332431793213\n",
      "epoch 252, loss 2.1788551807403564\n",
      "epoch 253, loss 2.178182601928711\n",
      "epoch 254, loss 2.177506923675537\n",
      "epoch 255, loss 2.1768264770507812\n",
      "epoch 256, loss 2.176149368286133\n",
      "epoch 257, loss 2.1754672527313232\n",
      "epoch 258, loss 2.1747894287109375\n",
      "epoch 259, loss 2.1741104125976562\n",
      "epoch 260, loss 2.173431158065796\n",
      "epoch 261, loss 2.1727511882781982\n",
      "epoch 262, loss 2.172075033187866\n",
      "epoch 263, loss 2.1713979244232178\n",
      "epoch 264, loss 2.1707167625427246\n",
      "epoch 265, loss 2.1700241565704346\n",
      "epoch 266, loss 2.1693508625030518\n",
      "epoch 267, loss 2.168659210205078\n",
      "epoch 268, loss 2.1679790019989014\n",
      "epoch 269, loss 2.1673014163970947\n",
      "epoch 270, loss 2.166618585586548\n",
      "epoch 271, loss 2.1659533977508545\n",
      "epoch 272, loss 2.165269374847412\n",
      "epoch 273, loss 2.1645829677581787\n",
      "epoch 274, loss 2.1639046669006348\n",
      "epoch 275, loss 2.163233757019043\n",
      "epoch 276, loss 2.162550687789917\n",
      "epoch 277, loss 2.1618618965148926\n",
      "epoch 278, loss 2.161186456680298\n",
      "epoch 279, loss 2.1605138778686523\n",
      "epoch 280, loss 2.1598312854766846\n",
      "epoch 281, loss 2.159149169921875\n",
      "epoch 282, loss 2.158459424972534\n",
      "epoch 283, loss 2.1577847003936768\n",
      "epoch 284, loss 2.157102108001709\n",
      "epoch 285, loss 2.156426429748535\n",
      "epoch 286, loss 2.1557366847991943\n",
      "epoch 287, loss 2.1550583839416504\n",
      "epoch 288, loss 2.15437650680542\n",
      "epoch 289, loss 2.1537036895751953\n",
      "epoch 290, loss 2.153024435043335\n",
      "epoch 291, loss 2.1523358821868896\n",
      "epoch 292, loss 2.1516573429107666\n",
      "epoch 293, loss 2.1509742736816406\n",
      "epoch 294, loss 2.150294065475464\n",
      "epoch 295, loss 2.149599075317383\n",
      "epoch 296, loss 2.14892315864563\n",
      "epoch 297, loss 2.1482341289520264\n",
      "epoch 298, loss 2.147555351257324\n",
      "epoch 299, loss 2.1468727588653564\n",
      "Pre normalize: \n",
      "tensor([[ 65.8543,  33.5186, -24.5034,  40.5939,  54.6151,  94.1886, -78.5908,\n",
      "           9.4336, -37.6536,  27.7517,  -3.4697, -72.9830,   2.0004,  33.8878,\n",
      "           7.1935, -28.2264, -87.1939,  33.4974,  11.6734,  36.2011]])\n",
      "tensor([-61.4961])\n",
      "After normalize: \n",
      "tensor([[ 66.2801,  33.7354, -24.6618,  40.8564,  54.9682,  94.7977, -79.0990,\n",
      "           9.4946, -37.8971,  27.9312,  -3.4921, -73.4550,   2.0134,  34.1069,\n",
      "           7.2400, -28.4090, -87.7578,  33.7140,  11.7489,  36.4352]])\n",
      "tensor([-61.8938])\n",
      "c value = tensor([1.0065], grad_fn=<MulBackward0>)\n",
      "epoch 300, loss 2.1740336418151855\n",
      "epoch 301, loss 2.1733407974243164\n",
      "epoch 302, loss 2.1726431846618652\n",
      "epoch 303, loss 2.1719486713409424\n",
      "epoch 304, loss 2.171254873275757\n",
      "epoch 305, loss 2.170557737350464\n",
      "epoch 306, loss 2.1698780059814453\n",
      "epoch 307, loss 2.1691677570343018\n",
      "epoch 308, loss 2.1684815883636475\n",
      "epoch 309, loss 2.1677744388580322\n",
      "epoch 310, loss 2.1670825481414795\n",
      "epoch 311, loss 2.1663830280303955\n",
      "epoch 312, loss 2.165677309036255\n",
      "epoch 313, loss 2.164979934692383\n",
      "epoch 314, loss 2.1642816066741943\n",
      "epoch 315, loss 2.163578748703003\n",
      "epoch 316, loss 2.1628777980804443\n",
      "epoch 317, loss 2.1621804237365723\n",
      "epoch 318, loss 2.1614723205566406\n",
      "epoch 319, loss 2.160778284072876\n",
      "epoch 320, loss 2.1600728034973145\n",
      "epoch 321, loss 2.159371852874756\n",
      "epoch 322, loss 2.158665895462036\n",
      "epoch 323, loss 2.1579744815826416\n",
      "epoch 324, loss 2.1572694778442383\n",
      "epoch 325, loss 2.156566619873047\n",
      "epoch 326, loss 2.155863046646118\n",
      "epoch 327, loss 2.155158042907715\n",
      "epoch 328, loss 2.1544675827026367\n",
      "epoch 329, loss 2.153768301010132\n",
      "epoch 330, loss 2.153064250946045\n",
      "epoch 331, loss 2.1523659229278564\n",
      "epoch 332, loss 2.1516659259796143\n",
      "epoch 333, loss 2.1509621143341064\n",
      "epoch 334, loss 2.150264024734497\n",
      "epoch 335, loss 2.1495702266693115\n",
      "epoch 336, loss 2.148866891860962\n",
      "epoch 337, loss 2.1481659412384033\n",
      "epoch 338, loss 2.1474521160125732\n",
      "epoch 339, loss 2.1467692852020264\n",
      "epoch 340, loss 2.146069049835205\n",
      "epoch 341, loss 2.1453633308410645\n",
      "epoch 342, loss 2.1446590423583984\n",
      "epoch 343, loss 2.143965244293213\n",
      "epoch 344, loss 2.1432714462280273\n",
      "epoch 345, loss 2.1425681114196777\n",
      "epoch 346, loss 2.1418676376342773\n",
      "epoch 347, loss 2.1411666870117188\n",
      "epoch 348, loss 2.140468120574951\n",
      "epoch 349, loss 2.1397669315338135\n",
      "Pre normalize: \n",
      "tensor([[ 66.2809,  33.7336, -24.6525,  40.8545,  54.9663,  94.7986, -79.1009,\n",
      "           9.4928, -37.8961,  27.9274,  -3.4943, -73.4542,   2.0122,  34.1081,\n",
      "           7.2410, -28.4082, -87.7597,  33.7121,  11.7477,  36.4333]])\n",
      "tensor([-61.8957])\n",
      "After normalize: \n",
      "tensor([[ 66.7401,  33.9673, -24.8233,  41.1375,  55.3471,  95.4554, -79.6489,\n",
      "           9.5585, -38.1587,  28.1209,  -3.5185, -73.9631,   2.0261,  34.3443,\n",
      "           7.2912, -28.6050, -88.3677,  33.9457,  11.8290,  36.6857]])\n",
      "tensor([-62.3245])\n",
      "c value = tensor([1.0069], grad_fn=<MulBackward0>)\n",
      "epoch 350, loss 2.1688084602355957\n",
      "epoch 351, loss 2.1681127548217773\n",
      "epoch 352, loss 2.167391777038574\n",
      "epoch 353, loss 2.1666791439056396\n",
      "epoch 354, loss 2.1659748554229736\n",
      "epoch 355, loss 2.1652514934539795\n",
      "epoch 356, loss 2.1645359992980957\n",
      "epoch 357, loss 2.163825750350952\n",
      "epoch 358, loss 2.1631102561950684\n",
      "epoch 359, loss 2.1624021530151367\n",
      "epoch 360, loss 2.16168475151062\n",
      "epoch 361, loss 2.1609694957733154\n",
      "epoch 362, loss 2.1602590084075928\n",
      "epoch 363, loss 2.159550428390503\n",
      "epoch 364, loss 2.1588258743286133\n",
      "epoch 365, loss 2.15810489654541\n",
      "epoch 366, loss 2.1573898792266846\n",
      "epoch 367, loss 2.156665802001953\n",
      "epoch 368, loss 2.155945301055908\n",
      "epoch 369, loss 2.155235528945923\n",
      "epoch 370, loss 2.154510021209717\n",
      "epoch 371, loss 2.1538007259368896\n",
      "epoch 372, loss 2.1530752182006836\n",
      "epoch 373, loss 2.152358293533325\n",
      "epoch 374, loss 2.151641607284546\n",
      "epoch 375, loss 2.1509218215942383\n",
      "epoch 376, loss 2.1502115726470947\n",
      "epoch 377, loss 2.149482011795044\n",
      "epoch 378, loss 2.148768424987793\n",
      "epoch 379, loss 2.1480517387390137\n",
      "epoch 380, loss 2.147338628768921\n",
      "epoch 381, loss 2.1466221809387207\n",
      "epoch 382, loss 2.1459052562713623\n",
      "epoch 383, loss 2.145174741744995\n",
      "epoch 384, loss 2.1444568634033203\n",
      "epoch 385, loss 2.14374041557312\n",
      "epoch 386, loss 2.1430251598358154\n",
      "epoch 387, loss 2.1423020362854004\n",
      "epoch 388, loss 2.1415796279907227\n",
      "epoch 389, loss 2.140861749649048\n",
      "epoch 390, loss 2.140139579772949\n",
      "epoch 391, loss 2.1394190788269043\n",
      "epoch 392, loss 2.1387007236480713\n",
      "epoch 393, loss 2.137981653213501\n",
      "epoch 394, loss 2.1372616291046143\n",
      "epoch 395, loss 2.136535406112671\n",
      "epoch 396, loss 2.1358182430267334\n",
      "epoch 397, loss 2.135101556777954\n",
      "epoch 398, loss 2.134373188018799\n",
      "epoch 399, loss 2.1336605548858643\n",
      "Pre normalize: \n",
      "tensor([[ 66.7408,  33.9654, -24.8139,  41.1356,  55.3452,  95.4564, -79.6508,\n",
      "           9.5567, -38.1577,  28.1171,  -3.5207, -73.9623,   2.0249,  34.3455,\n",
      "           7.2922, -28.6043, -88.3696,  33.9437,  11.8277,  36.6838]])\n",
      "tensor([-62.3264])\n",
      "After normalize: \n",
      "tensor([[ 67.2307,  34.2147, -24.9961,  41.4375,  55.7515,  96.1570, -80.2355,\n",
      "           9.6268, -38.4378,  28.3235,  -3.5466, -74.5053,   2.0398,  34.5976,\n",
      "           7.3457, -28.8142, -89.0182,  34.1929,  11.9145,  36.9530]])\n",
      "tensor([-62.7839])\n",
      "c value = tensor([1.0073], grad_fn=<MulBackward0>)\n",
      "epoch 400, loss 2.164367437362671\n",
      "epoch 401, loss 2.163637161254883\n",
      "epoch 402, loss 2.16290545463562\n",
      "epoch 403, loss 2.1621716022491455\n",
      "epoch 404, loss 2.1614341735839844\n",
      "epoch 405, loss 2.160705327987671\n",
      "epoch 406, loss 2.1599740982055664\n",
      "epoch 407, loss 2.159237861633301\n",
      "epoch 408, loss 2.158513069152832\n",
      "epoch 409, loss 2.1577796936035156\n",
      "epoch 410, loss 2.1570546627044678\n",
      "epoch 411, loss 2.1563241481781006\n",
      "epoch 412, loss 2.155590534210205\n",
      "epoch 413, loss 2.1548640727996826\n",
      "epoch 414, loss 2.1541314125061035\n",
      "epoch 415, loss 2.15339994430542\n",
      "epoch 416, loss 2.152667999267578\n",
      "epoch 417, loss 2.151933193206787\n",
      "epoch 418, loss 2.151193857192993\n",
      "epoch 419, loss 2.150463819503784\n",
      "epoch 420, loss 2.1497316360473633\n",
      "epoch 421, loss 2.148988962173462\n",
      "epoch 422, loss 2.148264169692993\n",
      "epoch 423, loss 2.1475162506103516\n",
      "epoch 424, loss 2.1467840671539307\n",
      "epoch 425, loss 2.146044969558716\n",
      "epoch 426, loss 2.1453073024749756\n",
      "epoch 427, loss 2.1445696353912354\n",
      "epoch 428, loss 2.143829345703125\n",
      "epoch 429, loss 2.143101930618286\n",
      "epoch 430, loss 2.1423556804656982\n",
      "epoch 431, loss 2.14162015914917\n",
      "epoch 432, loss 2.140883207321167\n",
      "epoch 433, loss 2.140143632888794\n",
      "epoch 434, loss 2.13940691947937\n",
      "epoch 435, loss 2.1386709213256836\n",
      "epoch 436, loss 2.1379377841949463\n",
      "epoch 437, loss 2.1372017860412598\n",
      "epoch 438, loss 2.136455535888672\n",
      "epoch 439, loss 2.135725259780884\n",
      "epoch 440, loss 2.1349878311157227\n",
      "epoch 441, loss 2.1342499256134033\n",
      "epoch 442, loss 2.133510112762451\n",
      "epoch 443, loss 2.132775068283081\n",
      "epoch 444, loss 2.132045030593872\n",
      "epoch 445, loss 2.1313014030456543\n",
      "epoch 446, loss 2.130565643310547\n",
      "epoch 447, loss 2.129829168319702\n",
      "epoch 448, loss 2.129086494445801\n",
      "epoch 449, loss 2.1283538341522217\n",
      "Pre normalize: \n",
      "tensor([[ 67.2315,  34.2128, -24.9867,  41.4356,  55.7496,  96.1582, -80.2374,\n",
      "           9.6249, -38.4369,  28.3197,  -3.5488, -74.5045,   2.0385,  34.5987,\n",
      "           7.3467, -28.8135, -89.0201,  34.1908,  11.9132,  36.9511]])\n",
      "tensor([-62.7859])\n",
      "After normalize: \n",
      "tensor([[ 67.7517,  34.4775, -25.1800,  41.7562,  56.1809,  96.9021, -80.8582,\n",
      "           9.6994, -38.7343,  28.5388,  -3.5763, -75.0809,   2.0543,  34.8664,\n",
      "           7.4036, -29.0364, -89.7089,  34.4553,  12.0053,  37.2370]])\n",
      "tensor([-63.2716])\n",
      "c value = tensor([1.0077], grad_fn=<MulBackward0>)\n",
      "epoch 450, loss 2.1606616973876953\n",
      "epoch 451, loss 2.1599156856536865\n",
      "epoch 452, loss 2.1591618061065674\n",
      "epoch 453, loss 2.15840744972229\n",
      "epoch 454, loss 2.157658576965332\n",
      "epoch 455, loss 2.156904697418213\n",
      "epoch 456, loss 2.15614652633667\n",
      "epoch 457, loss 2.1553900241851807\n",
      "epoch 458, loss 2.154639720916748\n",
      "epoch 459, loss 2.1538782119750977\n",
      "epoch 460, loss 2.153123140335083\n",
      "epoch 461, loss 2.1523704528808594\n",
      "epoch 462, loss 2.1516125202178955\n",
      "epoch 463, loss 2.150847911834717\n",
      "epoch 464, loss 2.1500966548919678\n",
      "epoch 465, loss 2.149338722229004\n",
      "epoch 466, loss 2.148576259613037\n",
      "epoch 467, loss 2.1478121280670166\n",
      "epoch 468, loss 2.1470589637756348\n",
      "epoch 469, loss 2.1462879180908203\n",
      "epoch 470, loss 2.14553165435791\n",
      "epoch 471, loss 2.144775867462158\n",
      "epoch 472, loss 2.144012451171875\n",
      "epoch 473, loss 2.1432530879974365\n",
      "epoch 474, loss 2.142493724822998\n",
      "epoch 475, loss 2.1417362689971924\n",
      "epoch 476, loss 2.140972137451172\n",
      "epoch 477, loss 2.140209674835205\n",
      "epoch 478, loss 2.1394588947296143\n",
      "epoch 479, loss 2.138705015182495\n",
      "epoch 480, loss 2.1379477977752686\n",
      "epoch 481, loss 2.137186050415039\n",
      "epoch 482, loss 2.136429786682129\n",
      "epoch 483, loss 2.1356639862060547\n",
      "epoch 484, loss 2.13490629196167\n",
      "epoch 485, loss 2.1341474056243896\n",
      "epoch 486, loss 2.133392572402954\n",
      "epoch 487, loss 2.1326346397399902\n",
      "epoch 488, loss 2.131875514984131\n",
      "epoch 489, loss 2.131124973297119\n",
      "epoch 490, loss 2.1303646564483643\n",
      "epoch 491, loss 2.1296138763427734\n",
      "epoch 492, loss 2.1288533210754395\n",
      "epoch 493, loss 2.1280972957611084\n",
      "epoch 494, loss 2.1273365020751953\n",
      "epoch 495, loss 2.126582145690918\n",
      "epoch 496, loss 2.125828981399536\n",
      "epoch 497, loss 2.1250665187835693\n",
      "epoch 498, loss 2.124314546585083\n",
      "epoch 499, loss 2.123554229736328\n",
      "Pre normalize: \n",
      "tensor([[ 67.7524,  34.4756, -25.1705,  41.7542,  56.1790,  96.9033, -80.8601,\n",
      "           9.6975, -38.7333,  28.5349,  -3.5786, -75.0801,   2.0529,  34.8676,\n",
      "           7.4046, -29.0355, -89.7108,  34.4532,  12.0039,  37.2351]])\n",
      "tensor([-63.2737])\n",
      "After normalize: \n",
      "tensor([[ 68.3123,  34.7605, -25.3785,  42.0993,  56.6432,  97.7041, -81.5283,\n",
      "           9.7776, -39.0534,  28.7707,  -3.6081, -75.7005,   2.0699,  35.1557,\n",
      "           7.4658, -29.2755, -90.4521,  34.7379,  12.1031,  37.5428]])\n",
      "tensor([-63.7966])\n",
      "c value = tensor([1.0083], grad_fn=<MulBackward0>)\n",
      "epoch 500, loss 2.1580352783203125\n",
      "epoch 501, loss 2.1572649478912354\n",
      "epoch 502, loss 2.156491279602051\n",
      "epoch 503, loss 2.155717134475708\n",
      "epoch 504, loss 2.1549532413482666\n",
      "epoch 505, loss 2.154179811477661\n",
      "epoch 506, loss 2.1534066200256348\n",
      "epoch 507, loss 2.152637004852295\n",
      "epoch 508, loss 2.1518750190734863\n",
      "epoch 509, loss 2.151097059249878\n",
      "epoch 510, loss 2.1503183841705322\n",
      "epoch 511, loss 2.1495473384857178\n",
      "epoch 512, loss 2.1487677097320557\n",
      "epoch 513, loss 2.147998571395874\n",
      "epoch 514, loss 2.1472246646881104\n",
      "epoch 515, loss 2.1464455127716064\n",
      "epoch 516, loss 2.145674705505371\n",
      "epoch 517, loss 2.144893169403076\n",
      "epoch 518, loss 2.144120454788208\n",
      "epoch 519, loss 2.1433398723602295\n",
      "epoch 520, loss 2.1425652503967285\n",
      "epoch 521, loss 2.141784191131592\n",
      "epoch 522, loss 2.1410086154937744\n",
      "epoch 523, loss 2.140233039855957\n",
      "epoch 524, loss 2.1394505500793457\n",
      "epoch 525, loss 2.1386783123016357\n",
      "epoch 526, loss 2.1379008293151855\n",
      "epoch 527, loss 2.1371238231658936\n",
      "epoch 528, loss 2.1363320350646973\n",
      "epoch 529, loss 2.1355650424957275\n",
      "epoch 530, loss 2.1347813606262207\n",
      "epoch 531, loss 2.134007215499878\n",
      "epoch 532, loss 2.1332271099090576\n",
      "epoch 533, loss 2.132448196411133\n",
      "epoch 534, loss 2.1316771507263184\n",
      "epoch 535, loss 2.1308975219726562\n",
      "epoch 536, loss 2.130117893218994\n",
      "epoch 537, loss 2.1293416023254395\n",
      "epoch 538, loss 2.1285667419433594\n",
      "epoch 539, loss 2.12778377532959\n",
      "epoch 540, loss 2.1270132064819336\n",
      "epoch 541, loss 2.1262307167053223\n",
      "epoch 542, loss 2.125454902648926\n",
      "epoch 543, loss 2.124676465988159\n",
      "epoch 544, loss 2.1238975524902344\n",
      "epoch 545, loss 2.1231167316436768\n",
      "epoch 546, loss 2.122344970703125\n",
      "epoch 547, loss 2.12156343460083\n",
      "epoch 548, loss 2.1207923889160156\n",
      "epoch 549, loss 2.120013475418091\n",
      "Pre normalize: \n",
      "tensor([[ 68.3131,  34.7586, -25.3689,  42.0972,  56.6413,  97.7052, -81.5302,\n",
      "           9.7756, -39.0524,  28.7668,  -3.6105, -75.6996,   2.0685,  35.1569,\n",
      "           7.4668, -29.2746, -90.4540,  34.7358,  12.1016,  37.5408]])\n",
      "tensor([-63.7987])\n",
      "After normalize: \n",
      "tensor([[ 68.9034,  35.0589, -25.5881,  42.4609,  57.1307,  98.5494, -82.2347,\n",
      "           9.8601, -39.3899,  29.0154,  -3.6417, -76.3537,   2.0864,  35.4606,\n",
      "           7.5313, -29.5276, -91.2356,  35.0360,  12.2062,  37.8652]])\n",
      "tensor([-64.3500])\n",
      "c value = tensor([1.0086], grad_fn=<MulBackward0>)\n",
      "epoch 550, loss 2.156010389328003\n",
      "epoch 551, loss 2.1552324295043945\n",
      "epoch 552, loss 2.15443754196167\n",
      "epoch 553, loss 2.1536483764648438\n",
      "epoch 554, loss 2.1528544425964355\n",
      "epoch 555, loss 2.1520602703094482\n",
      "epoch 556, loss 2.151257276535034\n",
      "epoch 557, loss 2.150453567504883\n",
      "epoch 558, loss 2.1496617794036865\n",
      "epoch 559, loss 2.148857593536377\n",
      "epoch 560, loss 2.148057222366333\n",
      "epoch 561, loss 2.14725923538208\n",
      "epoch 562, loss 2.1464600563049316\n",
      "epoch 563, loss 2.14567232131958\n",
      "epoch 564, loss 2.144869565963745\n",
      "epoch 565, loss 2.1440722942352295\n",
      "epoch 566, loss 2.1432759761810303\n",
      "epoch 567, loss 2.142467498779297\n",
      "epoch 568, loss 2.1416690349578857\n",
      "epoch 569, loss 2.1408779621124268\n",
      "epoch 570, loss 2.1400842666625977\n",
      "epoch 571, loss 2.1392853260040283\n",
      "epoch 572, loss 2.138481378555298\n",
      "epoch 573, loss 2.137697696685791\n",
      "epoch 574, loss 2.1368956565856934\n",
      "epoch 575, loss 2.1361002922058105\n",
      "epoch 576, loss 2.135293960571289\n",
      "epoch 577, loss 2.134507417678833\n",
      "epoch 578, loss 2.1337075233459473\n",
      "epoch 579, loss 2.1329095363616943\n",
      "epoch 580, loss 2.1321120262145996\n",
      "epoch 581, loss 2.131314992904663\n",
      "epoch 582, loss 2.1305179595947266\n",
      "epoch 583, loss 2.1297192573547363\n",
      "epoch 584, loss 2.1289255619049072\n",
      "epoch 585, loss 2.1281301975250244\n",
      "epoch 586, loss 2.1273269653320312\n",
      "epoch 587, loss 2.1265223026275635\n",
      "epoch 588, loss 2.1257224082946777\n",
      "epoch 589, loss 2.124929904937744\n",
      "epoch 590, loss 2.124126434326172\n",
      "epoch 591, loss 2.123333215713501\n",
      "epoch 592, loss 2.122530221939087\n",
      "epoch 593, loss 2.121737241744995\n",
      "epoch 594, loss 2.1209380626678467\n",
      "epoch 595, loss 2.1201391220092773\n",
      "epoch 596, loss 2.1193394660949707\n",
      "epoch 597, loss 2.1185474395751953\n",
      "epoch 598, loss 2.117753267288208\n",
      "epoch 599, loss 2.1169486045837402\n",
      "Pre normalize: \n",
      "tensor([[ 68.9041,  35.0570, -25.5784,  42.4588,  57.1286,  98.5506, -82.2368,\n",
      "           9.8581, -39.3889,  29.0115,  -3.6440, -76.3526,   2.0849,  35.4618,\n",
      "           7.5324, -29.5267, -91.2375,  35.0339,  12.2046,  37.8631]])\n",
      "tensor([-64.3521])\n",
      "After normalize: \n",
      "tensor([[ 69.5327,  35.3768, -25.8118,  42.8462,  57.6498,  99.4497, -82.9870,\n",
      "           9.9480, -39.7483,  29.2761,  -3.6772, -77.0492,   2.1040,  35.7853,\n",
      "           7.6011, -29.7961, -92.0699,  35.3535,  12.3159,  38.2086]])\n",
      "tensor([-64.9392])\n",
      "c value = tensor([1.0091], grad_fn=<MulBackward0>)\n",
      "epoch 600, loss 2.1549410820007324\n",
      "epoch 601, loss 2.154132127761841\n",
      "epoch 602, loss 2.1533076763153076\n",
      "epoch 603, loss 2.1525051593780518\n",
      "epoch 604, loss 2.1516871452331543\n",
      "epoch 605, loss 2.1508684158325195\n",
      "epoch 606, loss 2.1500511169433594\n",
      "epoch 607, loss 2.149245500564575\n",
      "epoch 608, loss 2.148425340652466\n",
      "epoch 609, loss 2.1476035118103027\n",
      "epoch 610, loss 2.1467950344085693\n",
      "epoch 611, loss 2.145967960357666\n",
      "epoch 612, loss 2.1451473236083984\n",
      "epoch 613, loss 2.144329786300659\n",
      "epoch 614, loss 2.1435108184814453\n",
      "epoch 615, loss 2.1427016258239746\n",
      "epoch 616, loss 2.141876697540283\n",
      "epoch 617, loss 2.1410510540008545\n",
      "epoch 618, loss 2.1402366161346436\n",
      "epoch 619, loss 2.139420509338379\n",
      "epoch 620, loss 2.138592004776001\n",
      "epoch 621, loss 2.137773275375366\n",
      "epoch 622, loss 2.1369423866271973\n",
      "epoch 623, loss 2.136129379272461\n",
      "epoch 624, loss 2.135313034057617\n",
      "epoch 625, loss 2.1344964504241943\n",
      "epoch 626, loss 2.1336755752563477\n",
      "epoch 627, loss 2.1328558921813965\n",
      "epoch 628, loss 2.132030487060547\n",
      "epoch 629, loss 2.1312131881713867\n",
      "epoch 630, loss 2.130385160446167\n",
      "epoch 631, loss 2.1295759677886963\n",
      "epoch 632, loss 2.1287522315979004\n",
      "epoch 633, loss 2.1279308795928955\n",
      "epoch 634, loss 2.127119302749634\n",
      "epoch 635, loss 2.126293182373047\n",
      "epoch 636, loss 2.125479221343994\n",
      "epoch 637, loss 2.1246485710144043\n",
      "epoch 638, loss 2.1238322257995605\n",
      "epoch 639, loss 2.123018741607666\n",
      "epoch 640, loss 2.1222035884857178\n",
      "epoch 641, loss 2.1213817596435547\n",
      "epoch 642, loss 2.120570421218872\n",
      "epoch 643, loss 2.119753122329712\n",
      "epoch 644, loss 2.118934392929077\n",
      "epoch 645, loss 2.1181209087371826\n",
      "epoch 646, loss 2.1173095703125\n",
      "epoch 647, loss 2.116492986679077\n",
      "epoch 648, loss 2.1156656742095947\n",
      "epoch 649, loss 2.114856719970703\n",
      "Pre normalize: \n",
      "tensor([[ 69.5335,  35.3748, -25.8020,  42.8441,  57.6477,  99.4508, -82.9893,\n",
      "           9.9460, -39.7473,  29.2722,  -3.6796, -77.0480,   2.1024,  35.7865,\n",
      "           7.6021, -29.7952, -92.0718,  35.3514,  12.3143,  38.2065]])\n",
      "tensor([-64.9415])\n",
      "After normalize: \n",
      "tensor([[ 70.1966,  35.7121, -26.0481,  43.2527,  58.1975, 100.3992, -83.7807,\n",
      "          10.0409, -40.1264,  29.5514,  -3.7147, -77.7828,   2.1225,  36.1278,\n",
      "           7.6746, -30.0794, -92.9499,  35.6885,  12.4317,  38.5708]])\n",
      "tensor([-65.5608])\n",
      "c value = tensor([1.0095], grad_fn=<MulBackward0>)\n",
      "epoch 650, loss 2.1545567512512207\n",
      "epoch 651, loss 2.1537272930145264\n",
      "epoch 652, loss 2.1528968811035156\n",
      "epoch 653, loss 2.152048349380493\n",
      "epoch 654, loss 2.151216983795166\n",
      "epoch 655, loss 2.150383949279785\n",
      "epoch 656, loss 2.1495449542999268\n",
      "epoch 657, loss 2.1487069129943848\n",
      "epoch 658, loss 2.147871732711792\n",
      "epoch 659, loss 2.1470320224761963\n",
      "epoch 660, loss 2.1461851596832275\n",
      "epoch 661, loss 2.145350933074951\n",
      "epoch 662, loss 2.144500732421875\n",
      "epoch 663, loss 2.1436619758605957\n",
      "epoch 664, loss 2.1428189277648926\n",
      "epoch 665, loss 2.1419689655303955\n",
      "epoch 666, loss 2.1411304473876953\n",
      "epoch 667, loss 2.1402807235717773\n",
      "epoch 668, loss 2.139439582824707\n",
      "epoch 669, loss 2.1385996341705322\n",
      "epoch 670, loss 2.1377580165863037\n",
      "epoch 671, loss 2.13690447807312\n",
      "epoch 672, loss 2.1360726356506348\n",
      "epoch 673, loss 2.1352224349975586\n",
      "epoch 674, loss 2.134377956390381\n",
      "epoch 675, loss 2.133538246154785\n",
      "epoch 676, loss 2.132700204849243\n",
      "epoch 677, loss 2.1318602561950684\n",
      "epoch 678, loss 2.131014823913574\n",
      "epoch 679, loss 2.1301774978637695\n",
      "epoch 680, loss 2.129333734512329\n",
      "epoch 681, loss 2.1285035610198975\n",
      "epoch 682, loss 2.1276583671569824\n",
      "epoch 683, loss 2.126823663711548\n",
      "epoch 684, loss 2.125980854034424\n",
      "epoch 685, loss 2.1251330375671387\n",
      "epoch 686, loss 2.124298572540283\n",
      "epoch 687, loss 2.1234545707702637\n",
      "epoch 688, loss 2.1226184368133545\n",
      "epoch 689, loss 2.121777296066284\n",
      "epoch 690, loss 2.120939016342163\n",
      "epoch 691, loss 2.120094060897827\n",
      "epoch 692, loss 2.1192474365234375\n",
      "epoch 693, loss 2.118410348892212\n",
      "epoch 694, loss 2.117567777633667\n",
      "epoch 695, loss 2.11672043800354\n",
      "epoch 696, loss 2.115877866744995\n",
      "epoch 697, loss 2.1150403022766113\n",
      "epoch 698, loss 2.1141977310180664\n",
      "epoch 699, loss 2.113356113433838\n",
      "Pre normalize: \n",
      "tensor([[ 70.1974,  35.7100, -26.0382,  43.2506,  58.1954, 100.4004, -83.7830,\n",
      "          10.0388, -40.1254,  29.5474,  -3.7171, -77.7817,   2.1209,  36.1291,\n",
      "           7.6757, -30.0785, -92.9518,  35.6862,  12.4300,  38.5687]])\n",
      "tensor([-65.5631])\n",
      "After normalize: \n",
      "tensor([[ 70.8993,  36.0671, -26.2986,  43.6831,  58.7773, 101.4044, -84.6208,\n",
      "          10.1392, -40.5267,  29.8429,  -3.7543, -78.5595,   2.1421,  36.4904,\n",
      "           7.7524, -30.3793, -93.8813,  36.0431,  12.5543,  38.9544]])\n",
      "tensor([-66.2187])\n",
      "c value = tensor([1.0100], grad_fn=<MulBackward0>)\n",
      "epoch 700, loss 2.1549675464630127\n",
      "epoch 701, loss 2.154113292694092\n",
      "epoch 702, loss 2.1532530784606934\n",
      "epoch 703, loss 2.1523990631103516\n",
      "epoch 704, loss 2.1515278816223145\n",
      "epoch 705, loss 2.1506752967834473\n",
      "epoch 706, loss 2.1498196125030518\n",
      "epoch 707, loss 2.1489620208740234\n",
      "epoch 708, loss 2.148101806640625\n",
      "epoch 709, loss 2.147249221801758\n",
      "epoch 710, loss 2.1463863849639893\n",
      "epoch 711, loss 2.145530939102173\n",
      "epoch 712, loss 2.144672155380249\n",
      "epoch 713, loss 2.143805503845215\n",
      "epoch 714, loss 2.142946243286133\n",
      "epoch 715, loss 2.1420834064483643\n",
      "epoch 716, loss 2.141214609146118\n",
      "epoch 717, loss 2.1403403282165527\n",
      "epoch 718, loss 2.1394715309143066\n",
      "epoch 719, loss 2.1385979652404785\n",
      "epoch 720, loss 2.137730836868286\n",
      "epoch 721, loss 2.1368653774261475\n",
      "epoch 722, loss 2.135993719100952\n",
      "epoch 723, loss 2.1351282596588135\n",
      "epoch 724, loss 2.1342613697052\n",
      "epoch 725, loss 2.133396863937378\n",
      "epoch 726, loss 2.132528781890869\n",
      "epoch 727, loss 2.1316587924957275\n",
      "epoch 728, loss 2.1308014392852783\n",
      "epoch 729, loss 2.129936695098877\n",
      "epoch 730, loss 2.12906551361084\n",
      "epoch 731, loss 2.1282095909118652\n",
      "epoch 732, loss 2.12733793258667\n",
      "epoch 733, loss 2.1264679431915283\n",
      "epoch 734, loss 2.125610589981079\n",
      "epoch 735, loss 2.124742031097412\n",
      "epoch 736, loss 2.1238818168640137\n",
      "epoch 737, loss 2.123011827468872\n",
      "epoch 738, loss 2.1221439838409424\n",
      "epoch 739, loss 2.1212832927703857\n",
      "epoch 740, loss 2.1204211711883545\n",
      "epoch 741, loss 2.1195456981658936\n",
      "epoch 742, loss 2.118683099746704\n",
      "epoch 743, loss 2.117823839187622\n",
      "epoch 744, loss 2.1169521808624268\n",
      "epoch 745, loss 2.116088390350342\n",
      "epoch 746, loss 2.1152260303497314\n",
      "epoch 747, loss 2.1143674850463867\n",
      "epoch 748, loss 2.1135060787200928\n",
      "epoch 749, loss 2.112645149230957\n",
      "Pre normalize: \n",
      "tensor([[ 70.9001,  36.0650, -26.2886,  43.6810,  58.7752, 101.4055, -84.6231,\n",
      "          10.1371, -40.5257,  29.8388,  -3.7567, -78.5583,   2.1405,  36.4918,\n",
      "           7.7535, -30.3783, -93.8835,  36.0408,  12.5526,  38.9523]])\n",
      "tensor([-66.2210])\n",
      "After normalize: \n",
      "tensor([[ 71.6410,  36.4419, -26.5633,  44.1374,  59.3894, 102.4652, -85.5074,\n",
      "          10.2430, -40.9492,  30.1507,  -3.7960, -79.3793,   2.1629,  36.8731,\n",
      "           7.8346, -30.6958, -94.8646,  36.4174,  12.6837,  39.3594]])\n",
      "tensor([-66.9130])\n",
      "c value = tensor([1.0105], grad_fn=<MulBackward0>)\n",
      "epoch 750, loss 2.156146764755249\n",
      "epoch 751, loss 2.155266761779785\n",
      "epoch 752, loss 2.154385805130005\n",
      "epoch 753, loss 2.1535048484802246\n",
      "epoch 754, loss 2.1526217460632324\n",
      "epoch 755, loss 2.1517295837402344\n",
      "epoch 756, loss 2.150852918624878\n",
      "epoch 757, loss 2.1499671936035156\n",
      "epoch 758, loss 2.1490800380706787\n",
      "epoch 759, loss 2.1481895446777344\n",
      "epoch 760, loss 2.147305727005005\n",
      "epoch 761, loss 2.1464030742645264\n",
      "epoch 762, loss 2.1455228328704834\n",
      "epoch 763, loss 2.144624948501587\n",
      "epoch 764, loss 2.1437361240386963\n",
      "epoch 765, loss 2.1428308486938477\n",
      "epoch 766, loss 2.141935110092163\n",
      "epoch 767, loss 2.1410441398620605\n",
      "epoch 768, loss 2.1401467323303223\n",
      "epoch 769, loss 2.139251947402954\n",
      "epoch 770, loss 2.138362169265747\n",
      "epoch 771, loss 2.137465476989746\n",
      "epoch 772, loss 2.13657283782959\n",
      "epoch 773, loss 2.135679244995117\n",
      "epoch 774, loss 2.1347789764404297\n",
      "epoch 775, loss 2.133881092071533\n",
      "epoch 776, loss 2.1329848766326904\n",
      "epoch 777, loss 2.1320948600769043\n",
      "epoch 778, loss 2.131208896636963\n",
      "epoch 779, loss 2.130312442779541\n",
      "epoch 780, loss 2.1294236183166504\n",
      "epoch 781, loss 2.1285295486450195\n",
      "epoch 782, loss 2.127640962600708\n",
      "epoch 783, loss 2.126746892929077\n",
      "epoch 784, loss 2.1258597373962402\n",
      "epoch 785, loss 2.1249663829803467\n",
      "epoch 786, loss 2.124075412750244\n",
      "epoch 787, loss 2.12319278717041\n",
      "epoch 788, loss 2.122303009033203\n",
      "epoch 789, loss 2.12142276763916\n",
      "epoch 790, loss 2.120529890060425\n",
      "epoch 791, loss 2.1196398735046387\n",
      "epoch 792, loss 2.1187503337860107\n",
      "epoch 793, loss 2.117866039276123\n",
      "epoch 794, loss 2.116989850997925\n",
      "epoch 795, loss 2.1160998344421387\n",
      "epoch 796, loss 2.1152193546295166\n",
      "epoch 797, loss 2.1143288612365723\n",
      "epoch 798, loss 2.1134440898895264\n",
      "epoch 799, loss 2.1125638484954834\n",
      "Pre normalize: \n",
      "tensor([[ 71.6418,  36.4398, -26.5533,  44.1352,  59.3873, 102.4664, -85.5097,\n",
      "          10.2409, -40.9483,  30.1466,  -3.7985, -79.3781,   2.1612,  36.8744,\n",
      "           7.8357, -30.6948, -94.8669,  36.4151,  12.6819,  39.3572]])\n",
      "tensor([-66.9153])\n",
      "After normalize: \n",
      "tensor([[ 72.4255,  36.8384, -26.8438,  44.6179,  60.0369, 103.5872, -86.4451,\n",
      "          10.3529, -41.3962,  30.4763,  -3.8400, -80.2464,   2.1848,  37.2778,\n",
      "           7.9214, -31.0306, -95.9046,  36.8135,  12.8206,  39.7877]])\n",
      "tensor([-67.6473])\n",
      "c value = tensor([1.0109], grad_fn=<MulBackward0>)\n",
      "epoch 800, loss 2.158132553100586\n",
      "epoch 801, loss 2.157222032546997\n",
      "epoch 802, loss 2.1563143730163574\n",
      "epoch 803, loss 2.1554083824157715\n",
      "epoch 804, loss 2.1544995307922363\n",
      "epoch 805, loss 2.153582811355591\n",
      "epoch 806, loss 2.1526715755462646\n",
      "epoch 807, loss 2.1517655849456787\n",
      "epoch 808, loss 2.150845527648926\n",
      "epoch 809, loss 2.1499369144439697\n",
      "epoch 810, loss 2.149016857147217\n",
      "epoch 811, loss 2.1481051445007324\n",
      "epoch 812, loss 2.147183656692505\n",
      "epoch 813, loss 2.1462626457214355\n",
      "epoch 814, loss 2.145338773727417\n",
      "epoch 815, loss 2.144427537918091\n",
      "epoch 816, loss 2.1435039043426514\n",
      "epoch 817, loss 2.142580270767212\n",
      "epoch 818, loss 2.1416516304016113\n",
      "epoch 819, loss 2.1407394409179688\n",
      "epoch 820, loss 2.1398227214813232\n",
      "epoch 821, loss 2.1388914585113525\n",
      "epoch 822, loss 2.1379644870758057\n",
      "epoch 823, loss 2.137033700942993\n",
      "epoch 824, loss 2.1361212730407715\n",
      "epoch 825, loss 2.135188341140747\n",
      "epoch 826, loss 2.1342685222625732\n",
      "epoch 827, loss 2.133355140686035\n",
      "epoch 828, loss 2.132441282272339\n",
      "epoch 829, loss 2.1315107345581055\n",
      "epoch 830, loss 2.130598545074463\n",
      "epoch 831, loss 2.129674196243286\n",
      "epoch 832, loss 2.128760814666748\n",
      "epoch 833, loss 2.1278493404388428\n",
      "epoch 834, loss 2.1269307136535645\n",
      "epoch 835, loss 2.126018762588501\n",
      "epoch 836, loss 2.125108480453491\n",
      "epoch 837, loss 2.124189615249634\n",
      "epoch 838, loss 2.123274087905884\n",
      "epoch 839, loss 2.1223714351654053\n",
      "epoch 840, loss 2.121452808380127\n",
      "epoch 841, loss 2.1205503940582275\n",
      "epoch 842, loss 2.11963152885437\n",
      "epoch 843, loss 2.1187121868133545\n",
      "epoch 844, loss 2.1178038120269775\n",
      "epoch 845, loss 2.116895914077759\n",
      "epoch 846, loss 2.1159756183624268\n",
      "epoch 847, loss 2.115067481994629\n",
      "epoch 848, loss 2.114156723022461\n",
      "epoch 849, loss 2.113246202468872\n",
      "Pre normalize: \n",
      "tensor([[ 72.4264,  36.8362, -26.8337,  44.6157,  60.0347, 103.5884, -86.4474,\n",
      "          10.3507, -41.3952,  30.4722,  -3.8426, -80.2453,   2.1830,  37.2792,\n",
      "           7.9225, -31.0296, -95.9069,  36.8112,  12.8187,  39.7855]])\n",
      "tensor([-67.6496])\n",
      "After normalize: \n",
      "tensor([[ 73.2547,  37.2575, -27.1406,  45.1259,  60.7212, 104.7730, -87.4360,\n",
      "          10.4691, -41.8686,  30.8207,  -3.8865, -81.1630,   2.2080,  37.7055,\n",
      "           8.0131, -31.3845, -97.0037,  37.2321,  12.9653,  40.2404]])\n",
      "tensor([-68.4232])\n",
      "c value = tensor([1.0114], grad_fn=<MulBackward0>)\n",
      "epoch 850, loss 2.160921573638916\n",
      "epoch 851, loss 2.159985303878784\n",
      "epoch 852, loss 2.1590538024902344\n",
      "epoch 853, loss 2.1581122875213623\n",
      "epoch 854, loss 2.1571741104125977\n",
      "epoch 855, loss 2.1562304496765137\n",
      "epoch 856, loss 2.1552841663360596\n",
      "epoch 857, loss 2.1543421745300293\n",
      "epoch 858, loss 2.153404474258423\n",
      "epoch 859, loss 2.152470588684082\n",
      "epoch 860, loss 2.1515231132507324\n",
      "epoch 861, loss 2.15058970451355\n",
      "epoch 862, loss 2.149648904800415\n",
      "epoch 863, loss 2.148703098297119\n",
      "epoch 864, loss 2.1477582454681396\n",
      "epoch 865, loss 2.1468024253845215\n",
      "epoch 866, loss 2.1458587646484375\n",
      "epoch 867, loss 2.1449027061462402\n",
      "epoch 868, loss 2.14395809173584\n",
      "epoch 869, loss 2.1430091857910156\n",
      "epoch 870, loss 2.1420576572418213\n",
      "epoch 871, loss 2.1411094665527344\n",
      "epoch 872, loss 2.14016056060791\n",
      "epoch 873, loss 2.139206647872925\n",
      "epoch 874, loss 2.138261556625366\n",
      "epoch 875, loss 2.137315273284912\n",
      "epoch 876, loss 2.136366844177246\n",
      "epoch 877, loss 2.1354119777679443\n",
      "epoch 878, loss 2.1344571113586426\n",
      "epoch 879, loss 2.133512020111084\n",
      "epoch 880, loss 2.1325571537017822\n",
      "epoch 881, loss 2.13161039352417\n",
      "epoch 882, loss 2.1306662559509277\n",
      "epoch 883, loss 2.129720687866211\n",
      "epoch 884, loss 2.128772497177124\n",
      "epoch 885, loss 2.1278202533721924\n",
      "epoch 886, loss 2.126875400543213\n",
      "epoch 887, loss 2.125934600830078\n",
      "epoch 888, loss 2.1249966621398926\n",
      "epoch 889, loss 2.124049425125122\n",
      "epoch 890, loss 2.1230974197387695\n",
      "epoch 891, loss 2.1221604347229004\n",
      "epoch 892, loss 2.1212165355682373\n",
      "epoch 893, loss 2.1202750205993652\n",
      "epoch 894, loss 2.119337320327759\n",
      "epoch 895, loss 2.1183876991271973\n",
      "epoch 896, loss 2.1174464225769043\n",
      "epoch 897, loss 2.1164958477020264\n",
      "epoch 898, loss 2.115562677383423\n",
      "epoch 899, loss 2.114619731903076\n",
      "Pre normalize: \n",
      "tensor([[ 73.2557,  37.2552, -27.1304,  45.1236,  60.7189, 104.7741, -87.4383,\n",
      "          10.4668, -41.8677,  30.8165,  -3.8891, -81.1618,   2.2062,  37.7068,\n",
      "           8.0142, -31.3834, -97.0060,  37.2297,  12.9633,  40.2381]])\n",
      "tensor([-68.4255])\n",
      "After normalize: \n",
      "tensor([[ 74.1324,  37.7011, -27.4551,  45.6636,  61.4456, 106.0281, -88.4848,\n",
      "          10.5921, -42.3688,  31.1854,  -3.9357, -82.1332,   2.2326,  38.1581,\n",
      "           8.1101, -31.7590, -98.1670,  37.6752,  13.1184,  40.7197]])\n",
      "tensor([-69.2445])\n",
      "c value = tensor([1.0120], grad_fn=<MulBackward0>)\n",
      "epoch 900, loss 2.164576768875122\n",
      "epoch 901, loss 2.163607120513916\n",
      "epoch 902, loss 2.1626369953155518\n",
      "epoch 903, loss 2.1616721153259277\n",
      "epoch 904, loss 2.1607091426849365\n",
      "epoch 905, loss 2.1597402095794678\n",
      "epoch 906, loss 2.15877366065979\n",
      "epoch 907, loss 2.1577959060668945\n",
      "epoch 908, loss 2.1568264961242676\n",
      "epoch 909, loss 2.155850887298584\n",
      "epoch 910, loss 2.1548843383789062\n",
      "epoch 911, loss 2.1538970470428467\n",
      "epoch 912, loss 2.1529245376586914\n",
      "epoch 913, loss 2.15195631980896\n",
      "epoch 914, loss 2.150968551635742\n",
      "epoch 915, loss 2.149989366531372\n",
      "epoch 916, loss 2.1490025520324707\n",
      "epoch 917, loss 2.1480324268341064\n",
      "epoch 918, loss 2.147047758102417\n",
      "epoch 919, loss 2.1460604667663574\n",
      "epoch 920, loss 2.1450791358947754\n",
      "epoch 921, loss 2.144101619720459\n",
      "epoch 922, loss 2.1431283950805664\n",
      "epoch 923, loss 2.1421561241149902\n",
      "epoch 924, loss 2.1411683559417725\n",
      "epoch 925, loss 2.140170097351074\n",
      "epoch 926, loss 2.139190912246704\n",
      "epoch 927, loss 2.1382126808166504\n",
      "epoch 928, loss 2.1372389793395996\n",
      "epoch 929, loss 2.136251211166382\n",
      "epoch 930, loss 2.135272979736328\n",
      "epoch 931, loss 2.1342833042144775\n",
      "epoch 932, loss 2.133323907852173\n",
      "epoch 933, loss 2.1323299407958984\n",
      "epoch 934, loss 2.1313657760620117\n",
      "epoch 935, loss 2.1303892135620117\n",
      "epoch 936, loss 2.1294138431549072\n",
      "epoch 937, loss 2.128429412841797\n",
      "epoch 938, loss 2.1274633407592773\n",
      "epoch 939, loss 2.1264891624450684\n",
      "epoch 940, loss 2.1255125999450684\n",
      "epoch 941, loss 2.124537706375122\n",
      "epoch 942, loss 2.1235690116882324\n",
      "epoch 943, loss 2.1225876808166504\n",
      "epoch 944, loss 2.1216189861297607\n",
      "epoch 945, loss 2.1206469535827637\n",
      "epoch 946, loss 2.1196672916412354\n",
      "epoch 947, loss 2.11869478225708\n",
      "epoch 948, loss 2.1177241802215576\n",
      "epoch 949, loss 2.116755247116089\n",
      "Pre normalize: \n",
      "tensor([[ 74.1336,  37.6988, -27.4449,  45.6612,  61.4433, 106.0293, -88.4871,\n",
      "          10.5897, -42.3677,  31.1811,  -3.9384, -82.1321,   2.2307,  38.1594,\n",
      "           8.1113, -31.7580, -98.1693,  37.6728,  13.1163,  40.7174]])\n",
      "tensor([-69.2471])\n",
      "After normalize: \n",
      "tensor([[ 75.0604,  38.1701, -27.7880,  46.2321,  62.2115, 107.3549, -89.5934,\n",
      "          10.7221, -42.8974,  31.5709,  -3.9876, -83.1589,   2.2585,  38.6365,\n",
      "           8.2127, -32.1550, -99.3967,  38.1438,  13.2803,  41.2265]])\n",
      "tensor([-70.1128])\n",
      "c value = tensor([1.0125], grad_fn=<MulBackward0>)\n",
      "epoch 950, loss 2.1690189838409424\n",
      "epoch 951, loss 2.1680171489715576\n",
      "epoch 952, loss 2.167022228240967\n",
      "epoch 953, loss 2.166023015975952\n",
      "epoch 954, loss 2.165018320083618\n",
      "epoch 955, loss 2.1640071868896484\n",
      "epoch 956, loss 2.1630048751831055\n",
      "epoch 957, loss 2.1619985103607178\n",
      "epoch 958, loss 2.1609928607940674\n",
      "epoch 959, loss 2.1599831581115723\n",
      "epoch 960, loss 2.158970355987549\n",
      "epoch 961, loss 2.157966375350952\n",
      "epoch 962, loss 2.156951904296875\n",
      "epoch 963, loss 2.1559395790100098\n",
      "epoch 964, loss 2.1549320220947266\n",
      "epoch 965, loss 2.153921127319336\n",
      "epoch 966, loss 2.15290904045105\n",
      "epoch 967, loss 2.151897668838501\n",
      "epoch 968, loss 2.1508820056915283\n",
      "epoch 969, loss 2.149871349334717\n",
      "epoch 970, loss 2.1488535404205322\n",
      "epoch 971, loss 2.1478402614593506\n",
      "epoch 972, loss 2.146831512451172\n",
      "epoch 973, loss 2.145815134048462\n",
      "epoch 974, loss 2.144806146621704\n",
      "epoch 975, loss 2.1437876224517822\n",
      "epoch 976, loss 2.1427884101867676\n",
      "epoch 977, loss 2.141777992248535\n",
      "epoch 978, loss 2.1407597064971924\n",
      "epoch 979, loss 2.1397476196289062\n",
      "epoch 980, loss 2.13873028755188\n",
      "epoch 981, loss 2.137733221054077\n",
      "epoch 982, loss 2.13671875\n",
      "epoch 983, loss 2.1357154846191406\n",
      "epoch 984, loss 2.134709119796753\n",
      "epoch 985, loss 2.1336984634399414\n",
      "epoch 986, loss 2.1326956748962402\n",
      "epoch 987, loss 2.1316921710968018\n",
      "epoch 988, loss 2.130676746368408\n",
      "epoch 989, loss 2.1296753883361816\n",
      "epoch 990, loss 2.128671646118164\n",
      "epoch 991, loss 2.127664089202881\n",
      "epoch 992, loss 2.126659631729126\n",
      "epoch 993, loss 2.12564754486084\n",
      "epoch 994, loss 2.124642848968506\n",
      "epoch 995, loss 2.1236348152160645\n",
      "epoch 996, loss 2.1226277351379395\n",
      "epoch 997, loss 2.121622323989868\n",
      "epoch 998, loss 2.1206228733062744\n",
      "epoch 999, loss 2.119617462158203\n",
      "Pre normalize: \n",
      "tensor([[ 75.0616,  38.1678, -27.7776,  46.2296,  62.2091, 107.3561, -89.5959,\n",
      "          10.7197, -42.8962,  31.5667,  -3.9903, -83.1578,   2.2566,  38.6379,\n",
      "           8.2139, -32.1539, -99.3990,  38.1413,  13.2782,  41.2240]])\n",
      "tensor([-70.1155])\n",
      "After normalize: \n",
      "tensor([[  76.0405,   38.6656,  -28.1399,   46.8325,   63.0204,  108.7561,\n",
      "          -90.7643,   10.8595,  -43.4557,   31.9783,   -4.0424,  -84.2423,\n",
      "            2.2860,   39.1418,    8.3210,  -32.5732, -100.6953,   38.6387,\n",
      "           13.4513,   41.7617]])\n",
      "tensor([-71.0299])\n",
      "c value = tensor([1.0130], grad_fn=<MulBackward0>)\n",
      "epoch 1000, loss 2.1742355823516846\n",
      "epoch 1001, loss 2.173199415206909\n",
      "epoch 1002, loss 2.1721596717834473\n",
      "epoch 1003, loss 2.1711246967315674\n",
      "epoch 1004, loss 2.170076608657837\n",
      "epoch 1005, loss 2.1690337657928467\n",
      "epoch 1006, loss 2.1679983139038086\n",
      "epoch 1007, loss 2.1669504642486572\n",
      "epoch 1008, loss 2.165898323059082\n",
      "epoch 1009, loss 2.164858102798462\n",
      "epoch 1010, loss 2.163806676864624\n",
      "epoch 1011, loss 2.162764310836792\n",
      "epoch 1012, loss 2.161729335784912\n",
      "epoch 1013, loss 2.160675048828125\n",
      "epoch 1014, loss 2.159635543823242\n",
      "epoch 1015, loss 2.158583879470825\n",
      "epoch 1016, loss 2.1575348377227783\n",
      "epoch 1017, loss 2.156491994857788\n",
      "epoch 1018, loss 2.155435085296631\n",
      "epoch 1019, loss 2.154397487640381\n",
      "epoch 1020, loss 2.1533429622650146\n",
      "epoch 1021, loss 2.1523020267486572\n",
      "epoch 1022, loss 2.1512434482574463\n",
      "epoch 1023, loss 2.1502022743225098\n",
      "epoch 1024, loss 2.1491520404815674\n",
      "epoch 1025, loss 2.148103952407837\n",
      "epoch 1026, loss 2.1470589637756348\n",
      "epoch 1027, loss 2.1460187435150146\n",
      "epoch 1028, loss 2.1449625492095947\n",
      "epoch 1029, loss 2.1439197063446045\n",
      "epoch 1030, loss 2.142878293991089\n",
      "epoch 1031, loss 2.141831636428833\n",
      "epoch 1032, loss 2.1407828330993652\n",
      "epoch 1033, loss 2.1397385597229004\n",
      "epoch 1034, loss 2.138699769973755\n",
      "epoch 1035, loss 2.137648105621338\n",
      "epoch 1036, loss 2.1366140842437744\n",
      "epoch 1037, loss 2.1355650424957275\n",
      "epoch 1038, loss 2.1345174312591553\n",
      "epoch 1039, loss 2.133474349975586\n",
      "epoch 1040, loss 2.1324336528778076\n",
      "epoch 1041, loss 2.1313936710357666\n",
      "epoch 1042, loss 2.130347728729248\n",
      "epoch 1043, loss 2.1293041706085205\n",
      "epoch 1044, loss 2.1282756328582764\n",
      "epoch 1045, loss 2.127222776412964\n",
      "epoch 1046, loss 2.1261844635009766\n",
      "epoch 1047, loss 2.125136613845825\n",
      "epoch 1048, loss 2.124098062515259\n",
      "epoch 1049, loss 2.1230573654174805\n",
      "Pre normalize: \n",
      "tensor([[  76.0416,   38.6631,  -28.1294,   46.8300,   63.0179,  108.7573,\n",
      "          -90.7670,   10.8571,  -43.4545,   31.9740,   -4.0451,  -84.2411,\n",
      "            2.2839,   39.1431,    8.3222,  -32.5721, -100.6976,   38.6361,\n",
      "           13.4491,   41.7592]])\n",
      "tensor([-71.0325])\n",
      "After normalize: \n",
      "tensor([[  77.0791,   39.1906,  -28.5132,   47.4690,   63.8778,  110.2412,\n",
      "          -92.0054,   11.0053,  -44.0474,   32.4103,   -4.1003,  -85.3905,\n",
      "            2.3151,   39.6772,    8.4358,  -33.0165, -102.0715,   39.1632,\n",
      "           13.6326,   42.3289]])\n",
      "tensor([-72.0017])\n",
      "c value = tensor([1.0136], grad_fn=<MulBackward0>)\n",
      "epoch 1050, loss 2.1803154945373535\n",
      "epoch 1051, loss 2.1792378425598145\n",
      "epoch 1052, loss 2.1781585216522217\n",
      "epoch 1053, loss 2.1770923137664795\n",
      "epoch 1054, loss 2.1760032176971436\n",
      "epoch 1055, loss 2.174928665161133\n",
      "epoch 1056, loss 2.1738412380218506\n",
      "epoch 1057, loss 2.1727662086486816\n",
      "epoch 1058, loss 2.171684980392456\n",
      "epoch 1059, loss 2.170607566833496\n",
      "epoch 1060, loss 2.169532060623169\n",
      "epoch 1061, loss 2.168462038040161\n",
      "epoch 1062, loss 2.167382001876831\n",
      "epoch 1063, loss 2.1663031578063965\n",
      "epoch 1064, loss 2.1652097702026367\n",
      "epoch 1065, loss 2.1641294956207275\n",
      "epoch 1066, loss 2.163039207458496\n",
      "epoch 1067, loss 2.1619389057159424\n",
      "epoch 1068, loss 2.1608424186706543\n",
      "epoch 1069, loss 2.1597466468811035\n",
      "epoch 1070, loss 2.1586599349975586\n",
      "epoch 1071, loss 2.157567262649536\n",
      "epoch 1072, loss 2.1564743518829346\n",
      "epoch 1073, loss 2.155386209487915\n",
      "epoch 1074, loss 2.154304027557373\n",
      "epoch 1075, loss 2.1532094478607178\n",
      "epoch 1076, loss 2.1521382331848145\n",
      "epoch 1077, loss 2.151057004928589\n",
      "epoch 1078, loss 2.1499736309051514\n",
      "epoch 1079, loss 2.1488852500915527\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2ccc616c700a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# graph_val_pen = model.train_network_with_penalty(num_epochs, given_fn, adam_optimizer, LAMBDA_PEN, LOWER_BOUND, UPPER_BOUND, N_POINTS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgraph_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madam_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOWER_BOUND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUPPER_BOUND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_POINTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-7325fd013b49>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(self, num_epochs, v_x, optimizer, lower_bound, upper_bound, n_points)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             loss = epsilon_Loss(v_x, self,\n\u001b[0;32m--> 131\u001b[0;31m                                 lower_bound, upper_bound, n_points)\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0my_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mx_epochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-614fe69b05b7>\u001b[0m in \u001b[0;36mepsilon_Loss\u001b[0;34m(v_x, model_u, lower_bound, upper_bound, n_points)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mu_xi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mu_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_u\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_prime_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mv_xi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7325fd013b49>\u001b[0m in \u001b[0;36mu_prime_2\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mu_prime_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mi_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mclone_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictiter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictiter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__deepcopy__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/site-packages/torch/nn/parameter.py\u001b[0m in \u001b[0;36m__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph_val = model.train_network(num_epochs, given_fn, adam_optimizer, LOWER_BOUND, UPPER_BOUND, N_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 731, loss 9313.361328125\n",
      "epoch 732, loss 9313.3408203125\n",
      "epoch 733, loss 9313.2626953125\n",
      "epoch 734, loss 9313.1572265625\n",
      "epoch 735, loss 9313.130859375\n",
      "epoch 736, loss 9313.0390625\n",
      "epoch 737, loss 9312.947265625\n",
      "epoch 738, loss 9312.8876953125\n",
      "epoch 739, loss 9312.828125\n",
      "epoch 740, loss 9312.814453125\n",
      "epoch 741, loss 9312.73828125\n",
      "epoch 742, loss 9312.6787109375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a8786a0ab5c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph_val_pen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network_with_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madam_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLAMBDA_PEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOWER_BOUND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUPPER_BOUND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_POINTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-7325fd013b49>\u001b[0m in \u001b[0;36mtrain_network_with_penalty\u001b[0;34m(self, num_epochs, v_x, optimizer, lambda_pen, lower_bound, upper_bound, n_points)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch {}, loss {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0;31m# Calculate the derivative(loss) w.r.t the model's parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph_val_pen = model.train_network_with_penalty(num_epochs, given_fn, adam_optimizer, LAMBDA_PEN, LOWER_BOUND, UPPER_BOUND, N_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkbD3fXRLGBI",
    "outputId": "7dfa28bb-5379-4069-a4be-cf3dc88e551d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.3806], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon_Loss(given_fn, model, LOWER_BOUND, UPPER_BOUND, N_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBYQKdIYHYOY"
   },
   "outputs": [],
   "source": [
    "loss_val = [i.detach().numpy().item() for i in graph_val[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKZP39qEHj_Z",
    "outputId": "065e7dfb-d136-4412-c8e1-70abb86d2b2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[270, 271, 272, 273, 274]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_val[0][270:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "jd2I0gRqHO8Z",
    "outputId": "15978184-99da-4f52-d206-d0deff5669aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_figure(graph_val[0][150:], loss_val[150:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GZ9S2gz-JM1"
   },
   "source": [
    "Dear professor, here's my current progress:\n",
    "1. I implement epsilon_Loss_penalty function. Now, the training process will iterate until it counts 5 consecutive times the loss difference between the current loss values with its previous loss < 1e-5. \n",
    "\n",
    "    Then, it finds the normalization constant c and apply to model.output weight layer and its bias. Then the algorithm updates the normalized output weight layer for the model. \n",
    "\n",
    "    The result: the training process only takes < 300 epochs to converge to a loss value of 78.7160. This occurs consistently after several re-runs. The problem: the constant c is 1 from my normalization function. I'm looking for a normalization from the PyTorch library to use instead. Please let me know if I can use this function for the normalization part: [PyTorch normalization function](https://pytorch.org/docs/stable/generated/torch.nn.functional.normalize.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjtOBhyZLspO"
   },
   "outputs": [],
   "source": [
    "# PLOT DATA\n",
    "def plot_figure(x_train, y_train, x_test=None, predicted=None):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 6), dpi=80)\n",
    "    plt.plot(x_train, y_train, '--', label='True data', alpha=0.5)\n",
    "    if predicted != None:\n",
    "        plt.plot(x_test, predicted, '--', label='Predictions', alpha=0.5)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "yOAl63W0qaq_",
    "outputId": "0966c5cc-146b-4f15-d330-522a5eafc1e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_figure(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "el6VL83Qddfj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPn9pDNlm88W"
   },
   "source": [
    "# HOW TO TRAIN THE NN MODEL:\n",
    "1. Reset adam_optimizer: \n",
    "```adam_opt.zero_grad()```\n",
    "2. Calculate loss\n",
    "3. Update the optimizer: \n",
    "```adam_opt.step()```\n",
    "\n",
    "\n",
    "        weight = weight - lr * gradient\n",
    "\n",
    "-> use lr and gradient to \"improve\" weight layer. \n",
    "\n",
    "Explanation:\n",
    "```adam_opt.step()```: Update the model's parameters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VfCCteSFrC8"
   },
   "outputs": [],
   "source": [
    "model_p = Nonlinear_2(4)\n",
    "adam_opt = torch.optim.Adam(model_p.parameters(), \n",
    "                                    lr=learningRate, \n",
    "                                    betas=(0.9, 0.999), \n",
    "                                    eps=1e-08, \n",
    "                                    weight_decay=0, \n",
    "                                    amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUjHqCmkNRzB",
    "outputId": "a8eba8cf-5f24-45cf-f588-85f437d798fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5799], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_i = torch.tensor([-2.5], requires_grad=True, dtype=torch.float)\n",
    "u_xi = model_p(x_i)\n",
    "print(u_xi)\n",
    "u_xi.backward()\n",
    "u_prime = x_i.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0A2qd2M1OXQ5",
    "outputId": "224f75ed-733d-4edc-f3ad-3d78dac7f66a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Hidden layers: \n",
      "Parameter containing:\n",
      "tensor([[-0.4363],\n",
      "        [-0.2556],\n",
      "        [ 0.6323],\n",
      "        [-0.8666]], requires_grad=True)\n",
      "\n",
      "\n",
      "- Hidden layers gradients (derivative of Loss w.r.t model params): \n",
      "None\n",
      "Parameter containing:\n",
      "tensor([ 0.9463, -0.8034, -0.8181,  0.5812], requires_grad=True)\n",
      "\n",
      "\n",
      "Output layers: \n",
      "Parameter containing:\n",
      "tensor([[-0.3294, -0.3360,  0.0453, -0.0283]], requires_grad=True)\n",
      "None\n",
      "Parameter containing:\n",
      "tensor([0.4850], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Access the model's parameters:\n",
    "# Model's hidden layer weight and bias\n",
    "print(\"- Hidden layers: \")\n",
    "print(model_p.hidden.weight)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"- Hidden layers gradients (derivative of Loss w.r.t model params): \")\n",
    "print(model_p.hidden.weight.grad)\n",
    "print(model_p.hidden.bias)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Model's output layer weight and bias\n",
    "print(\"Output layers: \")\n",
    "print(model_p.output.weight)\n",
    "print(model_p.hidden.weight.grad)\n",
    "print(model_p.output.bias)\n",
    "\n",
    "# model.zero_grad()\n",
    "\n",
    "i, o = (x_train[0], y_train[0])\n",
    "i = Variable(torch.from_numpy(i))\n",
    "o = Variable(torch.from_numpy(o))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BA_91qCHWhjs",
    "outputId": "20a262ba-f9a2-4cf1-b0c4-110596eb2f63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Hidden layers: \n",
      "Parameter containing:\n",
      "tensor([[-0.4363],\n",
      "        [-0.2556],\n",
      "        [ 0.6323],\n",
      "        [-0.8666]], requires_grad=True)\n",
      "\n",
      "\n",
      "- Hidden layers gradients (derivative of Loss w.r.t model params): \n",
      "None\n",
      "Parameter containing:\n",
      "tensor([ 0.9463, -0.8034, -0.8181,  0.5812], requires_grad=True)\n",
      "\n",
      "\n",
      "Output layers: \n",
      "Parameter containing:\n",
      "tensor([[-0.3294, -0.3360,  0.0453, -0.0283]], requires_grad=True)\n",
      "None\n",
      "Parameter containing:\n",
      "tensor([0.4850], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"- Hidden layers: \")\n",
    "print(model_p.hidden.weight)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"- Hidden layers gradients (derivative of Loss w.r.t model params): \")\n",
    "print(model_p.hidden.weight.grad)\n",
    "print(model_p.hidden.bias)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Model's output layer weight and bias\n",
    "print(\"Output layers: \")\n",
    "print(model_p.output.weight)\n",
    "print(model_p.hidden.weight.grad)\n",
    "print(model_p.output.bias)\n",
    "\n",
    "# model.zero_grad()\n",
    "\n",
    "i, o = (x_train[0], y_train[0])\n",
    "i = Variable(torch.from_numpy(i))\n",
    "o = Variable(torch.from_numpy(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jQeEbaMHkeA",
    "outputId": "32d5985d-7d0f-4048-c555-7ef3785149ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkmedTJ6J1Hr",
    "outputId": "01a83e6e-280c-4a72-d9c4-bff991258866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "t = 3*a**3 - b**2\n",
    "\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "t.backward(gradient=external_grad)\n",
    "\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyuNXuz-iLBZ"
   },
   "source": [
    "https://neptune.ai/blog/pytorch-loss-functions\n",
    "https://stackoverflow.com/questions/53980031/pytorch-custom-loss-function\n",
    "https://stackoverflow.com/questions/65947284/loss-with-custom-backward-function-in-pytorch-exploding-loss-in-simple-mse-exa\n",
    "https://www.youtube.com/watch?v=ma2KXWblllc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0PS0efCiJCp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PyTorch_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c21580189d9a9d7f1e3fef63ff70de58083dda94aa35d8ed937f03fa405217e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
